{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-04T20:48:31.718072Z",
     "start_time": "2021-03-04T20:48:28.487260Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries to be used\n",
    "\n",
    "# Warning messages display\n",
    "# import warnings\n",
    "# warnings.filterwarnings(action='once') # https://docs.python.org/3/library/warnings.html#the-warnings-filter\n",
    "\n",
    "# Directories/Files management\n",
    "import os.path\n",
    "## from zipfile import ZipFile # De momento no ha hecho falta \n",
    "\n",
    "# Timing\n",
    "import time\n",
    "\n",
    "# Data analysis and wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', None) # Show all columns in DataFrames\n",
    "pd.set_option('display.max_rows', 100) # If too high, it greatly slows down the output display and freezes the kernel\n",
    "\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot') # choose a style: 'plt.style.available'\n",
    "sns.set_theme(context='notebook',\n",
    "              style=\"darkgrid\") # {darkgrid, whitegrid, dark, white, ticks}\n",
    "palette = sns.color_palette(\"flare\", as_cmap=True);\n",
    "import altair as alt\n",
    "\n",
    "# Machine Learning\n",
    "## from sklearn.[...] import ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-04T20:48:31.814013Z",
     "start_time": "2021-03-04T20:48:31.727070Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-04T20:48:31.844994Z",
     "start_time": "2021-03-04T20:48:31.824007Z"
    }
   },
   "outputs": [],
   "source": [
    "t0 = time.perf_counter() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-04T20:48:31.873981Z",
     "start_time": "2021-03-04T20:48:31.850990Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on Windows.\n",
      "root path\t C:\\Users\\turge\\CompartidoVM\\0.TFM\n"
     ]
    }
   ],
   "source": [
    "# Detect Operating System running and manage paths accordingly\n",
    "\n",
    "if os.name == 'nt': # Windows\n",
    "    root = r\"C:\\Users\\turge\\CompartidoVM\\0.TFM\"\n",
    "    print(\"Running on Windows.\")\n",
    "elif os.name == 'posix': # Ubuntu\n",
    "    root = \"/home/dsc/shared/0.TFM\"\n",
    "    print(\"Running on Ubuntu.\")\n",
    "print(\"root path\\t\", root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-04T20:48:31.904956Z",
     "start_time": "2021-03-04T20:48:31.879972Z"
    }
   },
   "outputs": [],
   "source": [
    "cols = [\n",
    " 'MONTH',\n",
    " 'DAY_OF_MONTH',\n",
    " 'DAY_OF_WEEK',\n",
    " 'OP_UNIQUE_CARRIER',\n",
    " 'ORIGIN',\n",
    "#  'WBAN_Origin_OTP', # Redundant having the 'ORIGIN' feature\n",
    " 'DEST',\n",
    "#  'WBAN_Dest_OTP', # Redundant having the 'DEST' feature\n",
    " 'DEP_TIME_hour',\n",
    " 'TAXI_OUT_median',\n",
    " 'TAXI_IN_median',\n",
    " 'ARR_TIME_hour',\n",
    " 'CRS_ELAPSED_TIME',\n",
    " 'DISTANCE',\n",
    "#  'DISTANCE_GROUP', # Redundant having the 'DISTANCE' feature\n",
    "#  'STATION_Origin', # Redundant having the 'ORIGIN' feature\n",
    "#  'WMO_Origin', # Redundant having the 'ORIGIN' feature\n",
    "#  'WBAN_Origin_LCD', # Redundant having the 'ORIGIN' feature\n",
    " 'HourlyAltimeterSetting_Origin',\n",
    " 'HourlyDryBulbTemperature_Origin',\n",
    " 'HourlyPrecipitation_Origin',\n",
    " 'HourlyRelativeHumidity_Origin',\n",
    " 'HourlySkyConditions_Origin',\n",
    " 'HourlyVisibility_Origin',\n",
    "#  'REM_Origin', # Not relevant for the model\n",
    "#  'STATION_Dest', # Redundant having the 'DEST' feature\n",
    "#  'WMO_Dest', # Redundant having the 'DEST' feature\n",
    "#  'WBAN_Dest_LCD', # Redundant having the 'DEST' feature\n",
    " 'HourlyAltimeterSetting_Dest',\n",
    " 'HourlyDryBulbTemperature_Dest',\n",
    " 'HourlyPrecipitation_Dest',\n",
    " 'HourlyRelativeHumidity_Dest',\n",
    " 'HourlySkyConditions_Dest',\n",
    " 'HourlyVisibility_Dest',\n",
    "#  'REM_Dest', # Redundant having the 'ORIGIN' feature\n",
    " 'ARR_DEL15' # → Target !!\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-04T20:48:31.936936Z",
     "start_time": "2021-03-04T20:48:31.910954Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\turge\\\\CompartidoVM\\\\0.TFM\\\\Output_Data\\\\US_DoT-NOAA\\\\OTP_LCD_allColumns.csv'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_input_csv_path = os.path.join(root,\n",
    "                                           \"Output_Data\",\n",
    "                                           \"US_DoT-NOAA\",\n",
    "                                           \"OTP_LCD_allColumns.csv\")\n",
    "preprocessed_input_csv_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-04T20:48:28.496Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_all = pd.read_csv(preprocessed_input_csv_path,\n",
    "                     encoding='latin1',\n",
    "                     usecols=cols,\n",
    "                     dtype={\n",
    "                            'MONTH' : 'string',\n",
    "                            'DAY_OF_MONTH' : 'string',\n",
    "                            'DAY_OF_WEEK' : 'string',\n",
    "                            'OP_UNIQUE_CARRIER' : 'string',\n",
    "                            'ORIGIN' : 'string',\n",
    "                            'DEST' : 'string',\n",
    "                            'DEP_TIME_hour' : 'string',\n",
    "                            'TAXI_OUT_median' : 'int32',\n",
    "                            'TAXI_IN_median' : 'int32',\n",
    "                            'ARR_TIME_hour' : 'string',\n",
    "                            'CRS_ELAPSED_TIME' : 'int32',\n",
    "                            'DISTANCE' : 'int32',\n",
    "                            'HourlyAltimeterSetting_Origin' : 'float32',\n",
    "                            'HourlyDryBulbTemperature_Origin' : 'float32',\n",
    "                            'HourlyPrecipitation_Origin' : 'float32',\n",
    "                            'HourlyRelativeHumidity_Origin' : 'float32',\n",
    "                            'HourlySkyConditions_Origin' : 'string',\n",
    "                            'HourlyVisibility_Origin' : 'float32',\n",
    "                            'HourlyAltimeterSetting_Dest' : 'float32',\n",
    "                            'HourlyDryBulbTemperature_Dest' : 'float32',\n",
    "                            'HourlyPrecipitation_Dest' : 'float32',\n",
    "                            'HourlyRelativeHumidity_Dest' : 'float32',\n",
    "                            'HourlySkyConditions_Dest' : 'string',\n",
    "                            'HourlyVisibility_Dest' : 'float32',\n",
    "                            'ARR_DEL15' : 'int32'\n",
    "                           }\n",
    "                    )\n",
    "df_all.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-04T20:48:28.513Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_all.info()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-02T17:58:15.551Z"
    }
   },
   "source": [
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "# Complete report:\n",
    "prof = ProfileReport(df_all, minimal=True)\n",
    "prof.to_file('complete_report_XGboost15.html')\n",
    "\n",
    "# # Sample report (more computationally efficient)\n",
    "# prof = ProfileReport(df_all.sample(10000)) \n",
    "# prof.to_file('sample_report.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-04T20:48:28.576Z"
    }
   },
   "outputs": [],
   "source": [
    "# For deterministic purposes, let's define a seed:\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-04T20:48:28.580Z"
    }
   },
   "outputs": [],
   "source": [
    "# For the initial runs, define a reasonably-sized sample:\n",
    "sample_length = int(1e5)\n",
    "\n",
    "# The dataset is somehow imbalanced (80% on-time / 20% delayed), so let's perform some quick undersampling:\n",
    "delayed = df_all[df_all['ARR_DEL15'] == 1].sample(sample_length // 2)\n",
    "ontime = df_all[df_all['ARR_DEL15'] == 0].sample(sample_length // 2)\n",
    "df = delayed.append(ontime)\n",
    "\n",
    "# Get dummies for the categorical features:\n",
    "# df = pd.get_dummies(data=df, columns=df.select_dtypes('category').columns)\n",
    "\n",
    "print(\"On-time flights: {:7d} ({:5.2f}%)\".format(len(ontime), 100*len(ontime)/len(df)))\n",
    "print(\"Delayed flights: {:7d} ({:5.2f}%)\".format(len(delayed), 100*len(delayed)/len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / Test / Validation split\n",
    "Break the dataset into three blocks:\n",
    "1. Training (70%)\n",
    "2. Test (15%)\n",
    "3. Validation (15%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-04T20:48:28.588Z"
    }
   },
   "outputs": [],
   "source": [
    "def target_encoder(df, column, target, index=None, method='mean'):\n",
    "    \"\"\"\n",
    "    Target-based encoding is numerization of a categorical variables via the target variable. Main purpose is to deal\n",
    "    with high cardinality categorical features without exploding dimensionality. This replaces the categorical variable\n",
    "    with just one new numerical variable. Each category or level of the categorical variable is represented by a\n",
    "    summary statistic of the target for that level.\n",
    "    Args:\n",
    "        df (pandas df): Pandas DataFrame containing the categorical column and target.\n",
    "        column (str): Categorical variable column to be encoded.\n",
    "        target (str): Target on which to encode.\n",
    "        index (arr): Can be supplied to use targets only from the train index. Avoids data leakage from the test fold\n",
    "        method (str): Summary statistic of the target. Mean, median or std. deviation.\n",
    "    Returns:\n",
    "        arr: Encoded categorical column.\n",
    "    \"\"\"\n",
    "\n",
    "    index = df.index if index is None else index # Encode the entire input df if no specific indices is supplied\n",
    "\n",
    "    if method == 'mean':\n",
    "        encoded_column = df[column].map(df.iloc[index].groupby(column)[target].mean())\n",
    "    elif method == 'median':\n",
    "        encoded_column = df[column].map(df.iloc[index].groupby(column)[target].median())\n",
    "    elif method == 'std':\n",
    "        encoded_column = df[column].map(df.iloc[index].groupby(column)[target].std())\n",
    "    else:\n",
    "        raise ValueError(\"Incorrect method supplied: '{}'. Must be one of 'mean', 'median', 'std'\".format(method))\n",
    "\n",
    "    return encoded_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-04T20:48:28.594Z"
    }
   },
   "outputs": [],
   "source": [
    "df.select_dtypes('string').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-04T20:48:28.600Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Divide the data:\n",
    "train_idx = df.sample(frac=0.7).index\n",
    "for c in df.select_dtypes('string').columns:\n",
    "    df[c] = target_encoder(df_all, column=c, target='ARR_DEL15', index=train_idx, method='mean')\n",
    "train = df.loc[train_idx, :]\n",
    "test_valid = df.drop(train.index)\n",
    "test = test_valid.sample(frac=0.5)\n",
    "valid = test_valid.drop(test.index)\n",
    "\n",
    "print(\"Total dataset:      {:7d} ({:6.2f}%)\".format(len(df), 100*len(df)/len(df)))\n",
    "print(\"Training dataset:   {:7d} ( {:5.2f}%)\".format(len(train), 100*len(train)/len(df)))\n",
    "print(\"Test dataset:       {:7d} ( {:5.2f}%)\".format(len(test), 100*len(test)/len(df)))\n",
    "print(\"Validation dataset: {:7d} ( {:5.2f}%)\".format(len(valid), 100*len(valid)/len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-04T20:48:28.607Z"
    }
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-04T20:48:28.614Z"
    }
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-04T20:48:28.621Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train / Test partition:\n",
    "X_train, y_train = train.drop(\"ARR_DEL15\", axis=1), train[\"ARR_DEL15\"]\n",
    "X_test, y_test = test.drop(\"ARR_DEL15\", axis=1), test[\"ARR_DEL15\"]\n",
    "\n",
    "# Validation:\n",
    "X_valid, y_valid = valid.drop(\"ARR_DEL15\", axis=1), valid[\"ARR_DEL15\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`num_boost_round`: corresponds to the number of boosting rounds or trees to build\n",
    "- Its optimal value highly depends on the other parameters, and thus it should be re-tuned each time you update a parameter.\n",
    "- You could do this by tuning it together with all parameters in a grid-search, but it requires a lot of computational effort.\n",
    "- Fortunately XGBoost provides a nice way to find the best number of rounds whilst training.\n",
    "- We can test our model at each step and see if adding a new tree/round improves performance.\n",
    "- If performance haven’t improved for N rounds (N is defined by the variable `early_stopping_round`), we stop the training and keep the best number of boosting rounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-04T20:48:28.626Z"
    }
   },
   "outputs": [],
   "source": [
    "# Basic model definition:\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, verbosity=1, random_state=0, objective= 'binary:logistic',\n",
    "                          booster='gbtree', tree_method='auto', num_boost_round = 999, early_stopping_round=10)\n",
    "\n",
    "params = {\n",
    "            'min_child_weight': [0.1, 1, 5, 10, 50],\n",
    "            'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "            'subsample': [0.6, 0.8, 1.0],\n",
    "            'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "            'max_depth': [5, 10, 25, 50],\n",
    "            'learning_rate': [0.0001, 0.001, 0.1, 1],\n",
    "            'n_estimators': [50, 100, 250, 500],\n",
    "            'reg_alpha': [0.0001, 0.001, 0.1, 1],\n",
    "            'reg_lambda': [0.0001, 0.001, 0.1, 1]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Info: https://stackoverflow.com/questions/51587535/custom-evaluation-function-based-on-f1-for-use-in-xgboost-python-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-04T20:48:28.634Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def f1_eval(y_pred, dtrain):\n",
    "    y_true = dtrain.get_label()\n",
    "    err = 1 - f1_score(y_true, np.round(y_pred))\n",
    "    return 'f1_err', err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-04T20:48:28.640Z"
    }
   },
   "outputs": [],
   "source": [
    "# Source: https://github.com/dask/dask-searchcv/issues/51\n",
    "# Info: https://ml.dask.org/modules/generated/dask_ml.model_selection.RandomizedSearchCV.html\n",
    "\n",
    "from dask_ml.model_selection import RandomizedSearchCV\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "dask_rscv = RandomizedSearchCV(xgb_model, \n",
    "                                   cv=5,\n",
    "                                   param_distributions=params, \n",
    "                                   n_iter=10, # Number of parameter settings that are sampled → trades off runtime vs quality\n",
    "                                   scoring='recall', \n",
    "                                   n_jobs=-2, # all CPUs but one are used\n",
    "                                   random_state=1001)\n",
    "\n",
    "with ProgressBar():\n",
    "    dask_rscv.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-04T20:48:28.646Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv_res = pd.DataFrame(dask_rscv.cv_results_)\n",
    "cv_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-04T20:48:28.651Z"
    }
   },
   "outputs": [],
   "source": [
    "dask_rscv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-04T20:48:28.659Z"
    }
   },
   "outputs": [],
   "source": [
    "dask_rscv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-04T20:48:28.663Z"
    }
   },
   "outputs": [],
   "source": [
    "dask_rscv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-04T20:48:28.667Z"
    }
   },
   "outputs": [],
   "source": [
    "dask_rscv.best_index_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-04T20:48:28.672Z"
    }
   },
   "outputs": [],
   "source": [
    "dask_rscv.scorer_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-04T20:48:28.680Z"
    }
   },
   "outputs": [],
   "source": [
    "dask_rscv.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-04T20:48:28.687Z"
    }
   },
   "outputs": [],
   "source": [
    "dask_rscv.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-04T20:48:28.691Z"
    }
   },
   "outputs": [],
   "source": [
    "dask_rscv.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-04T20:48:28.703Z"
    }
   },
   "outputs": [],
   "source": [
    "dask_rscv.visualize(filename='dask_rscv_23', format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-04T20:48:28.712Z"
    }
   },
   "outputs": [],
   "source": [
    "best_dask_rscv = dask_rscv.best_estimator_\n",
    "best_dask_rscv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional metrics results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-04T20:48:28.739Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the model's metrics:\n",
    "\n",
    "from sklearn.metrics import fbeta_score, f1_score, recall_score, precision_score, accuracy_score, \\\n",
    "                            confusion_matrix, classification_report, roc_curve, roc_auc_score, plot_roc_curve\n",
    "\n",
    "def clf_metrics(classifier, y_test, y_pred):\n",
    "    print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    print(\"Normalized confusion matrix:\\n\", confusion_matrix(y_test, y_pred, normalize='true'), '\\n')\n",
    "    print(classification_report(y_test, y_pred, target_names=['on-time', 'delayed']))\n",
    "    print(\"F-beta (ß=2) = {:6.3f}\".format(fbeta_score(y_test, y_pred, beta=2)))   \n",
    "    print(\"F1 =           {:6.3f}\".format(f1_score(y_test, y_pred)))   \n",
    "    print(\"Recall =       {:6.3f}\".format(recall_score(y_test, y_pred)))   \n",
    "    print(\"Precision =    {:6.3f}\".format(precision_score(y_test, y_pred)))   \n",
    "    print(\"Accuracy =     {:6.3f}\".format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-04T20:48:28.746Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train_pred = best_dask_rscv.predict(X_train)\n",
    "y_test_pred = best_dask_rscv.predict(X_test)\n",
    "y_valid_pred = best_dask_rscv.predict(X_valid)\n",
    "y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-04T20:48:28.788Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"-+-+-+ TRAINING dataset +-+-+-\\n\")\n",
    "clf_metrics(best_dask_rscv, y_train, y_train_pred)\n",
    "print(\"-------------------------------------------------------\\n\")\n",
    "print(\"-+-+-+ TEST dataset: +-+-+-\\n\")\n",
    "clf_metrics(best_dask_rscv, y_test, y_test_pred)\n",
    "print(\"-------------------------------------------------------\\n\")\n",
    "print(\"-+-+-+ VALIDATION dataset: +-+-+-\\n\")\n",
    "clf_metrics(best_dask_rscv, y_valid, y_valid_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-04T20:48:28.805Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train_probabilities = best_dask_rscv.predict_proba(X_train)[:,1]\n",
    "y_test_probabilities = best_dask_rscv.predict_proba(X_test)[:,1]\n",
    "y_valid_probabilities = best_dask_rscv.predict_proba(X_valid)[:,1]\n",
    "y_test_probabilities[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-04T20:48:28.820Z"
    }
   },
   "outputs": [],
   "source": [
    "fpr_train, tpr_train, _ = roc_curve(y_train,  y_train_probabilities)\n",
    "fpr_test, tpr_test, _ = roc_curve(y_test,  y_test_probabilities)\n",
    "fpr_valid, tpr_valid, _ = roc_curve(y_valid,  y_valid_probabilities)\n",
    "\n",
    "auc_train = roc_auc_score(y_train, y_train_probabilities)\n",
    "auc_test = roc_auc_score(y_test, y_test_probabilities)\n",
    "auc_valid = roc_auc_score(y_valid, y_valid_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-04T20:48:28.828Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the figure:\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "\n",
    "plot_roc_curve(best_dask_rscv, X_train, y_train, color='red', ax=ax, name='XGBoost_train')\n",
    "plot_roc_curve(best_dask_rscv, X_test, y_test, color='blue', ax=ax, name='XGBoost_test')\n",
    "plot_roc_curve(best_dask_rscv, X_valid, y_valid, color='green', ax=ax, name='XGBoost_valid')\n",
    "plt.plot([0,1], [0,1], color='orange', linestyle='--')\n",
    "\n",
    "plt.xticks(np.arange(0.0, 1.1, step=0.1))\n",
    "plt.xlabel(\"fpr\", fontsize=15)\n",
    "\n",
    "plt.yticks(np.arange(0.0, 1.1, step=0.1))\n",
    "plt.ylabel(\"tpr\", fontsize=15)\n",
    "\n",
    "plt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\n",
    "plt.legend(prop={'size':13}, loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance (data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-04T20:48:28.839Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's build a dictionary to better see each INFORMATIVE feature with its corresponding importance:\n",
    "feature_importance_dict = dict(zip(X_train.columns, best_dask_rscv.feature_importances_))\n",
    "feature_importance_df = pd.DataFrame(feature_importance_dict.items(), columns=['Feature', 'Importance']) \\\n",
    "                          .sort_values(by='Importance', ascending=False)\n",
    "feature_importance_df = feature_importance_df[feature_importance_df['Importance'] != 0]\n",
    "feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-04T20:48:28.846Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.barplot(y='Feature', x='Importance', data=feature_importance_df, orient='h');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-04T20:48:28.856Z"
    }
   },
   "outputs": [],
   "source": [
    "# Source: https://machinelearningmastery.com/calculate-feature-importance-with-python/\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# perform permutation importance\n",
    "results = permutation_importance(best_dask_rscv, X_train, y_train, scoring='f1')\n",
    "# get importance\n",
    "importance = dict(zip(X_train.columns, results.importances_mean))\n",
    "permutation_importance_df = pd.DataFrame(importance.items(), columns=['Feature', 'Permutation_Importance']) \\\n",
    "                          .sort_values(by='Permutation_Importance', ascending=False)\n",
    "permutation_importance_df = permutation_importance_df[permutation_importance_df['Permutation_Importance'] != 0]\n",
    "permutation_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-04T20:48:28.861Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.barplot(y='Feature', x='Permutation_Importance', data=permutation_importance_df, orient='h');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance (plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weight-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-04T20:48:28.919Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from xgboost import plot_importance\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "plot_importance(booster=best_dask_rscv,\n",
    "                ax=ax,\n",
    "                grid=True,\n",
    "                importance_type='weight', # ”weight” is the number of times a feature appears in a tree\n",
    "                max_num_features=None, \n",
    "                height=0.5,\n",
    "                xlim=None,\n",
    "                ylim=None, \n",
    "                title='Feature importance (WEIGHT-based)',\n",
    "                xlabel='F score',\n",
    "                ylabel='Features',\n",
    "                show_values=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gain-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-04T20:48:28.957Z"
    }
   },
   "outputs": [],
   "source": [
    "from xgboost import plot_importance\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "plot_importance(booster=best_dask_rscv,\n",
    "                ax=ax,\n",
    "                grid=True,\n",
    "                importance_type='gain',\n",
    "                max_num_features=None, \n",
    "                height=0.5,\n",
    "                xlim=None,\n",
    "                ylim=None, \n",
    "                title='Feature importance (GAIN-based)',\n",
    "                xlabel='F score',\n",
    "                ylabel='Features',\n",
    "                show_values=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-04T20:48:28.972Z"
    }
   },
   "outputs": [],
   "source": [
    "from xgboost import plot_tree\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(80,50))\n",
    "\n",
    "plot_tree(booster=best_dask_rscv,\n",
    "          num_trees=0,\n",
    "          ax=ax)\n",
    "\n",
    "plt.savefig('plot_tree_23_best_dask_rscv.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphviz (tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Convert specified tree to graphviz instance. IPython can automatically plot the returned graphiz instance. Otherwise, you should call `.render()` method of the returned graphiz instance.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-04T20:48:28.996Z"
    }
   },
   "outputs": [],
   "source": [
    "from xgboost import to_graphviz\n",
    "\n",
    "tree_graphviz = to_graphviz(booster=best_dask_rscv,\n",
    "                            yes_color='#0000FF',\n",
    "                            no_color='#FF0000',\n",
    "                            condition_node_params={'shape': 'box',\n",
    "                                                   'style': 'filled,rounded',\n",
    "                                                   'fillcolor': '#78bceb'},\n",
    "                            leaf_node_params={'shape': 'box',\n",
    "                                              'style': 'filled',\n",
    "                                              'fillcolor': '#e48038'})\n",
    "\n",
    "format = 'png'\n",
    "tree_graphviz = to_graphviz(best_dask_rscv)\n",
    "tree_graphviz.graph_attr = {'dpi':'400'}\n",
    "tree_graphviz.render('tree_graphviz_23_best_dask_rscv', format = format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-04T20:48:29.015Z"
    }
   },
   "outputs": [],
   "source": [
    "t1 = time.perf_counter() - t0\n",
    "print(\"Time elapsed: {:2.0f}h {:2.0f}min {:2.0f}s\".format(t1//3600, (t1%3600)//60, (t1%3600)%60))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Time elapsed:  0h  3min  6s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-04T20:48:29.122Z"
    }
   },
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "\n",
    "# Save model to file:\n",
    "dump(best_dask_rscv, \"XGBoost_23_best_dask_rscv.joblib.dat\")\n",
    "print(\"Saved model to: XGBoost_23_best_dask_rscv.joblib.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-04T20:48:29.132Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load model from file:\n",
    "loaded_model = load(\"XGBoost_23_best_dask_rscv.joblib.dat\")\n",
    "print(\"Loaded model from: XGBoost_23_best_dask_rscv.joblib.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-04T20:48:29.140Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make predictions for test data\n",
    "predictions = loaded_model.predict(X_test)\n",
    "\n",
    "# Evaluate predictions:\n",
    "recall = recall_score(y_test, predictions)\n",
    "print(\"Loaded model recall: {:6.3f}\".format(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "829px",
    "left": "1550px",
    "right": "20px",
    "top": "113px",
    "width": "347px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
