{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Ask an interesting question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Delay binary classifier (yes/no, >15 min)\n",
    "- Better isolate the key features to predict delays so that airlines focus their efforts on their weakest points\n",
    "- Account for uncontrollable variables (e.g. unpredicted adverse weather) and prepare to act consequently\n",
    "2. Additional questions:\n",
    "- When is the best time of day/day of week/time of year to fly to minimize delays?\n",
    "- Do older planes suffer more delays?\n",
    "- How does the number of people flying between different locations change over time?\n",
    "- How well does weather predict plane delays?\n",
    "- Can you detect cascading failures as delays in one airport create delays in others? Are there critical links in the system?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Get the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On-Time : Reporting Carrier On-Time Performance (1987-present)\n",
    "\n",
    "Source: https://www.transtats.bts.gov/DatabaseInfo.asp?DB_ID=120&DB_URL=Mode_ID=1&Mode_Desc=Aviation&Subject_ID2=0\n",
    "\n",
    "<em>Note</em>: Over time both the code and the name of a carrier may change and the same code or name may be assumed by a different airline. To ensure that you are analyzing data from the same airline, TranStats provides four airline-specific variables that identify one and only one carrier or its entity: Airline ID (AirlineID), Unique Carrier Code (UniqueCarrier), Unique Carrier Name (UniqueCarrierName), and Unique Entity (UniqCarrierEntity). A unique airline (carrier) is defined as one holding and reporting under the same DOT certificate regardless of its Code, Name, or holding company/corporation. US Airways and America West started to report combined on-time data in January 2006 and combined traffic and financial data in October 2007 following their 2005 merger announcement. Delta and Northwest began reporting jointly in January 2010 following their 2008 merger announcement. Continental Micronesia was combined into Continental Airlines in December 2010 and joint reporting began in January 2011. Atlantic Southeast and ExpressJet began reporting jointly in January 2012. United and Continental began reporting jointly in January 2012 following their 2010 merger announcement. Endeavor (9E) operated as Pinnacle prior to August 2013. Envoy (MQ) operated as American Eagle prior to April 2014. Southwest (WN) and AirTran (FL) began reporting jointly in January 2015 following their 2011 merger announcement. American (AA) and US Airways (US) began reporting jointly as AA in July 2015 following their 2013 merger announcement. Alaska (AS) and Virgin America (VX) began reporting jointly as AS in April 2018 following their 2016 merger announcement.\n",
    "\n",
    "        \n",
    "- **AC_Types**:\n",
    "    - **Summary**:\n",
    "        - Reporting Carrier On-Time Performance (1987-present)\n",
    "    - **Description**:\n",
    "        - Reporting carriers are required to (or voluntarily) report on-time data for flights they operate: on-time arrival and departure data for non-stop domestic flights by month and year, by carrier and by origin and destination airport. Includes scheduled and actual departure and arrival times, canceled and diverted flights, taxi-out and taxi-in times, causes of delay and cancellation, air time, and non-stop distance. Use Download for individual flight data.\n",
    "    - **File**:\n",
    "        - YYMM_123456789_T_ONTIME_REPORTING.zip *(YY = Year ; MM = Month)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries to be used\n",
    "\n",
    "# Warning messages display\n",
    "## import warnings\n",
    "## warnings.filterwarnings(action='ignore') # https://docs.python.org/3/library/warnings.html#the-warnings-filter\n",
    "\n",
    "# Directories/Files management\n",
    "import os.path\n",
    "## from zipfile import ZipFile # De momento no ha hecho falta \n",
    "\n",
    "# Timing\n",
    "import time\n",
    "\n",
    "# Memory monitoring\n",
    "%load_ext memory_profiler\n",
    "### Use '%memit' to check at each point\n",
    "\n",
    "# Data analysis and wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', None) # Show all columns in DataFrames\n",
    "## pd.set_option('display.max_rows', None) # It greatly slows down the output display and freezes the kernel\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot') # choose a style: 'plt.style.available'\n",
    "sns.set_theme(context='notebook',\n",
    "              style=\"darkgrid\") # {darkgrid, whitegrid, dark, white, ticks}\n",
    "palette = sns.color_palette(\"flare\", as_cmap=True);\n",
    "import altair as alt\n",
    "\n",
    "# Machine Learning\n",
    "## from sklearn.[...] import ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.perf_counter() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect Operating System running and manage paths accordingly\n",
    "\n",
    "if os.name == 'nt': # Windows\n",
    "    root = r\"C:\\Users\\turge\\CompartidoVM\\0.TFM\"\n",
    "    print(\"Running on Windows.\")\n",
    "elif os.name == 'posix': # Ubuntu\n",
    "    root = \"/home/dsc/shared/0.TFM\"\n",
    "    print(\"Running on Ubuntu.\")\n",
    "print(\"root path\\t\", root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAMPLE FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single file import (i.e. month-sized database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "csv_path = os.path.join(root,\n",
    "                        \"Raw_Data\",\n",
    "                        \"US_DoT\",\n",
    "                        \"ONTIME_REPORTING\",\n",
    "                        \"1901_921771952_T_ONTIME_REPORTING.zip\")\n",
    "csv_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering that each file is considerably big, and that later on many of them will have to be grouped, a first exploration will be done considering the first 10,000 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Since 'pd.read_csv' works fine with zipped csv files, we can proceed directly:\n",
    "cols = pd.read_csv(csv_path, nrows=1).columns # After normally importing it, an undesired extra blank column is loaded\n",
    "df_0 = pd.read_csv(csv_path,\n",
    "                  encoding='latin1',\n",
    "                  nrows=10000,\n",
    "                  usecols=cols[:-1], # This way, the extra column is disregarded for the loading process\n",
    "                  low_memory = False)\n",
    "df_0.sample(5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cols = list(df_0.columns)\n",
    "cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see which columns will remain, and skip the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MULTIPLE FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's proceed with multiple-file importing, through concatenation into a single DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run only the first time to read the 12 month individual files and generate the global file (entire year 2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Read individual month files"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "directory_in_str = os.path.join(root,\n",
    "                                \"Raw_Data\",\n",
    "                                \"US_DoT\",\n",
    "                                \"ONTIME_REPORTING\")\n",
    "directory_in_str"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# List the files' paths corresponding to each month of year 2019\n",
    "\n",
    "file_list = []\n",
    "try:\n",
    "    os.listdir(directory_in_str)\n",
    "except FileNotFoundError:\n",
    "    print(\"The system cannot find the specified path:\\n\" + directory_in_str + \"\\nPlease check the path has been properly set.\")\n",
    "else:\n",
    "    for file in os.listdir(directory_in_str):\n",
    "        if file.endswith(\".zip\") and file.startswith(\"19\"):\n",
    "            file_list.append(os.path.join(directory_in_str, file))\n",
    "            continue\n",
    "        else:\n",
    "            continue\n",
    "file_list    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "# Create a DataFrame from the 12 month-files corresponding to the year 2019\n",
    "\n",
    "cols = pd.read_csv(csv_path, nrows=1).columns # After normally importing it, an undesired extra blank column is loaded\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for i, csv_path in enumerate(file_list):\n",
    "    if i == 13: # Fail-safe: in case the list captured more than 12 files\n",
    "        break\n",
    "    df_month = pd.read_csv(csv_path,\n",
    "                           encoding='latin1',\n",
    "                           nrows=1e6, # Fail-safe: in case the file is inadvertently too big\n",
    "                           usecols=cols[:-1], # This way, the extra column is disregarded for the loading process\n",
    "                           low_memory = False) # This will prevent from auto-dtypes\n",
    "    df = df.append(df_month)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Generate file (2019)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "output_csv_path = os.path.join(root,\n",
    "                               \"Output_Data\",\n",
    "                               \"US_DoT\",\n",
    "                               \"AL_OTP_MVP_Preprocessed_19_v1.csv\")\n",
    "output_csv_path"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "df.to_csv(path_or_buf=output_csv_path,\n",
    "          index=False,\n",
    "          encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%memit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "csv_path = os.path.join(root,\n",
    "                        \"Output_Data\",\n",
    "                        \"US_DoT\",\n",
    "                        \"AL_OTP_MVP_Preprocessed_19_v1.csv\")\n",
    "csv_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Chunk-loading procedure is required so as to prevent memory-saturation errors from appearing:\n",
    "\n",
    "chunks_list = []\n",
    "chunks = pd.read_csv(csv_path,\n",
    "                     encoding='latin1',\n",
    "#                      nrows=1e6, # Fail-safe: in case the file is inadvertently too big\n",
    "                     chunksize=1e6,\n",
    "                     usecols=cols[:-1], # This way, the extra column is disregarded for the loading process\n",
    "                     low_memory = False)\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    if i == 13: # Fail-safe: for debugging purposes only\n",
    "        break\n",
    "    chunks_list.append(chunk)\n",
    "\n",
    "    \n",
    "df = pd.concat(chunks_list, axis=0)\n",
    "del chunks_list\n",
    "\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%memit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some missing values. Let's further delve into it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Absolute & Relative frequency of missing values by column:\n",
    "pd.set_option('display.max_rows', 110) # It greatly slows down the output display and freezes the kernel\n",
    "missing = pd.DataFrame([df.isna().sum(), df.isna().sum() / len(df) * 100], index=['Absolute', 'Relative']).T.sort_values(by='Relative', ascending=False)\n",
    "missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show which columns present more than 5% of missing data:\n",
    "empty_df = missing[missing['Relative'] > 5]\n",
    "empty_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Drop every column with more than 90% of missing values:\n",
    "empty90_df = missing[missing['Relative'] > 90]\n",
    "empty90_df_cols = empty90_df.index.to_list()\n",
    "df = df.drop(empty90_df_cols, axis=1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CHECK AGAIN after manipulating the missing data.\n",
    "# Show which columns present more than 5% of missing data:\n",
    "empty_df = missing[missing['Relative'] > 5]\n",
    "empty_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "empty_df_cols = empty_df.index.to_list()\n",
    "df[empty_df_cols] = df[empty_df_cols].fillna(value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%memit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "isolated_elements_missing = missing.loc[(missing['Relative'] < 5) & (missing['Relative'] > 0)]\n",
    "isolated_elements_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Quick approach → check how many rows contain empty values to see if directly dropping them would be feasible:\n",
    "\n",
    "# Check which rows have at least 1 NaN:\n",
    "df[df.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# DF's length as is:\n",
    "original_length = len(df)\n",
    "print('Original dataset length:\\t', original_length)\n",
    "\n",
    "# Check how many rows have at least 1 NaN:\n",
    "manipulated_length = len(df.drop(df[df.isna().any(axis=1)].index, axis=0))\n",
    "print('Manipulated dataset length:\\t', manipulated_length)\n",
    "\n",
    "# Dropped rows, absolute and relative number:\n",
    "print('{} rows deleted, accounting for {:.2f}% of the original dataset.'.format(original_length - manipulated_length, (original_length - manipulated_length) / original_length * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering that it is only ~2% of the data, and that the dataset is big enough, let's just simply drop those rows as a quick  cleaning method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df = df.drop(df[df.isna().any(axis=1)].index, axis=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# CHECK AGAIN after manipulating the missing data.\n",
    "# Absolute & Relative frequency of missing values by column:\n",
    "pd.set_option('display.max_rows', 110) # It greatly slows down the output display and freezes the kernel\n",
    "missing_2 = pd.DataFrame([df.isna().sum(), df.isna().sum() / len(df) * 100], index=['Absolute', 'Relative']).T.sort_values(by='Relative', ascending=False)\n",
    "missing_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. Now the dataset is totally free of missing values.\n",
    "\n",
    "Let's store the recently cleaned dataset in a CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 10) # Go back to the default display value (10 rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run only the first time to generate the global CLEAN file (year 2019)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "output_csv_path = os.path.join(root,\n",
    "                               \"Output_Data\",\n",
    "                               \"US_DoT\",\n",
    "                               \"AL_OTP_MVP_Preprocessed_19_v2_clean.csv\")\n",
    "output_csv_path"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "# Wall time: 10min 42s\n",
    "\n",
    "df.to_csv(path_or_buf=output_csv_path,\n",
    "          index=False,\n",
    "          encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_cols = [\n",
    "# Time Period\n",
    " 'YEAR',\n",
    "#  'QUARTER', # Disregarded: redundant\n",
    " 'MONTH',\n",
    " 'DAY_OF_MONTH',\n",
    " 'DAY_OF_WEEK',\n",
    "#  'FL_DATE', # Disregarded: redundant\n",
    "# Airline / Aircraft\n",
    " 'OP_UNIQUE_CARRIER',\n",
    " 'OP_CARRIER_AIRLINE_ID',\n",
    " 'OP_CARRIER',\n",
    " 'TAIL_NUM',\n",
    " 'OP_CARRIER_FL_NUM',\n",
    "# Origin\n",
    " 'ORIGIN_AIRPORT_ID',\n",
    " 'ORIGIN_AIRPORT_SEQ_ID',\n",
    " 'ORIGIN_CITY_MARKET_ID',\n",
    " 'ORIGIN',\n",
    " 'ORIGIN_CITY_NAME',\n",
    " 'ORIGIN_STATE_ABR',\n",
    " 'ORIGIN_STATE_FIPS',\n",
    " 'ORIGIN_STATE_NM',\n",
    " 'ORIGIN_WAC',\n",
    "# Destination\n",
    " 'DEST_AIRPORT_ID',\n",
    " 'DEST_AIRPORT_SEQ_ID',\n",
    " 'DEST_CITY_MARKET_ID',\n",
    " 'DEST',\n",
    " 'DEST_CITY_NAME',\n",
    " 'DEST_STATE_ABR',\n",
    " 'DEST_STATE_FIPS',\n",
    " 'DEST_STATE_NM',\n",
    " 'DEST_WAC',\n",
    "# Departure Performance\n",
    " 'CRS_DEP_TIME',\n",
    " 'DEP_TIME',\n",
    " 'DEP_DELAY',\n",
    " 'DEP_DELAY_NEW',\n",
    " 'DEP_DEL15',\n",
    " 'DEP_DELAY_GROUP',\n",
    " 'DEP_TIME_BLK',\n",
    " 'TAXI_OUT',\n",
    " 'WHEELS_OFF',\n",
    "# Arrival Performance\n",
    " 'WHEELS_ON',\n",
    " 'TAXI_IN',\n",
    " 'CRS_ARR_TIME',\n",
    " 'ARR_TIME',\n",
    " 'ARR_DELAY',\n",
    " 'ARR_DELAY_NEW',\n",
    " 'ARR_DEL15',\n",
    " 'ARR_DELAY_GROUP',\n",
    " 'ARR_TIME_BLK',\n",
    "# Cancellations and Diversions\n",
    " 'CANCELLED',\n",
    " 'CANCELLATION_CODE',\n",
    " 'DIVERTED',\n",
    "# Flight Summaries\n",
    " 'CRS_ELAPSED_TIME',\n",
    " 'ACTUAL_ELAPSED_TIME',\n",
    " 'AIR_TIME',\n",
    " 'FLIGHTS',\n",
    " 'DISTANCE',\n",
    " 'DISTANCE_GROUP',\n",
    "# Cause of Delay\n",
    " 'CARRIER_DELAY',\n",
    " 'WEATHER_DELAY',\n",
    " 'NAS_DELAY',\n",
    " 'SECURITY_DELAY',\n",
    " 'LATE_AIRCRAFT_DELAY',\n",
    "# Gate Return Information at Origin Airport (Data starts 10/2008)\n",
    " 'FIRST_DEP_TIME',\n",
    " 'TOTAL_ADD_GTIME',\n",
    " 'LONGEST_ADD_GTIME',\n",
    "# Diverted Airport Information (Data starts 10/2008)\n",
    " 'DIV_AIRPORT_LANDINGS',\n",
    " 'DIV_REACHED_DEST',\n",
    " 'DIV_ACTUAL_ELAPSED_TIME',\n",
    " 'DIV_ARR_DELAY',\n",
    " 'DIV_DISTANCE',\n",
    " 'DIV1_AIRPORT',\n",
    " 'DIV1_AIRPORT_ID',\n",
    " 'DIV1_AIRPORT_SEQ_ID',\n",
    " 'DIV1_WHEELS_ON',\n",
    " 'DIV1_TOTAL_GTIME',\n",
    " 'DIV1_LONGEST_GTIME',\n",
    " 'DIV1_WHEELS_OFF',\n",
    " 'DIV1_TAIL_NUM',\n",
    " 'DIV2_AIRPORT',\n",
    " 'DIV2_AIRPORT_ID',\n",
    " 'DIV2_AIRPORT_SEQ_ID',\n",
    " 'DIV2_WHEELS_ON',\n",
    " 'DIV2_TOTAL_GTIME',\n",
    " 'DIV2_LONGEST_GTIME',\n",
    " 'DIV2_WHEELS_OFF',\n",
    " 'DIV2_TAIL_NUM',\n",
    " 'DIV3_AIRPORT',\n",
    " 'DIV3_AIRPORT_ID',\n",
    " 'DIV3_AIRPORT_SEQ_ID',\n",
    " 'DIV3_WHEELS_ON',\n",
    " 'DIV3_TOTAL_GTIME',\n",
    " 'DIV3_LONGEST_GTIME',\n",
    " 'DIV3_WHEELS_OFF',\n",
    " 'DIV3_TAIL_NUM',\n",
    " 'DIV4_AIRPORT',\n",
    " 'DIV4_AIRPORT_ID',\n",
    " 'DIV4_AIRPORT_SEQ_ID',\n",
    " 'DIV4_WHEELS_ON',\n",
    " 'DIV4_TOTAL_GTIME',\n",
    " 'DIV4_LONGEST_GTIME',\n",
    " 'DIV4_WHEELS_OFF',\n",
    " 'DIV4_TAIL_NUM',\n",
    " 'DIV5_AIRPORT',\n",
    " 'DIV5_AIRPORT_ID',\n",
    " 'DIV5_AIRPORT_SEQ_ID',\n",
    " 'DIV5_WHEELS_ON',\n",
    " 'DIV5_TOTAL_GTIME',\n",
    " 'DIV5_LONGEST_GTIME',\n",
    " 'DIV5_WHEELS_OFF',\n",
    " 'DIV5_TAIL_NUM'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{} columns kept from a total of {}.\".format(len(int_cols), len(cols)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional information on each column meaning can be found [here](https://www.transtats.bts.gov/Fields.asp?Table_ID=236&SYS_Table_Name=T_ONTIME_REPORTING&User_Table_Name=Reporting%20Carrier%20On-Time%20Performance%20(1987-present)&Year_Info=1&First_Year=1987&Last_Year=2020&Rate_Info=0&Frequency=Monthly&Data_Frequency=Annual,Quarterly,Monthly)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cols = [\n",
    " 'YEAR',\n",
    " 'QUARTER',\n",
    " 'MONTH',\n",
    " 'DAY_OF_MONTH',\n",
    " 'DAY_OF_WEEK',\n",
    "#  'FL_DATE',\n",
    "#  'OP_UNIQUE_CARRIER',\n",
    "#  'OP_CARRIER_AIRLINE_ID',\n",
    "#  'OP_CARRIER',\n",
    "#  'TAIL_NUM',\n",
    "#  'OP_CARRIER_FL_NUM',\n",
    "#  'ORIGIN_AIRPORT_ID',\n",
    "#  'ORIGIN_AIRPORT_SEQ_ID',\n",
    "#  'ORIGIN_CITY_MARKET_ID',\n",
    "#  'ORIGIN',\n",
    "#  'ORIGIN_CITY_NAME',\n",
    "#  'ORIGIN_STATE_ABR',\n",
    "#  'ORIGIN_STATE_FIPS',\n",
    "#  'ORIGIN_STATE_NM',\n",
    "#  'ORIGIN_WAC',\n",
    "#  'DEST_AIRPORT_ID',\n",
    "#  'DEST_AIRPORT_SEQ_ID',\n",
    "#  'DEST_CITY_MARKET_ID',\n",
    "#  'DEST',\n",
    "#  'DEST_CITY_NAME',\n",
    "#  'DEST_STATE_ABR',\n",
    "#  'DEST_STATE_FIPS',\n",
    "#  'DEST_STATE_NM',\n",
    "#  'DEST_WAC',\n",
    "#  'CRS_DEP_TIME',\n",
    "#  'DEP_TIME',\n",
    " 'DEP_DELAY',\n",
    " 'DEP_DELAY_NEW',\n",
    " 'DEP_DEL15',\n",
    "#  'DEP_DELAY_GROUP',\n",
    "#  'DEP_TIME_BLK',\n",
    " 'TAXI_OUT',\n",
    "#  'WHEELS_OFF',\n",
    "#  'WHEELS_ON',\n",
    " 'TAXI_IN',\n",
    "#  'CRS_ARR_TIME',\n",
    "#  'ARR_TIME',\n",
    " 'ARR_DELAY',\n",
    " 'ARR_DELAY_NEW',\n",
    " 'ARR_DEL15',\n",
    "#  'ARR_DELAY_GROUP',\n",
    "#  'ARR_TIME_BLK',\n",
    " 'CANCELLED',\n",
    " 'CANCELLATION_CODE',\n",
    " 'DIVERTED',\n",
    "#  'CRS_ELAPSED_TIME',\n",
    "#  'ACTUAL_ELAPSED_TIME',\n",
    " 'AIR_TIME',\n",
    " 'FLIGHTS',\n",
    " 'DISTANCE',\n",
    "#  'DISTANCE_GROUP',\n",
    " 'CARRIER_DELAY',\n",
    " 'WEATHER_DELAY',\n",
    " 'NAS_DELAY',\n",
    " 'SECURITY_DELAY',\n",
    " 'LATE_AIRCRAFT_DELAY',\n",
    "#  'FIRST_DEP_TIME',\n",
    "#  'TOTAL_ADD_GTIME',\n",
    "#  'LONGEST_ADD_GTIME',\n",
    "#  'DIV_AIRPORT_LANDINGS',\n",
    "#  'DIV_REACHED_DEST',\n",
    "#  'DIV_ACTUAL_ELAPSED_TIME',\n",
    " 'DIV_ARR_DELAY',\n",
    " 'DIV_DISTANCE',\n",
    "#  'DIV1_AIRPORT',\n",
    "#  'DIV1_AIRPORT_ID',\n",
    "#  'DIV1_AIRPORT_SEQ_ID',\n",
    "#  'DIV1_WHEELS_ON',\n",
    "#  'DIV1_TOTAL_GTIME',\n",
    "#  'DIV1_LONGEST_GTIME',\n",
    "#  'DIV1_WHEELS_OFF',\n",
    "#  'DIV1_TAIL_NUM',\n",
    "#  'DIV2_AIRPORT',\n",
    "#  'DIV2_AIRPORT_ID',\n",
    "#  'DIV2_AIRPORT_SEQ_ID',\n",
    "#  'DIV2_WHEELS_ON',\n",
    "#  'DIV2_TOTAL_GTIME',\n",
    "#  'DIV2_LONGEST_GTIME',\n",
    "#  'DIV2_WHEELS_OFF',\n",
    "#  'DIV2_TAIL_NUM',\n",
    "#  'DIV3_AIRPORT',\n",
    "#  'DIV3_AIRPORT_ID',\n",
    "#  'DIV3_AIRPORT_SEQ_ID',\n",
    "#  'DIV3_WHEELS_ON',\n",
    "#  'DIV3_TOTAL_GTIME',\n",
    "#  'DIV3_LONGEST_GTIME',\n",
    "#  'DIV3_WHEELS_OFF',\n",
    "#  'DIV3_TAIL_NUM',\n",
    "#  'DIV4_AIRPORT',\n",
    "#  'DIV4_AIRPORT_ID',\n",
    "#  'DIV4_AIRPORT_SEQ_ID',\n",
    "#  'DIV4_WHEELS_ON',\n",
    "#  'DIV4_TOTAL_GTIME',\n",
    "#  'DIV4_LONGEST_GTIME',\n",
    "#  'DIV4_WHEELS_OFF',\n",
    "#  'DIV4_TAIL_NUM',\n",
    "#  'DIV5_AIRPORT',\n",
    "#  'DIV5_AIRPORT_ID',\n",
    "#  'DIV5_AIRPORT_SEQ_ID',\n",
    "#  'DIV5_WHEELS_ON',\n",
    "#  'DIV5_TOTAL_GTIME',\n",
    "#  'DIV5_LONGEST_GTIME',\n",
    "#  'DIV5_WHEELS_OFF',\n",
    "#  'DIV5_TAIL_NUM'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df0 = df0[cols]\n",
    "df0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_csv_path = os.path.join(root,\n",
    "                               \"Output_Data\",\n",
    "                               \"US_DoT\",\n",
    "                               \"AL_OTP_MVP_Preprocessed.csv\")\n",
    "output_csv_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df0.to_csv(path_or_buf=output_csv_path,\n",
    "           index=False,\n",
    "           encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.perf_counter()  - t0\n",
    "print(\"Time elapsed: \", t1) # CPU seconds elapsed (floating point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Communicate and visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
