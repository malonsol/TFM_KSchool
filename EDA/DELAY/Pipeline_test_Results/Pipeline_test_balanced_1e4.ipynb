{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries to be used\n",
    "\n",
    "# Warning messages display\n",
    "# import warnings\n",
    "# warnings.filterwarnings(action='once') # https://docs.python.org/3/library/warnings.html#the-warnings-filter\n",
    "\n",
    "# Directories/Files management\n",
    "import os.path\n",
    "## from zipfile import ZipFile # De momento no ha hecho falta \n",
    "\n",
    "# Timing\n",
    "import time\n",
    "\n",
    "# Data analysis and wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', None) # Show all columns in DataFrames\n",
    "pd.set_option('display.max_rows', 100) # If too high, it greatly slows down the output display and freezes the kernel\n",
    "\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot') # choose a style: 'plt.style.available'\n",
    "sns.set_theme(context='notebook',\n",
    "              style=\"darkgrid\") # {darkgrid, whitegrid, dark, white, ticks}\n",
    "palette = sns.color_palette(\"flare\", as_cmap=True);\n",
    "import altair as alt\n",
    "\n",
    "# Machine Learning\n",
    "## from sklearn.[...] import ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.perf_counter() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on Windows.\n",
      "root path\t C:\\Users\\turge\\CompartidoVM\\0.TFM\n"
     ]
    }
   ],
   "source": [
    "# Detect Operating System running and manage paths accordingly\n",
    "\n",
    "if os.name == 'nt': # Windows\n",
    "    root = r\"C:\\Users\\turge\\CompartidoVM\\0.TFM\"\n",
    "    print(\"Running on Windows.\")\n",
    "elif os.name == 'posix': # Ubuntu\n",
    "    root = \"/home/dsc/shared/0.TFM\"\n",
    "    print(\"Running on Ubuntu.\")\n",
    "print(\"root path\\t\", root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cols = [\n",
    "    \n",
    "### -----  < X > (PRE-FLIGHT DATA) -----\n",
    "\n",
    "# Time Period\n",
    "#  'YEAR', # Disregarded: for the time being, analysis limted to 2019\n",
    "#  'QUARTER', # Disregarded: redundant\n",
    " 'MONTH',\n",
    " 'DAY_OF_MONTH',\n",
    " 'DAY_OF_WEEK',\n",
    "#  'FL_DATE', # Disregarded: redundant\n",
    "# Airline / Aircraft\n",
    " 'OP_UNIQUE_CARRIER',\n",
    "#  'OP_CARRIER_AIRLINE_ID', # Disregarded: redundant\n",
    "#  'OP_CARRIER', # Disregarded: redundant\n",
    " 'TAIL_NUM',\n",
    "#  'OP_CARRIER_FL_NUM', # Unknown in advance?\n",
    "# Origin\n",
    "#  'ORIGIN_AIRPORT_ID', # Disregarded: redundant\n",
    "#  'ORIGIN_AIRPORT_SEQ_ID', # Disregarded: redundant\n",
    " 'ORIGIN_CITY_MARKET_ID',\n",
    " 'ORIGIN',\n",
    "#  'ORIGIN_CITY_NAME', # Disregarded: redundant\n",
    "#  'ORIGIN_STATE_ABR', # Disregarded: redundant\n",
    "#  'ORIGIN_STATE_FIPS', # Federal Information Processing Standards # Not used for the moment\n",
    "#  'ORIGIN_STATE_NM', # Disregarded: redundant\n",
    "#  'ORIGIN_WAC', # World Area Code # Not used for the moment\n",
    "# Destination\n",
    "#  'DEST_AIRPORT_ID', # Disregarded: redundant\n",
    "#  'DEST_AIRPORT_SEQ_ID', # Disregarded: redundant\n",
    " 'DEST_CITY_MARKET_ID',\n",
    " 'DEST',\n",
    "#  'DEST_CITY_NAME', # Disregarded: redundant\n",
    "#  'DEST_STATE_ABR', # Disregarded: redundant\n",
    "#  'DEST_STATE_FIPS', # Federal Information Processing Standards # Not used for the moment\n",
    "#  'DEST_STATE_NM', # Disregarded: redundant\n",
    "#  'DEST_WAC', # World Area Code # Not used for the moment\n",
    "# Departure Performance\n",
    " 'CRS_DEP_TIME',\n",
    "#  'TAXI_OUT_median', #  Output / However, the median for each airport could be used as input !! (explanation below)   \n",
    "# Arrival Performance\n",
    " 'CRS_ARR_TIME',\n",
    "#  'TAXI_IN_median', #  Output / However, the median for each airport could be used as input !! (explanation below) \n",
    "# Flight Summaries\n",
    " 'CRS_ELAPSED_TIME',\n",
    " 'FLIGHTS',\n",
    " 'DISTANCE',\n",
    " 'DISTANCE_GROUP',\n",
    "\n",
    "### ----- < y > (PRE-FLIGHT DATA) -----\n",
    "\n",
    "# Departure Performance\n",
    "#  'DEP_TIME', # Disregarded: redundant\n",
    "#  'DEP_DELAY', # Disregarded: other potentially useful target\n",
    "#  'DEP_DELAY_NEW', # Disregarded: redundant\n",
    "#  'DEP_DEL15', # Disregarded: other potentially useful target\n",
    "#  'DEP_DELAY_GROUP', # Disregarded: not relevant for this particular analysis\n",
    "#  'DEP_TIME_BLK', # Disregarded: redundant\n",
    "#  'TAXI_OUT', #  Output / However, the median for each airport could be used as input !! (explanation below)\n",
    "#  'WHEELS_OFF', # Disregarded: redundant\n",
    "# Arrival Performance\n",
    "#  'WHEELS_ON', # Disregarded: redundant\n",
    "#  'TAXI_IN', #  Output / However, the median for each airport could be used as input !! (explanation below)\n",
    "#  'ARR_TIME', # Disregarded: redundant\n",
    "#  'ARR_DELAY', # -------------------------------------------> MAIN TARGET !! (i.e. < y >)\n",
    "#  'ARR_DELAY_NEW', # Disregarded: redundant\n",
    " 'ARR_DEL15', # Disregarded: other potentially useful target\n",
    "#  'ARR_DELAY_GROUP', # Disregarded: not relevant for this particular analysis\n",
    "#  'ARR_TIME_BLK', # Disregarded: redundant\n",
    "# Cancellations and Diversions\n",
    "#  'CANCELLED', # Disregarded: not relevant for this particular analysis\n",
    "#  'CANCELLATION_CODE', # Disregarded: not relevant for this particular analysis\n",
    "#  'DIVERTED', # Disregarded: not relevant for this particular analysis\n",
    "# Flight Summaries\n",
    "#  'ACTUAL_ELAPSED_TIME', # Disregarded: redundant\n",
    "#  'AIR_TIME', # Disregarded: redundant\n",
    "# Cause of Delay\n",
    "#  'CARRIER_DELAY', # Disregarded: other potentially useful target\n",
    "#  'WEATHER_DELAY', # Disregarded: other potentially useful target\n",
    "#  'NAS_DELAY', # Disregarded: other potentially useful target\n",
    "#  'SECURITY_DELAY', # Disregarded: other potentially useful target\n",
    "#  'LATE_AIRCRAFT_DELAY', # Disregarded: other potentially useful target\n",
    "# Gate Return Information at Origin Airport (Data starts 10/2008)\n",
    "#  'FIRST_DEP_TIME', # Disregarded: not relevant for this particular analysis\n",
    "#  'TOTAL_ADD_GTIME', # Disregarded: not relevant for this particular analysis\n",
    "#  'LONGEST_ADD_GTIME', # Disregarded: not relevant for this particular analysis\n",
    "# Diverted Airport Information (Data starts 10/2008)\n",
    "#  'DIV_AIRPORT_LANDINGS', # Disregarded: not relevant for this particular analysis\n",
    "]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "     'MONTH',\n",
    "     'DAY_OF_MONTH',\n",
    "     'DAY_OF_WEEK',\n",
    "     'OP_UNIQUE_CARRIER',\n",
    "     'TAIL_NUM',\n",
    "     'ORIGIN_CITY_MARKET_ID',\n",
    "     'ORIGIN',\n",
    "     'DEST_CITY_MARKET_ID',\n",
    "     'DEST',\n",
    "     'CRS_DEP_TIME',\n",
    "     'DEP_TIME',\n",
    "     'TAXI_OUT_median',\n",
    "     'TAXI_IN_median',\n",
    "#      'DEP_DELAY',\n",
    "#      'DEP_DEL15',\n",
    "#      'DEP_DELAY_GROUP',\n",
    "#      'TAXI_OUT',\n",
    "#      'TAXI_IN',\n",
    "     'CRS_ARR_TIME',\n",
    "#      'ARR_TIME',\n",
    "#      'ARR_DELAY',\n",
    "     'ARR_DEL15',\n",
    "#      'ARR_DELAY_GROUP',\n",
    "#      'CANCELLED',\n",
    "     'CRS_ELAPSED_TIME',\n",
    "     'DISTANCE',\n",
    "     'DISTANCE_GROUP',\n",
    "#      'CARRIER_DELAY',\n",
    "#      'WEATHER_DELAY',\n",
    "#      'NAS_DELAY',\n",
    "#      'SECURITY_DELAY',\n",
    "#      'LATE_AIRCRAFT_DELAY',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\turge\\\\CompartidoVM\\\\0.TFM\\\\Output_Data\\\\US_DoT\\\\AL_OTP_MVP_Preprocessed_19_v2_clean.csv'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_input_csv_path = os.path.join(root,\n",
    "                                           \"Output_Data\",\n",
    "                                           \"US_DoT\",\n",
    "                                           \"AL_OTP_MVP_Preprocessed_19_v2_clean.csv\")\n",
    "preprocessed_input_csv_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 26.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_all = pd.read_csv(preprocessed_input_csv_path,\n",
    "                 encoding='latin1',\n",
    "#                  nrows=1e4,\n",
    "                 usecols=cols, # This way, the extra column is disregarded for the loading process\n",
    "                 low_memory = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7268232, 18)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 18), (7258232, 18))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_length = int(1e4)\n",
    "delayed = df_all[df_all['ARR_DEL15'] == 1].sample(sample_length // 2)\n",
    "not_delayed = df_all[df_all['ARR_DEL15'] == 0].sample(sample_length // 2)\n",
    "df = delayed.append(not_delayed)\n",
    "\n",
    "df_validation = df_all.loc[set(df_all.index) - set(df.index), :]\n",
    "\n",
    "df.shape, df_validation.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Dealing with categorical features with high cardinality: Target Encoding](https://medium.com/@kr.vishwesh54/dealing-with-categorical-features-with-high-cardinality-target-encoding-baa9298bf257)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY_OF_MONTH</th>\n",
       "      <th>DAY_OF_WEEK</th>\n",
       "      <th>OP_UNIQUE_CARRIER</th>\n",
       "      <th>TAIL_NUM</th>\n",
       "      <th>ORIGIN_CITY_MARKET_ID</th>\n",
       "      <th>ORIGIN</th>\n",
       "      <th>DEST_CITY_MARKET_ID</th>\n",
       "      <th>DEST</th>\n",
       "      <th>CRS_DEP_TIME</th>\n",
       "      <th>DEP_TIME</th>\n",
       "      <th>CRS_ARR_TIME</th>\n",
       "      <th>CRS_ELAPSED_TIME</th>\n",
       "      <th>DISTANCE</th>\n",
       "      <th>DISTANCE_GROUP</th>\n",
       "      <th>TAXI_OUT_median</th>\n",
       "      <th>TAXI_IN_median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>886514</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>9E</td>\n",
       "      <td>N335PQ</td>\n",
       "      <td>30257</td>\n",
       "      <td>ALB</td>\n",
       "      <td>31295</td>\n",
       "      <td>DTW</td>\n",
       "      <td>540</td>\n",
       "      <td>629.0</td>\n",
       "      <td>750</td>\n",
       "      <td>130.0</td>\n",
       "      <td>489.0</td>\n",
       "      <td>2</td>\n",
       "      <td>14.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2981528</th>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>MQ</td>\n",
       "      <td>N605KS</td>\n",
       "      <td>30599</td>\n",
       "      <td>BHM</td>\n",
       "      <td>32467</td>\n",
       "      <td>MIA</td>\n",
       "      <td>600</td>\n",
       "      <td>554.0</td>\n",
       "      <td>905</td>\n",
       "      <td>125.0</td>\n",
       "      <td>661.0</td>\n",
       "      <td>3</td>\n",
       "      <td>14.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6635664</th>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>AA</td>\n",
       "      <td>N938NN</td>\n",
       "      <td>33495</td>\n",
       "      <td>MSY</td>\n",
       "      <td>31057</td>\n",
       "      <td>CLT</td>\n",
       "      <td>1809</td>\n",
       "      <td>1816.0</td>\n",
       "      <td>2105</td>\n",
       "      <td>116.0</td>\n",
       "      <td>651.0</td>\n",
       "      <td>3</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265703</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>AS</td>\n",
       "      <td>N854VA</td>\n",
       "      <td>32457</td>\n",
       "      <td>SFO</td>\n",
       "      <td>32211</td>\n",
       "      <td>LAS</td>\n",
       "      <td>925</td>\n",
       "      <td>924.0</td>\n",
       "      <td>1100</td>\n",
       "      <td>95.0</td>\n",
       "      <td>414.0</td>\n",
       "      <td>2</td>\n",
       "      <td>20.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5552550</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>AA</td>\n",
       "      <td>N936NN</td>\n",
       "      <td>31650</td>\n",
       "      <td>MSP</td>\n",
       "      <td>30977</td>\n",
       "      <td>ORD</td>\n",
       "      <td>1410</td>\n",
       "      <td>1408.0</td>\n",
       "      <td>1549</td>\n",
       "      <td>99.0</td>\n",
       "      <td>334.0</td>\n",
       "      <td>2</td>\n",
       "      <td>16.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         MONTH  DAY_OF_MONTH  DAY_OF_WEEK OP_UNIQUE_CARRIER TAIL_NUM  \\\n",
       "886514       2             6            3                9E   N335PQ   \n",
       "2981528      6            23            7                MQ   N605KS   \n",
       "6635664     11            10            7                AA   N938NN   \n",
       "265703       1            17            4                AS   N854VA   \n",
       "5552550     10             4            5                AA   N936NN   \n",
       "\n",
       "         ORIGIN_CITY_MARKET_ID ORIGIN  DEST_CITY_MARKET_ID DEST  CRS_DEP_TIME  \\\n",
       "886514                   30257    ALB                31295  DTW           540   \n",
       "2981528                  30599    BHM                32467  MIA           600   \n",
       "6635664                  33495    MSY                31057  CLT          1809   \n",
       "265703                   32457    SFO                32211  LAS           925   \n",
       "5552550                  31650    MSP                30977  ORD          1410   \n",
       "\n",
       "         DEP_TIME  CRS_ARR_TIME  CRS_ELAPSED_TIME  DISTANCE  DISTANCE_GROUP  \\\n",
       "886514      629.0           750             130.0     489.0               2   \n",
       "2981528     554.0           905             125.0     661.0               3   \n",
       "6635664    1816.0          2105             116.0     651.0               3   \n",
       "265703      924.0          1100              95.0     414.0               2   \n",
       "5552550    1408.0          1549              99.0     334.0               2   \n",
       "\n",
       "         TAXI_OUT_median  TAXI_IN_median  \n",
       "886514              14.0             8.0  \n",
       "2981528             14.0             5.0  \n",
       "6635664             14.0             9.0  \n",
       "265703              20.0             7.0  \n",
       "5552550             16.0            14.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.drop('ARR_DEL15', axis=1)\n",
    "X.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3405170    0.0\n",
       "1107423    0.0\n",
       "5274686    1.0\n",
       "4296801    0.0\n",
       "238581     0.0\n",
       "Name: ARR_DEL15, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df['ARR_DEL15']\n",
    "y.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_freq(col='', df=df):\n",
    "    i = 0\n",
    "    for v in df[col].value_counts().sort_index():\n",
    "        print(\"{} : {} records ({:.2f}%)\" \\\n",
    "              .format(df[col].value_counts().sort_index().index[i], v,  v / len(df) * 100))\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONTH : 12 unique values\n",
      "1 : 777 records (7.77%)\n",
      "2 : 801 records (8.01%)\n",
      "3 : 810 records (8.10%)\n",
      "4 : 793 records (7.93%)\n",
      "5 : 900 records (9.00%)\n",
      "6 : 940 records (9.40%)\n",
      "7 : 903 records (9.03%)\n",
      "8 : 885 records (8.85%)\n",
      "9 : 749 records (7.49%)\n",
      "10 : 820 records (8.20%)\n",
      "11 : 772 records (7.72%)\n",
      "12 : 850 records (8.50%)\n",
      "\n",
      "DAY_OF_MONTH : 31 unique values\n",
      "1 : 338 records (3.38%)\n",
      "2 : 328 records (3.28%)\n",
      "3 : 277 records (2.77%)\n",
      "4 : 300 records (3.00%)\n",
      "5 : 267 records (2.67%)\n",
      "6 : 321 records (3.21%)\n",
      "7 : 330 records (3.30%)\n",
      "8 : 338 records (3.38%)\n",
      "9 : 333 records (3.33%)\n",
      "10 : 345 records (3.45%)\n",
      "11 : 361 records (3.61%)\n",
      "12 : 309 records (3.09%)\n",
      "13 : 348 records (3.48%)\n",
      "14 : 309 records (3.09%)\n",
      "15 : 330 records (3.30%)\n",
      "16 : 317 records (3.17%)\n",
      "17 : 341 records (3.41%)\n",
      "18 : 358 records (3.58%)\n",
      "19 : 346 records (3.46%)\n",
      "20 : 391 records (3.91%)\n",
      "21 : 355 records (3.55%)\n",
      "22 : 346 records (3.46%)\n",
      "23 : 304 records (3.04%)\n",
      "24 : 306 records (3.06%)\n",
      "25 : 315 records (3.15%)\n",
      "26 : 317 records (3.17%)\n",
      "27 : 344 records (3.44%)\n",
      "28 : 310 records (3.10%)\n",
      "29 : 317 records (3.17%)\n",
      "30 : 315 records (3.15%)\n",
      "31 : 184 records (1.84%)\n",
      "\n",
      "DAY_OF_WEEK : 7 unique values\n",
      "1 : 1521 records (15.21%)\n",
      "2 : 1431 records (14.31%)\n",
      "3 : 1417 records (14.17%)\n",
      "4 : 1520 records (15.20%)\n",
      "5 : 1510 records (15.10%)\n",
      "6 : 1135 records (11.35%)\n",
      "7 : 1466 records (14.66%)\n",
      "\n",
      "OP_UNIQUE_CARRIER : 17 unique values\n",
      "9E : 355 records (3.55%)\n",
      "AA : 1360 records (13.60%)\n",
      "AS : 361 records (3.61%)\n",
      "B6 : 458 records (4.58%)\n",
      "DL : 1213 records (12.13%)\n",
      "EV : 208 records (2.08%)\n",
      "F9 : 208 records (2.08%)\n",
      "G4 : 149 records (1.49%)\n",
      "HA : 109 records (1.09%)\n",
      "MQ : 448 records (4.48%)\n",
      "NK : 294 records (2.94%)\n",
      "OH : 390 records (3.90%)\n",
      "OO : 1096 records (10.96%)\n",
      "UA : 861 records (8.61%)\n",
      "WN : 1730 records (17.30%)\n",
      "YV : 327 records (3.27%)\n",
      "YX : 433 records (4.33%)\n",
      "\n",
      "TAIL_NUM : 4398 unique values\n",
      "\n",
      "ORIGIN_CITY_MARKET_ID : 275 unique values\n",
      "\n",
      "ORIGIN : 299 unique values\n",
      "\n",
      "DEST_CITY_MARKET_ID : 279 unique values\n",
      "\n",
      "DEST : 302 unique values\n",
      "\n",
      "CRS_DEP_TIME : 1073 unique values\n",
      "\n",
      "DEP_TIME : 1215 unique values\n",
      "\n",
      "CRS_ARR_TIME : 1188 unique values\n",
      "\n",
      "ARR_DEL15 : 2 unique values\n",
      "0.0 : 5000 records (50.00%)\n",
      "1.0 : 5000 records (50.00%)\n",
      "\n",
      "CRS_ELAPSED_TIME : 389 unique values\n",
      "\n",
      "DISTANCE : 1270 unique values\n",
      "\n",
      "DISTANCE_GROUP : 11 unique values\n",
      "1 : 1230 records (12.30%)\n",
      "2 : 2424 records (24.24%)\n",
      "3 : 1980 records (19.80%)\n",
      "4 : 1552 records (15.52%)\n",
      "5 : 1118 records (11.18%)\n",
      "6 : 452 records (4.52%)\n",
      "7 : 415 records (4.15%)\n",
      "8 : 237 records (2.37%)\n",
      "9 : 157 records (1.57%)\n",
      "10 : 243 records (2.43%)\n",
      "11 : 192 records (1.92%)\n",
      "\n",
      "TAXI_OUT_median : 28 unique values\n",
      "6.0 : 1 records (0.01%)\n",
      "7.0 : 3 records (0.03%)\n",
      "8.0 : 84 records (0.84%)\n",
      "9.0 : 361 records (3.61%)\n",
      "9.5 : 1 records (0.01%)\n",
      "10.0 : 683 records (6.83%)\n",
      "11.0 : 529 records (5.29%)\n",
      "12.0 : 827 records (8.27%)\n",
      "12.5 : 4 records (0.04%)\n",
      "13.0 : 778 records (7.78%)\n",
      "13.5 : 3 records (0.03%)\n",
      "14.0 : 919 records (9.19%)\n",
      "15.0 : 999 records (9.99%)\n",
      "16.0 : 1266 records (12.66%)\n",
      "16.5 : 3 records (0.03%)\n",
      "17.0 : 949 records (9.49%)\n",
      "17.5 : 2 records (0.02%)\n",
      "18.0 : 437 records (4.37%)\n",
      "19.0 : 521 records (5.21%)\n",
      "20.0 : 508 records (5.08%)\n",
      "21.0 : 457 records (4.57%)\n",
      "22.0 : 284 records (2.84%)\n",
      "23.0 : 189 records (1.89%)\n",
      "24.0 : 106 records (1.06%)\n",
      "25.0 : 41 records (0.41%)\n",
      "26.0 : 10 records (0.10%)\n",
      "26.5 : 1 records (0.01%)\n",
      "27.0 : 34 records (0.34%)\n",
      "\n",
      "TAXI_IN_median : 17 unique values\n",
      "2.0 : 40 records (0.40%)\n",
      "3.0 : 612 records (6.12%)\n",
      "3.5 : 5 records (0.05%)\n",
      "4.0 : 1709 records (17.09%)\n",
      "5.0 : 1685 records (16.85%)\n",
      "5.5 : 4 records (0.04%)\n",
      "6.0 : 1375 records (13.75%)\n",
      "6.5 : 2 records (0.02%)\n",
      "7.0 : 1008 records (10.08%)\n",
      "8.0 : 1260 records (12.60%)\n",
      "9.0 : 879 records (8.79%)\n",
      "10.0 : 322 records (3.22%)\n",
      "11.0 : 419 records (4.19%)\n",
      "12.0 : 219 records (2.19%)\n",
      "13.0 : 341 records (3.41%)\n",
      "14.0 : 114 records (1.14%)\n",
      "15.0 : 6 records (0.06%)\n",
      "\n",
      "Wall time: 284 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for col in df.columns:\n",
    "    print(col, ':', df[col].nunique(), 'unique values')\n",
    "    if df[col].nunique() < 50:\n",
    "        val_freq(col)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for col in X.columns:\n",
    "    a = df.groupby([col], as_index=False).agg(['sum'])['ARR_DEL15']\n",
    "    a.plot(legend=True)\n",
    "    plt.title(col)\n",
    "    b = df.groupby([col], as_index=False).agg(['count'])['ARR_DEL15']\n",
    "    b.plot(legend=True)\n",
    "    plt.title(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusiones:\n",
    "- En junio se concentra el mayor número de retrasos. Curiosamente, hay notablemente más vuelos en mayo, julio, agosto y octubre.\n",
    "- Los D, L, X y J se concentra el mayor número de retrasos (especialmente X). D y X es cuando más vuelos hay con diferencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 2247.82 MiB, increment: 0.13 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Simple Guide to Scikit-learn Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before building the pipeline, let's split the data into a train and test set so that the performance of the model can be validated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The first step in building the pipeline is to define each transformer type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 181 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Next, let's use the ColumnTransformer to apply the transformations to the correct columns in the dataframe. Before building this, the numeric and categorical columns shall be listed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 16 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "numeric_features = df.select_dtypes(include=['int64', 'float64']).drop(['ARR_DEL15'], axis=1).columns\n",
    "categorical_features = df.select_dtypes(include=['object']).columns\n",
    "from sklearn.compose import ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create a pipeline that combines the preprocessor created above with a classifier. In this case a simple RandomForestClassifier has been used to start with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', RandomForestClassifier(random_state=0))])\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A pipeline can also be used during the model selection process**. The following example code loops through a number of scikit-learn classifiers applying the transformations and training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(n_neighbors=3)\n",
      "model score: 0.572\n",
      "0.572264836495761 0.5698492462311557 0.5738866396761133 0.5658682634730539 0.572\n",
      "[[577 435]\n",
      " [421 567]] \n",
      "\n",
      "SVC(C=0.025, probability=True, random_state=0)\n",
      "model score: 0.597\n",
      "0.6627680311890838 0.6278855032317635 0.6882591093117408 0.5772495755517827 0.597\n",
      "[[514 498]\n",
      " [308 680]] \n",
      "\n",
      "DecisionTreeClassifier(random_state=0)\n",
      "model score: 0.799\n",
      "0.7931034482758621 0.7955239064089522 0.791497975708502 0.7995910020449898 0.799\n",
      "[[816 196]\n",
      " [206 782]] \n",
      "\n",
      "RandomForestClassifier(random_state=0)\n",
      "model score: 0.638\n",
      "0.664210316669986 0.6485172581429266 0.6751012145748988 0.6239476145930777 0.6385\n",
      "[[610 402]\n",
      " [321 667]] \n",
      "\n",
      "AdaBoostClassifier(random_state=0)\n",
      "model score: 0.657\n",
      "0.638514891880865 0.6460268317853457 0.6336032388663968 0.6589473684210526 0.657\n",
      "[[688 324]\n",
      " [362 626]] \n",
      "\n",
      "GradientBoostingClassifier(random_state=0)\n",
      "model score: 0.794\n",
      "0.7157961514062171 0.7671388101983004 0.6852226720647774 0.8712998712998713 0.7945\n",
      "[[912 100]\n",
      " [311 677]] \n",
      "\n",
      "Wall time: 43.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.metrics import accuracy_score, log_loss, f1_score, fbeta_score, recall_score, precision_score, roc_auc_score, roc_curve, confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"rbf\", C=0.025, probability=True, random_state=0),\n",
    "    DecisionTreeClassifier(random_state=0),\n",
    "    RandomForestClassifier(random_state=0),\n",
    "    AdaBoostClassifier(random_state=0),\n",
    "    GradientBoostingClassifier(random_state=0)\n",
    "    ]\n",
    "\n",
    "for classifier in classifiers:\n",
    "    pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', classifier)])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    print(classifier)\n",
    "    print(\"model score: %.3f\" % pipe.score(X_test, y_test))    \n",
    "    # TEST !!!     \n",
    "    predictions = pipe.predict(X_test)\n",
    "    print(fbeta_score(y_test, predictions, beta=2), f1_score(y_test, predictions), recall_score(y_test, predictions), precision_score(y_test, predictions), accuracy_score(y_test, predictions))\n",
    "    print(confusion_matrix(y_test, predictions), \"\\n\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The pipeline can also be used in grid search** to find the best performing parameters. To do this, let's first create a parameter grid for the chosen model.\n",
    "\n",
    "*One important thing to note is that there is a need to append the name given to the classifier part of the pipeline to each parameter name. In the code above its name is ‘classifier’ so 'classifier__' has been added to each parameter.*\n",
    "\n",
    "Next a grid search object has been created, which includes the original pipeline. When fit is called, the transformations are applied to the data, before a cross-validated grid-search is performed over the parameter grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['explained_variance', 'r2', 'max_error', 'neg_median_absolute_error', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_root_mean_squared_error', 'neg_mean_poisson_deviance', 'neg_mean_gamma_deviance', 'accuracy', 'roc_auc', 'roc_auc_ovr', 'roc_auc_ovo', 'roc_auc_ovr_weighted', 'roc_auc_ovo_weighted', 'balanced_accuracy', 'average_precision', 'neg_log_loss', 'neg_brier_score', 'adjusted_rand_score', 'homogeneity_score', 'completeness_score', 'v_measure_score', 'mutual_info_score', 'adjusted_mutual_info_score', 'normalized_mutual_info_score', 'fowlkes_mallows_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'jaccard', 'jaccard_macro', 'jaccard_micro', 'jaccard_samples', 'jaccard_weighted'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.metrics.SCORERS.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier__criterion': 'entropy', 'classifier__max_depth': 4, 'classifier__max_features': 'log2', 'classifier__n_estimators': 500}\n",
      "0.646832010271799\n",
      "Wall time: 6min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "param_grid = { \n",
    "    'classifier__n_estimators': [200, 500],\n",
    "    'classifier__max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'classifier__max_depth' : [4,5,6,7,8],\n",
    "    'classifier__criterion' :['gini', 'entropy']}\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "scoring = {'AUC': 'roc_auc', 'F1': 'f1', 'Precision': 'precision',\n",
    "           'Recall': 'recall', 'Accuracy': 'accuracy'}\n",
    "\n",
    "CV = GridSearchCV(rf, param_grid, n_jobs= 1, scoring=scoring, refit='F1')\n",
    "# Setting refit='AUC', refits an estimator on the whole dataset with the\n",
    "# parameter setting that has the best cross-validated AUC score.\n",
    "# That estimator is made available at ``gs.best_estimator_`` along with\n",
    "# parameters like ``gs.best_score_``, ``gs.best_params_`` and\n",
    "# ``gs.best_index_``\n",
    "\n",
    "CV.fit(X_train, y_train)  \n",
    "print(CV.best_params_)    \n",
    "print(CV.best_score_)\n",
    "CVscores = pd.DataFrame(CV.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_classifier__criterion</th>\n",
       "      <th>param_classifier__max_depth</th>\n",
       "      <th>param_classifier__max_features</th>\n",
       "      <th>param_classifier__n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_AUC</th>\n",
       "      <th>split1_test_AUC</th>\n",
       "      <th>split2_test_AUC</th>\n",
       "      <th>split3_test_AUC</th>\n",
       "      <th>split4_test_AUC</th>\n",
       "      <th>mean_test_AUC</th>\n",
       "      <th>std_test_AUC</th>\n",
       "      <th>rank_test_AUC</th>\n",
       "      <th>split0_test_F1</th>\n",
       "      <th>split1_test_F1</th>\n",
       "      <th>split2_test_F1</th>\n",
       "      <th>split3_test_F1</th>\n",
       "      <th>split4_test_F1</th>\n",
       "      <th>mean_test_F1</th>\n",
       "      <th>std_test_F1</th>\n",
       "      <th>rank_test_F1</th>\n",
       "      <th>split0_test_Precision</th>\n",
       "      <th>split1_test_Precision</th>\n",
       "      <th>split2_test_Precision</th>\n",
       "      <th>split3_test_Precision</th>\n",
       "      <th>split4_test_Precision</th>\n",
       "      <th>mean_test_Precision</th>\n",
       "      <th>std_test_Precision</th>\n",
       "      <th>rank_test_Precision</th>\n",
       "      <th>split0_test_Recall</th>\n",
       "      <th>split1_test_Recall</th>\n",
       "      <th>split2_test_Recall</th>\n",
       "      <th>split3_test_Recall</th>\n",
       "      <th>split4_test_Recall</th>\n",
       "      <th>mean_test_Recall</th>\n",
       "      <th>std_test_Recall</th>\n",
       "      <th>rank_test_Recall</th>\n",
       "      <th>split0_test_Accuracy</th>\n",
       "      <th>split1_test_Accuracy</th>\n",
       "      <th>split2_test_Accuracy</th>\n",
       "      <th>split3_test_Accuracy</th>\n",
       "      <th>split4_test_Accuracy</th>\n",
       "      <th>mean_test_Accuracy</th>\n",
       "      <th>std_test_Accuracy</th>\n",
       "      <th>rank_test_Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.614637</td>\n",
       "      <td>0.012429</td>\n",
       "      <td>0.133190</td>\n",
       "      <td>0.000975</td>\n",
       "      <td>gini</td>\n",
       "      <td>4</td>\n",
       "      <td>auto</td>\n",
       "      <td>200</td>\n",
       "      <td>{'classifier__criterion': 'gini', 'classifier_...</td>\n",
       "      <td>0.613044</td>\n",
       "      <td>0.642552</td>\n",
       "      <td>0.605176</td>\n",
       "      <td>0.625003</td>\n",
       "      <td>0.637132</td>\n",
       "      <td>0.624581</td>\n",
       "      <td>0.014074</td>\n",
       "      <td>37</td>\n",
       "      <td>0.620728</td>\n",
       "      <td>0.641360</td>\n",
       "      <td>0.619830</td>\n",
       "      <td>0.644701</td>\n",
       "      <td>0.624856</td>\n",
       "      <td>0.630295</td>\n",
       "      <td>0.010588</td>\n",
       "      <td>37</td>\n",
       "      <td>0.563581</td>\n",
       "      <td>0.587747</td>\n",
       "      <td>0.568017</td>\n",
       "      <td>0.576621</td>\n",
       "      <td>0.580749</td>\n",
       "      <td>0.575343</td>\n",
       "      <td>0.008680</td>\n",
       "      <td>39</td>\n",
       "      <td>0.690773</td>\n",
       "      <td>0.705736</td>\n",
       "      <td>0.682045</td>\n",
       "      <td>0.731009</td>\n",
       "      <td>0.676214</td>\n",
       "      <td>0.697155</td>\n",
       "      <td>0.019633</td>\n",
       "      <td>29</td>\n",
       "      <td>0.576875</td>\n",
       "      <td>0.604375</td>\n",
       "      <td>0.580625</td>\n",
       "      <td>0.595625</td>\n",
       "      <td>0.592500</td>\n",
       "      <td>0.590000</td>\n",
       "      <td>0.010047</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.414895</td>\n",
       "      <td>0.052682</td>\n",
       "      <td>0.278257</td>\n",
       "      <td>0.001220</td>\n",
       "      <td>gini</td>\n",
       "      <td>4</td>\n",
       "      <td>auto</td>\n",
       "      <td>500</td>\n",
       "      <td>{'classifier__criterion': 'gini', 'classifier_...</td>\n",
       "      <td>0.611952</td>\n",
       "      <td>0.642968</td>\n",
       "      <td>0.609313</td>\n",
       "      <td>0.624326</td>\n",
       "      <td>0.636976</td>\n",
       "      <td>0.625107</td>\n",
       "      <td>0.013289</td>\n",
       "      <td>35</td>\n",
       "      <td>0.621576</td>\n",
       "      <td>0.639047</td>\n",
       "      <td>0.621820</td>\n",
       "      <td>0.639600</td>\n",
       "      <td>0.626659</td>\n",
       "      <td>0.629740</td>\n",
       "      <td>0.008033</td>\n",
       "      <td>43</td>\n",
       "      <td>0.563323</td>\n",
       "      <td>0.586458</td>\n",
       "      <td>0.568769</td>\n",
       "      <td>0.577889</td>\n",
       "      <td>0.583871</td>\n",
       "      <td>0.576062</td>\n",
       "      <td>0.008808</td>\n",
       "      <td>35</td>\n",
       "      <td>0.693267</td>\n",
       "      <td>0.701995</td>\n",
       "      <td>0.685786</td>\n",
       "      <td>0.716065</td>\n",
       "      <td>0.676214</td>\n",
       "      <td>0.694665</td>\n",
       "      <td>0.013660</td>\n",
       "      <td>39</td>\n",
       "      <td>0.576875</td>\n",
       "      <td>0.602500</td>\n",
       "      <td>0.581875</td>\n",
       "      <td>0.595000</td>\n",
       "      <td>0.595625</td>\n",
       "      <td>0.590375</td>\n",
       "      <td>0.009492</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.604426</td>\n",
       "      <td>0.002314</td>\n",
       "      <td>0.133517</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>gini</td>\n",
       "      <td>4</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>200</td>\n",
       "      <td>{'classifier__criterion': 'gini', 'classifier_...</td>\n",
       "      <td>0.613044</td>\n",
       "      <td>0.642552</td>\n",
       "      <td>0.605176</td>\n",
       "      <td>0.625003</td>\n",
       "      <td>0.637132</td>\n",
       "      <td>0.624581</td>\n",
       "      <td>0.014074</td>\n",
       "      <td>37</td>\n",
       "      <td>0.620728</td>\n",
       "      <td>0.641360</td>\n",
       "      <td>0.619830</td>\n",
       "      <td>0.644701</td>\n",
       "      <td>0.624856</td>\n",
       "      <td>0.630295</td>\n",
       "      <td>0.010588</td>\n",
       "      <td>37</td>\n",
       "      <td>0.563581</td>\n",
       "      <td>0.587747</td>\n",
       "      <td>0.568017</td>\n",
       "      <td>0.576621</td>\n",
       "      <td>0.580749</td>\n",
       "      <td>0.575343</td>\n",
       "      <td>0.008680</td>\n",
       "      <td>39</td>\n",
       "      <td>0.690773</td>\n",
       "      <td>0.705736</td>\n",
       "      <td>0.682045</td>\n",
       "      <td>0.731009</td>\n",
       "      <td>0.676214</td>\n",
       "      <td>0.697155</td>\n",
       "      <td>0.019633</td>\n",
       "      <td>29</td>\n",
       "      <td>0.576875</td>\n",
       "      <td>0.604375</td>\n",
       "      <td>0.580625</td>\n",
       "      <td>0.595625</td>\n",
       "      <td>0.592500</td>\n",
       "      <td>0.590000</td>\n",
       "      <td>0.010047</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.405861</td>\n",
       "      <td>0.033879</td>\n",
       "      <td>0.277828</td>\n",
       "      <td>0.002096</td>\n",
       "      <td>gini</td>\n",
       "      <td>4</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>500</td>\n",
       "      <td>{'classifier__criterion': 'gini', 'classifier_...</td>\n",
       "      <td>0.611952</td>\n",
       "      <td>0.642968</td>\n",
       "      <td>0.609313</td>\n",
       "      <td>0.624326</td>\n",
       "      <td>0.636976</td>\n",
       "      <td>0.625107</td>\n",
       "      <td>0.013289</td>\n",
       "      <td>35</td>\n",
       "      <td>0.621576</td>\n",
       "      <td>0.639047</td>\n",
       "      <td>0.621820</td>\n",
       "      <td>0.639600</td>\n",
       "      <td>0.626659</td>\n",
       "      <td>0.629740</td>\n",
       "      <td>0.008033</td>\n",
       "      <td>43</td>\n",
       "      <td>0.563323</td>\n",
       "      <td>0.586458</td>\n",
       "      <td>0.568769</td>\n",
       "      <td>0.577889</td>\n",
       "      <td>0.583871</td>\n",
       "      <td>0.576062</td>\n",
       "      <td>0.008808</td>\n",
       "      <td>35</td>\n",
       "      <td>0.693267</td>\n",
       "      <td>0.701995</td>\n",
       "      <td>0.685786</td>\n",
       "      <td>0.716065</td>\n",
       "      <td>0.676214</td>\n",
       "      <td>0.694665</td>\n",
       "      <td>0.013660</td>\n",
       "      <td>39</td>\n",
       "      <td>0.576875</td>\n",
       "      <td>0.602500</td>\n",
       "      <td>0.581875</td>\n",
       "      <td>0.595000</td>\n",
       "      <td>0.595625</td>\n",
       "      <td>0.590375</td>\n",
       "      <td>0.009492</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.552373</td>\n",
       "      <td>0.027854</td>\n",
       "      <td>0.132140</td>\n",
       "      <td>0.003648</td>\n",
       "      <td>gini</td>\n",
       "      <td>4</td>\n",
       "      <td>log2</td>\n",
       "      <td>200</td>\n",
       "      <td>{'classifier__criterion': 'gini', 'classifier_...</td>\n",
       "      <td>0.591492</td>\n",
       "      <td>0.626086</td>\n",
       "      <td>0.591423</td>\n",
       "      <td>0.609137</td>\n",
       "      <td>0.611161</td>\n",
       "      <td>0.605860</td>\n",
       "      <td>0.013136</td>\n",
       "      <td>57</td>\n",
       "      <td>0.621681</td>\n",
       "      <td>0.652861</td>\n",
       "      <td>0.632708</td>\n",
       "      <td>0.645092</td>\n",
       "      <td>0.626440</td>\n",
       "      <td>0.635756</td>\n",
       "      <td>0.011610</td>\n",
       "      <td>8</td>\n",
       "      <td>0.558648</td>\n",
       "      <td>0.579864</td>\n",
       "      <td>0.555033</td>\n",
       "      <td>0.568982</td>\n",
       "      <td>0.559804</td>\n",
       "      <td>0.564466</td>\n",
       "      <td>0.008967</td>\n",
       "      <td>58</td>\n",
       "      <td>0.700748</td>\n",
       "      <td>0.746883</td>\n",
       "      <td>0.735661</td>\n",
       "      <td>0.744707</td>\n",
       "      <td>0.711083</td>\n",
       "      <td>0.727817</td>\n",
       "      <td>0.018564</td>\n",
       "      <td>6</td>\n",
       "      <td>0.572500</td>\n",
       "      <td>0.601875</td>\n",
       "      <td>0.571875</td>\n",
       "      <td>0.588750</td>\n",
       "      <td>0.574375</td>\n",
       "      <td>0.581875</td>\n",
       "      <td>0.011759</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.248124</td>\n",
       "      <td>0.031093</td>\n",
       "      <td>0.289824</td>\n",
       "      <td>0.031066</td>\n",
       "      <td>gini</td>\n",
       "      <td>4</td>\n",
       "      <td>log2</td>\n",
       "      <td>500</td>\n",
       "      <td>{'classifier__criterion': 'gini', 'classifier_...</td>\n",
       "      <td>0.601905</td>\n",
       "      <td>0.635799</td>\n",
       "      <td>0.599987</td>\n",
       "      <td>0.618952</td>\n",
       "      <td>0.620361</td>\n",
       "      <td>0.615401</td>\n",
       "      <td>0.013214</td>\n",
       "      <td>47</td>\n",
       "      <td>0.636853</td>\n",
       "      <td>0.660657</td>\n",
       "      <td>0.625478</td>\n",
       "      <td>0.645301</td>\n",
       "      <td>0.652654</td>\n",
       "      <td>0.644189</td>\n",
       "      <td>0.012231</td>\n",
       "      <td>2</td>\n",
       "      <td>0.560721</td>\n",
       "      <td>0.574723</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.572254</td>\n",
       "      <td>0.564545</td>\n",
       "      <td>0.565841</td>\n",
       "      <td>0.006734</td>\n",
       "      <td>57</td>\n",
       "      <td>0.736908</td>\n",
       "      <td>0.776808</td>\n",
       "      <td>0.713217</td>\n",
       "      <td>0.739726</td>\n",
       "      <td>0.773350</td>\n",
       "      <td>0.748002</td>\n",
       "      <td>0.023974</td>\n",
       "      <td>2</td>\n",
       "      <td>0.578750</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.571875</td>\n",
       "      <td>0.591875</td>\n",
       "      <td>0.586875</td>\n",
       "      <td>0.585875</td>\n",
       "      <td>0.009831</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.637909</td>\n",
       "      <td>0.006190</td>\n",
       "      <td>0.134833</td>\n",
       "      <td>0.000673</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>auto</td>\n",
       "      <td>200</td>\n",
       "      <td>{'classifier__criterion': 'gini', 'classifier_...</td>\n",
       "      <td>0.611729</td>\n",
       "      <td>0.641802</td>\n",
       "      <td>0.610959</td>\n",
       "      <td>0.632507</td>\n",
       "      <td>0.644059</td>\n",
       "      <td>0.628211</td>\n",
       "      <td>0.014308</td>\n",
       "      <td>23</td>\n",
       "      <td>0.620728</td>\n",
       "      <td>0.641084</td>\n",
       "      <td>0.622524</td>\n",
       "      <td>0.648498</td>\n",
       "      <td>0.628704</td>\n",
       "      <td>0.632308</td>\n",
       "      <td>0.010788</td>\n",
       "      <td>21</td>\n",
       "      <td>0.563581</td>\n",
       "      <td>0.585567</td>\n",
       "      <td>0.569948</td>\n",
       "      <td>0.585930</td>\n",
       "      <td>0.589325</td>\n",
       "      <td>0.578870</td>\n",
       "      <td>0.010172</td>\n",
       "      <td>15</td>\n",
       "      <td>0.690773</td>\n",
       "      <td>0.708229</td>\n",
       "      <td>0.685786</td>\n",
       "      <td>0.726027</td>\n",
       "      <td>0.673724</td>\n",
       "      <td>0.696908</td>\n",
       "      <td>0.018303</td>\n",
       "      <td>31</td>\n",
       "      <td>0.576875</td>\n",
       "      <td>0.602500</td>\n",
       "      <td>0.583125</td>\n",
       "      <td>0.605000</td>\n",
       "      <td>0.600625</td>\n",
       "      <td>0.593625</td>\n",
       "      <td>0.011384</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.483945</td>\n",
       "      <td>0.025338</td>\n",
       "      <td>0.285308</td>\n",
       "      <td>0.003228</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>auto</td>\n",
       "      <td>500</td>\n",
       "      <td>{'classifier__criterion': 'gini', 'classifier_...</td>\n",
       "      <td>0.612840</td>\n",
       "      <td>0.646523</td>\n",
       "      <td>0.612276</td>\n",
       "      <td>0.630623</td>\n",
       "      <td>0.642453</td>\n",
       "      <td>0.628943</td>\n",
       "      <td>0.014363</td>\n",
       "      <td>17</td>\n",
       "      <td>0.619608</td>\n",
       "      <td>0.643301</td>\n",
       "      <td>0.621668</td>\n",
       "      <td>0.640625</td>\n",
       "      <td>0.632065</td>\n",
       "      <td>0.631453</td>\n",
       "      <td>0.009601</td>\n",
       "      <td>30</td>\n",
       "      <td>0.562564</td>\n",
       "      <td>0.588418</td>\n",
       "      <td>0.570239</td>\n",
       "      <td>0.580384</td>\n",
       "      <td>0.588614</td>\n",
       "      <td>0.578044</td>\n",
       "      <td>0.010251</td>\n",
       "      <td>21</td>\n",
       "      <td>0.689526</td>\n",
       "      <td>0.709476</td>\n",
       "      <td>0.683292</td>\n",
       "      <td>0.714819</td>\n",
       "      <td>0.682441</td>\n",
       "      <td>0.695911</td>\n",
       "      <td>0.013587</td>\n",
       "      <td>33</td>\n",
       "      <td>0.575625</td>\n",
       "      <td>0.605625</td>\n",
       "      <td>0.583125</td>\n",
       "      <td>0.597500</td>\n",
       "      <td>0.601250</td>\n",
       "      <td>0.592625</td>\n",
       "      <td>0.011370</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.634473</td>\n",
       "      <td>0.003329</td>\n",
       "      <td>0.135550</td>\n",
       "      <td>0.001229</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>200</td>\n",
       "      <td>{'classifier__criterion': 'gini', 'classifier_...</td>\n",
       "      <td>0.611729</td>\n",
       "      <td>0.641802</td>\n",
       "      <td>0.610959</td>\n",
       "      <td>0.632507</td>\n",
       "      <td>0.644059</td>\n",
       "      <td>0.628211</td>\n",
       "      <td>0.014308</td>\n",
       "      <td>23</td>\n",
       "      <td>0.620728</td>\n",
       "      <td>0.641084</td>\n",
       "      <td>0.622524</td>\n",
       "      <td>0.648498</td>\n",
       "      <td>0.628704</td>\n",
       "      <td>0.632308</td>\n",
       "      <td>0.010788</td>\n",
       "      <td>21</td>\n",
       "      <td>0.563581</td>\n",
       "      <td>0.585567</td>\n",
       "      <td>0.569948</td>\n",
       "      <td>0.585930</td>\n",
       "      <td>0.589325</td>\n",
       "      <td>0.578870</td>\n",
       "      <td>0.010172</td>\n",
       "      <td>15</td>\n",
       "      <td>0.690773</td>\n",
       "      <td>0.708229</td>\n",
       "      <td>0.685786</td>\n",
       "      <td>0.726027</td>\n",
       "      <td>0.673724</td>\n",
       "      <td>0.696908</td>\n",
       "      <td>0.018303</td>\n",
       "      <td>31</td>\n",
       "      <td>0.576875</td>\n",
       "      <td>0.602500</td>\n",
       "      <td>0.583125</td>\n",
       "      <td>0.605000</td>\n",
       "      <td>0.600625</td>\n",
       "      <td>0.593625</td>\n",
       "      <td>0.011384</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.492746</td>\n",
       "      <td>0.044471</td>\n",
       "      <td>0.293618</td>\n",
       "      <td>0.011679</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>500</td>\n",
       "      <td>{'classifier__criterion': 'gini', 'classifier_...</td>\n",
       "      <td>0.612840</td>\n",
       "      <td>0.646523</td>\n",
       "      <td>0.612276</td>\n",
       "      <td>0.630623</td>\n",
       "      <td>0.642453</td>\n",
       "      <td>0.628943</td>\n",
       "      <td>0.014363</td>\n",
       "      <td>17</td>\n",
       "      <td>0.619608</td>\n",
       "      <td>0.643301</td>\n",
       "      <td>0.621668</td>\n",
       "      <td>0.640625</td>\n",
       "      <td>0.632065</td>\n",
       "      <td>0.631453</td>\n",
       "      <td>0.009601</td>\n",
       "      <td>30</td>\n",
       "      <td>0.562564</td>\n",
       "      <td>0.588418</td>\n",
       "      <td>0.570239</td>\n",
       "      <td>0.580384</td>\n",
       "      <td>0.588614</td>\n",
       "      <td>0.578044</td>\n",
       "      <td>0.010251</td>\n",
       "      <td>21</td>\n",
       "      <td>0.689526</td>\n",
       "      <td>0.709476</td>\n",
       "      <td>0.683292</td>\n",
       "      <td>0.714819</td>\n",
       "      <td>0.682441</td>\n",
       "      <td>0.695911</td>\n",
       "      <td>0.013587</td>\n",
       "      <td>33</td>\n",
       "      <td>0.575625</td>\n",
       "      <td>0.605625</td>\n",
       "      <td>0.583125</td>\n",
       "      <td>0.597500</td>\n",
       "      <td>0.601250</td>\n",
       "      <td>0.592625</td>\n",
       "      <td>0.011370</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.544089</td>\n",
       "      <td>0.003051</td>\n",
       "      <td>0.132517</td>\n",
       "      <td>0.000799</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>log2</td>\n",
       "      <td>200</td>\n",
       "      <td>{'classifier__criterion': 'gini', 'classifier_...</td>\n",
       "      <td>0.586392</td>\n",
       "      <td>0.623357</td>\n",
       "      <td>0.587444</td>\n",
       "      <td>0.610341</td>\n",
       "      <td>0.613508</td>\n",
       "      <td>0.604209</td>\n",
       "      <td>0.014760</td>\n",
       "      <td>60</td>\n",
       "      <td>0.614782</td>\n",
       "      <td>0.641180</td>\n",
       "      <td>0.621978</td>\n",
       "      <td>0.638600</td>\n",
       "      <td>0.619765</td>\n",
       "      <td>0.627261</td>\n",
       "      <td>0.010603</td>\n",
       "      <td>55</td>\n",
       "      <td>0.557927</td>\n",
       "      <td>0.570457</td>\n",
       "      <td>0.555992</td>\n",
       "      <td>0.569201</td>\n",
       "      <td>0.561741</td>\n",
       "      <td>0.563063</td>\n",
       "      <td>0.005839</td>\n",
       "      <td>59</td>\n",
       "      <td>0.684539</td>\n",
       "      <td>0.731920</td>\n",
       "      <td>0.705736</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.691158</td>\n",
       "      <td>0.708125</td>\n",
       "      <td>0.018883</td>\n",
       "      <td>12</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.589375</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.586875</td>\n",
       "      <td>0.574375</td>\n",
       "      <td>0.578125</td>\n",
       "      <td>0.008357</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.261127</td>\n",
       "      <td>0.014927</td>\n",
       "      <td>0.288621</td>\n",
       "      <td>0.018334</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>log2</td>\n",
       "      <td>500</td>\n",
       "      <td>{'classifier__criterion': 'gini', 'classifier_...</td>\n",
       "      <td>0.597817</td>\n",
       "      <td>0.630983</td>\n",
       "      <td>0.596448</td>\n",
       "      <td>0.619739</td>\n",
       "      <td>0.621008</td>\n",
       "      <td>0.613199</td>\n",
       "      <td>0.013691</td>\n",
       "      <td>49</td>\n",
       "      <td>0.629006</td>\n",
       "      <td>0.647764</td>\n",
       "      <td>0.625277</td>\n",
       "      <td>0.643288</td>\n",
       "      <td>0.645230</td>\n",
       "      <td>0.638113</td>\n",
       "      <td>0.009146</td>\n",
       "      <td>5</td>\n",
       "      <td>0.557267</td>\n",
       "      <td>0.575581</td>\n",
       "      <td>0.562874</td>\n",
       "      <td>0.574364</td>\n",
       "      <td>0.566322</td>\n",
       "      <td>0.567282</td>\n",
       "      <td>0.006924</td>\n",
       "      <td>52</td>\n",
       "      <td>0.721945</td>\n",
       "      <td>0.740648</td>\n",
       "      <td>0.703242</td>\n",
       "      <td>0.731009</td>\n",
       "      <td>0.749689</td>\n",
       "      <td>0.729307</td>\n",
       "      <td>0.016003</td>\n",
       "      <td>4</td>\n",
       "      <td>0.573125</td>\n",
       "      <td>0.596250</td>\n",
       "      <td>0.577500</td>\n",
       "      <td>0.593125</td>\n",
       "      <td>0.586250</td>\n",
       "      <td>0.585250</td>\n",
       "      <td>0.008844</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.665351</td>\n",
       "      <td>0.004558</td>\n",
       "      <td>0.139602</td>\n",
       "      <td>0.002534</td>\n",
       "      <td>gini</td>\n",
       "      <td>6</td>\n",
       "      <td>auto</td>\n",
       "      <td>200</td>\n",
       "      <td>{'classifier__criterion': 'gini', 'classifier_...</td>\n",
       "      <td>0.617146</td>\n",
       "      <td>0.640520</td>\n",
       "      <td>0.610404</td>\n",
       "      <td>0.631481</td>\n",
       "      <td>0.642968</td>\n",
       "      <td>0.628504</td>\n",
       "      <td>0.012799</td>\n",
       "      <td>21</td>\n",
       "      <td>0.617813</td>\n",
       "      <td>0.640821</td>\n",
       "      <td>0.617312</td>\n",
       "      <td>0.639955</td>\n",
       "      <td>0.629305</td>\n",
       "      <td>0.629041</td>\n",
       "      <td>0.010214</td>\n",
       "      <td>49</td>\n",
       "      <td>0.563786</td>\n",
       "      <td>0.590336</td>\n",
       "      <td>0.568134</td>\n",
       "      <td>0.578471</td>\n",
       "      <td>0.592308</td>\n",
       "      <td>0.578607</td>\n",
       "      <td>0.011442</td>\n",
       "      <td>17</td>\n",
       "      <td>0.683292</td>\n",
       "      <td>0.700748</td>\n",
       "      <td>0.675810</td>\n",
       "      <td>0.716065</td>\n",
       "      <td>0.671233</td>\n",
       "      <td>0.689430</td>\n",
       "      <td>0.016684</td>\n",
       "      <td>57</td>\n",
       "      <td>0.576250</td>\n",
       "      <td>0.606250</td>\n",
       "      <td>0.580000</td>\n",
       "      <td>0.595625</td>\n",
       "      <td>0.603125</td>\n",
       "      <td>0.592250</td>\n",
       "      <td>0.012097</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.606794</td>\n",
       "      <td>0.133745</td>\n",
       "      <td>0.317934</td>\n",
       "      <td>0.033516</td>\n",
       "      <td>gini</td>\n",
       "      <td>6</td>\n",
       "      <td>auto</td>\n",
       "      <td>500</td>\n",
       "      <td>{'classifier__criterion': 'gini', 'classifier_...</td>\n",
       "      <td>0.615787</td>\n",
       "      <td>0.643601</td>\n",
       "      <td>0.609687</td>\n",
       "      <td>0.631537</td>\n",
       "      <td>0.642545</td>\n",
       "      <td>0.628631</td>\n",
       "      <td>0.013784</td>\n",
       "      <td>19</td>\n",
       "      <td>0.618079</td>\n",
       "      <td>0.638952</td>\n",
       "      <td>0.620533</td>\n",
       "      <td>0.634064</td>\n",
       "      <td>0.632736</td>\n",
       "      <td>0.628873</td>\n",
       "      <td>0.008118</td>\n",
       "      <td>51</td>\n",
       "      <td>0.565083</td>\n",
       "      <td>0.588050</td>\n",
       "      <td>0.569199</td>\n",
       "      <td>0.577869</td>\n",
       "      <td>0.590713</td>\n",
       "      <td>0.578183</td>\n",
       "      <td>0.010068</td>\n",
       "      <td>19</td>\n",
       "      <td>0.682045</td>\n",
       "      <td>0.699501</td>\n",
       "      <td>0.682045</td>\n",
       "      <td>0.702366</td>\n",
       "      <td>0.681196</td>\n",
       "      <td>0.689431</td>\n",
       "      <td>0.009441</td>\n",
       "      <td>55</td>\n",
       "      <td>0.577500</td>\n",
       "      <td>0.603750</td>\n",
       "      <td>0.581875</td>\n",
       "      <td>0.593125</td>\n",
       "      <td>0.603125</td>\n",
       "      <td>0.591875</td>\n",
       "      <td>0.010731</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.678823</td>\n",
       "      <td>0.010622</td>\n",
       "      <td>0.137115</td>\n",
       "      <td>0.000979</td>\n",
       "      <td>gini</td>\n",
       "      <td>6</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>200</td>\n",
       "      <td>{'classifier__criterion': 'gini', 'classifier_...</td>\n",
       "      <td>0.617146</td>\n",
       "      <td>0.640520</td>\n",
       "      <td>0.610404</td>\n",
       "      <td>0.631481</td>\n",
       "      <td>0.642968</td>\n",
       "      <td>0.628504</td>\n",
       "      <td>0.012799</td>\n",
       "      <td>21</td>\n",
       "      <td>0.617813</td>\n",
       "      <td>0.640821</td>\n",
       "      <td>0.617312</td>\n",
       "      <td>0.639955</td>\n",
       "      <td>0.629305</td>\n",
       "      <td>0.629041</td>\n",
       "      <td>0.010214</td>\n",
       "      <td>49</td>\n",
       "      <td>0.563786</td>\n",
       "      <td>0.590336</td>\n",
       "      <td>0.568134</td>\n",
       "      <td>0.578471</td>\n",
       "      <td>0.592308</td>\n",
       "      <td>0.578607</td>\n",
       "      <td>0.011442</td>\n",
       "      <td>17</td>\n",
       "      <td>0.683292</td>\n",
       "      <td>0.700748</td>\n",
       "      <td>0.675810</td>\n",
       "      <td>0.716065</td>\n",
       "      <td>0.671233</td>\n",
       "      <td>0.689430</td>\n",
       "      <td>0.016684</td>\n",
       "      <td>57</td>\n",
       "      <td>0.576250</td>\n",
       "      <td>0.606250</td>\n",
       "      <td>0.580000</td>\n",
       "      <td>0.595625</td>\n",
       "      <td>0.603125</td>\n",
       "      <td>0.592250</td>\n",
       "      <td>0.012097</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.574796</td>\n",
       "      <td>0.017881</td>\n",
       "      <td>0.302278</td>\n",
       "      <td>0.018876</td>\n",
       "      <td>gini</td>\n",
       "      <td>6</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>500</td>\n",
       "      <td>{'classifier__criterion': 'gini', 'classifier_...</td>\n",
       "      <td>0.615787</td>\n",
       "      <td>0.643601</td>\n",
       "      <td>0.609687</td>\n",
       "      <td>0.631537</td>\n",
       "      <td>0.642545</td>\n",
       "      <td>0.628631</td>\n",
       "      <td>0.013784</td>\n",
       "      <td>19</td>\n",
       "      <td>0.618079</td>\n",
       "      <td>0.638952</td>\n",
       "      <td>0.620533</td>\n",
       "      <td>0.634064</td>\n",
       "      <td>0.632736</td>\n",
       "      <td>0.628873</td>\n",
       "      <td>0.008118</td>\n",
       "      <td>51</td>\n",
       "      <td>0.565083</td>\n",
       "      <td>0.588050</td>\n",
       "      <td>0.569199</td>\n",
       "      <td>0.577869</td>\n",
       "      <td>0.590713</td>\n",
       "      <td>0.578183</td>\n",
       "      <td>0.010068</td>\n",
       "      <td>19</td>\n",
       "      <td>0.682045</td>\n",
       "      <td>0.699501</td>\n",
       "      <td>0.682045</td>\n",
       "      <td>0.702366</td>\n",
       "      <td>0.681196</td>\n",
       "      <td>0.689431</td>\n",
       "      <td>0.009441</td>\n",
       "      <td>55</td>\n",
       "      <td>0.577500</td>\n",
       "      <td>0.603750</td>\n",
       "      <td>0.581875</td>\n",
       "      <td>0.593125</td>\n",
       "      <td>0.603125</td>\n",
       "      <td>0.591875</td>\n",
       "      <td>0.010731</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.559676</td>\n",
       "      <td>0.003329</td>\n",
       "      <td>0.134636</td>\n",
       "      <td>0.000628</td>\n",
       "      <td>gini</td>\n",
       "      <td>6</td>\n",
       "      <td>log2</td>\n",
       "      <td>200</td>\n",
       "      <td>{'classifier__criterion': 'gini', 'classifier_...</td>\n",
       "      <td>0.586279</td>\n",
       "      <td>0.624566</td>\n",
       "      <td>0.601087</td>\n",
       "      <td>0.615580</td>\n",
       "      <td>0.619969</td>\n",
       "      <td>0.609496</td>\n",
       "      <td>0.014025</td>\n",
       "      <td>53</td>\n",
       "      <td>0.609921</td>\n",
       "      <td>0.637266</td>\n",
       "      <td>0.621111</td>\n",
       "      <td>0.641676</td>\n",
       "      <td>0.631461</td>\n",
       "      <td>0.628287</td>\n",
       "      <td>0.011478</td>\n",
       "      <td>53</td>\n",
       "      <td>0.556584</td>\n",
       "      <td>0.571146</td>\n",
       "      <td>0.560120</td>\n",
       "      <td>0.575668</td>\n",
       "      <td>0.575230</td>\n",
       "      <td>0.567750</td>\n",
       "      <td>0.007913</td>\n",
       "      <td>50</td>\n",
       "      <td>0.674564</td>\n",
       "      <td>0.720698</td>\n",
       "      <td>0.697007</td>\n",
       "      <td>0.724782</td>\n",
       "      <td>0.699875</td>\n",
       "      <td>0.703385</td>\n",
       "      <td>0.018118</td>\n",
       "      <td>14</td>\n",
       "      <td>0.567500</td>\n",
       "      <td>0.588750</td>\n",
       "      <td>0.573750</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>0.590000</td>\n",
       "      <td>0.582750</td>\n",
       "      <td>0.010229</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.293071</td>\n",
       "      <td>0.018681</td>\n",
       "      <td>0.284031</td>\n",
       "      <td>0.002590</td>\n",
       "      <td>gini</td>\n",
       "      <td>6</td>\n",
       "      <td>log2</td>\n",
       "      <td>500</td>\n",
       "      <td>{'classifier__criterion': 'gini', 'classifier_...</td>\n",
       "      <td>0.597600</td>\n",
       "      <td>0.632043</td>\n",
       "      <td>0.600631</td>\n",
       "      <td>0.627548</td>\n",
       "      <td>0.625736</td>\n",
       "      <td>0.616712</td>\n",
       "      <td>0.014545</td>\n",
       "      <td>44</td>\n",
       "      <td>0.625954</td>\n",
       "      <td>0.644948</td>\n",
       "      <td>0.621951</td>\n",
       "      <td>0.646086</td>\n",
       "      <td>0.650219</td>\n",
       "      <td>0.637832</td>\n",
       "      <td>0.011537</td>\n",
       "      <td>6</td>\n",
       "      <td>0.556202</td>\n",
       "      <td>0.578791</td>\n",
       "      <td>0.559880</td>\n",
       "      <td>0.579624</td>\n",
       "      <td>0.580803</td>\n",
       "      <td>0.571060</td>\n",
       "      <td>0.010713</td>\n",
       "      <td>45</td>\n",
       "      <td>0.715711</td>\n",
       "      <td>0.728180</td>\n",
       "      <td>0.699501</td>\n",
       "      <td>0.729763</td>\n",
       "      <td>0.738481</td>\n",
       "      <td>0.722327</td>\n",
       "      <td>0.013530</td>\n",
       "      <td>7</td>\n",
       "      <td>0.571250</td>\n",
       "      <td>0.598125</td>\n",
       "      <td>0.573750</td>\n",
       "      <td>0.598750</td>\n",
       "      <td>0.601250</td>\n",
       "      <td>0.588625</td>\n",
       "      <td>0.013231</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.716295</td>\n",
       "      <td>0.015048</td>\n",
       "      <td>0.140513</td>\n",
       "      <td>0.001743</td>\n",
       "      <td>gini</td>\n",
       "      <td>7</td>\n",
       "      <td>auto</td>\n",
       "      <td>200</td>\n",
       "      <td>{'classifier__criterion': 'gini', 'classifier_...</td>\n",
       "      <td>0.619519</td>\n",
       "      <td>0.645710</td>\n",
       "      <td>0.614029</td>\n",
       "      <td>0.633723</td>\n",
       "      <td>0.644672</td>\n",
       "      <td>0.631531</td>\n",
       "      <td>0.012878</td>\n",
       "      <td>5</td>\n",
       "      <td>0.621972</td>\n",
       "      <td>0.646259</td>\n",
       "      <td>0.620296</td>\n",
       "      <td>0.646205</td>\n",
       "      <td>0.616725</td>\n",
       "      <td>0.630291</td>\n",
       "      <td>0.013125</td>\n",
       "      <td>39</td>\n",
       "      <td>0.567318</td>\n",
       "      <td>0.592516</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.585440</td>\n",
       "      <td>0.577802</td>\n",
       "      <td>0.578901</td>\n",
       "      <td>0.009158</td>\n",
       "      <td>13</td>\n",
       "      <td>0.688279</td>\n",
       "      <td>0.710723</td>\n",
       "      <td>0.678304</td>\n",
       "      <td>0.721046</td>\n",
       "      <td>0.661270</td>\n",
       "      <td>0.691925</td>\n",
       "      <td>0.021633</td>\n",
       "      <td>44</td>\n",
       "      <td>0.580625</td>\n",
       "      <td>0.610000</td>\n",
       "      <td>0.583750</td>\n",
       "      <td>0.603750</td>\n",
       "      <td>0.587500</td>\n",
       "      <td>0.593125</td>\n",
       "      <td>0.011605</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.655672</td>\n",
       "      <td>0.018399</td>\n",
       "      <td>0.298640</td>\n",
       "      <td>0.001979</td>\n",
       "      <td>gini</td>\n",
       "      <td>7</td>\n",
       "      <td>auto</td>\n",
       "      <td>500</td>\n",
       "      <td>{'classifier__criterion': 'gini', 'classifier_...</td>\n",
       "      <td>0.616366</td>\n",
       "      <td>0.646471</td>\n",
       "      <td>0.615934</td>\n",
       "      <td>0.628656</td>\n",
       "      <td>0.644065</td>\n",
       "      <td>0.630298</td>\n",
       "      <td>0.013071</td>\n",
       "      <td>11</td>\n",
       "      <td>0.617730</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.617228</td>\n",
       "      <td>0.636569</td>\n",
       "      <td>0.637565</td>\n",
       "      <td>0.629818</td>\n",
       "      <td>0.010138</td>\n",
       "      <td>41</td>\n",
       "      <td>0.564499</td>\n",
       "      <td>0.590717</td>\n",
       "      <td>0.568875</td>\n",
       "      <td>0.582043</td>\n",
       "      <td>0.591684</td>\n",
       "      <td>0.579564</td>\n",
       "      <td>0.011123</td>\n",
       "      <td>9</td>\n",
       "      <td>0.682045</td>\n",
       "      <td>0.698254</td>\n",
       "      <td>0.674564</td>\n",
       "      <td>0.702366</td>\n",
       "      <td>0.691158</td>\n",
       "      <td>0.689677</td>\n",
       "      <td>0.010232</td>\n",
       "      <td>53</td>\n",
       "      <td>0.576875</td>\n",
       "      <td>0.606250</td>\n",
       "      <td>0.580625</td>\n",
       "      <td>0.597500</td>\n",
       "      <td>0.605625</td>\n",
       "      <td>0.593375</td>\n",
       "      <td>0.012391</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.715739</td>\n",
       "      <td>0.017276</td>\n",
       "      <td>0.140439</td>\n",
       "      <td>0.001342</td>\n",
       "      <td>gini</td>\n",
       "      <td>7</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>200</td>\n",
       "      <td>{'classifier__criterion': 'gini', 'classifier_...</td>\n",
       "      <td>0.619519</td>\n",
       "      <td>0.645710</td>\n",
       "      <td>0.614029</td>\n",
       "      <td>0.633723</td>\n",
       "      <td>0.644672</td>\n",
       "      <td>0.631531</td>\n",
       "      <td>0.012878</td>\n",
       "      <td>5</td>\n",
       "      <td>0.621972</td>\n",
       "      <td>0.646259</td>\n",
       "      <td>0.620296</td>\n",
       "      <td>0.646205</td>\n",
       "      <td>0.616725</td>\n",
       "      <td>0.630291</td>\n",
       "      <td>0.013125</td>\n",
       "      <td>39</td>\n",
       "      <td>0.567318</td>\n",
       "      <td>0.592516</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.585440</td>\n",
       "      <td>0.577802</td>\n",
       "      <td>0.578901</td>\n",
       "      <td>0.009158</td>\n",
       "      <td>13</td>\n",
       "      <td>0.688279</td>\n",
       "      <td>0.710723</td>\n",
       "      <td>0.678304</td>\n",
       "      <td>0.721046</td>\n",
       "      <td>0.661270</td>\n",
       "      <td>0.691925</td>\n",
       "      <td>0.021633</td>\n",
       "      <td>44</td>\n",
       "      <td>0.580625</td>\n",
       "      <td>0.610000</td>\n",
       "      <td>0.583750</td>\n",
       "      <td>0.603750</td>\n",
       "      <td>0.587500</td>\n",
       "      <td>0.593125</td>\n",
       "      <td>0.011605</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.671541</td>\n",
       "      <td>0.033684</td>\n",
       "      <td>0.302612</td>\n",
       "      <td>0.004618</td>\n",
       "      <td>gini</td>\n",
       "      <td>7</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>500</td>\n",
       "      <td>{'classifier__criterion': 'gini', 'classifier_...</td>\n",
       "      <td>0.616366</td>\n",
       "      <td>0.646471</td>\n",
       "      <td>0.615934</td>\n",
       "      <td>0.628656</td>\n",
       "      <td>0.644065</td>\n",
       "      <td>0.630298</td>\n",
       "      <td>0.013071</td>\n",
       "      <td>11</td>\n",
       "      <td>0.617730</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.617228</td>\n",
       "      <td>0.636569</td>\n",
       "      <td>0.637565</td>\n",
       "      <td>0.629818</td>\n",
       "      <td>0.010138</td>\n",
       "      <td>41</td>\n",
       "      <td>0.564499</td>\n",
       "      <td>0.590717</td>\n",
       "      <td>0.568875</td>\n",
       "      <td>0.582043</td>\n",
       "      <td>0.591684</td>\n",
       "      <td>0.579564</td>\n",
       "      <td>0.011123</td>\n",
       "      <td>9</td>\n",
       "      <td>0.682045</td>\n",
       "      <td>0.698254</td>\n",
       "      <td>0.674564</td>\n",
       "      <td>0.702366</td>\n",
       "      <td>0.691158</td>\n",
       "      <td>0.689677</td>\n",
       "      <td>0.010232</td>\n",
       "      <td>53</td>\n",
       "      <td>0.576875</td>\n",
       "      <td>0.606250</td>\n",
       "      <td>0.580625</td>\n",
       "      <td>0.597500</td>\n",
       "      <td>0.605625</td>\n",
       "      <td>0.593375</td>\n",
       "      <td>0.012391</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.567735</td>\n",
       "      <td>0.004235</td>\n",
       "      <td>0.137315</td>\n",
       "      <td>0.002243</td>\n",
       "      <td>gini</td>\n",
       "      <td>7</td>\n",
       "      <td>log2</td>\n",
       "      <td>200</td>\n",
       "      <td>{'classifier__criterion': 'gini', 'classifier_...</td>\n",
       "      <td>0.584013</td>\n",
       "      <td>0.623935</td>\n",
       "      <td>0.594944</td>\n",
       "      <td>0.622007</td>\n",
       "      <td>0.621323</td>\n",
       "      <td>0.609244</td>\n",
       "      <td>0.016527</td>\n",
       "      <td>55</td>\n",
       "      <td>0.600114</td>\n",
       "      <td>0.628090</td>\n",
       "      <td>0.624790</td>\n",
       "      <td>0.643372</td>\n",
       "      <td>0.619429</td>\n",
       "      <td>0.623159</td>\n",
       "      <td>0.014004</td>\n",
       "      <td>59</td>\n",
       "      <td>0.553102</td>\n",
       "      <td>0.571575</td>\n",
       "      <td>0.567788</td>\n",
       "      <td>0.580000</td>\n",
       "      <td>0.572334</td>\n",
       "      <td>0.568960</td>\n",
       "      <td>0.008865</td>\n",
       "      <td>48</td>\n",
       "      <td>0.655860</td>\n",
       "      <td>0.697007</td>\n",
       "      <td>0.694514</td>\n",
       "      <td>0.722291</td>\n",
       "      <td>0.674969</td>\n",
       "      <td>0.688928</td>\n",
       "      <td>0.022351</td>\n",
       "      <td>59</td>\n",
       "      <td>0.561875</td>\n",
       "      <td>0.586250</td>\n",
       "      <td>0.581875</td>\n",
       "      <td>0.598125</td>\n",
       "      <td>0.583750</td>\n",
       "      <td>0.582375</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.305767</td>\n",
       "      <td>0.008277</td>\n",
       "      <td>0.293613</td>\n",
       "      <td>0.005774</td>\n",
       "      <td>gini</td>\n",
       "      <td>7</td>\n",
       "      <td>log2</td>\n",
       "      <td>500</td>\n",
       "      <td>{'classifier__criterion': 'gini', 'classifier_...</td>\n",
       "      <td>0.599644</td>\n",
       "      <td>0.630757</td>\n",
       "      <td>0.597997</td>\n",
       "      <td>0.628879</td>\n",
       "      <td>0.627737</td>\n",
       "      <td>0.617003</td>\n",
       "      <td>0.014886</td>\n",
       "      <td>43</td>\n",
       "      <td>0.625828</td>\n",
       "      <td>0.634444</td>\n",
       "      <td>0.616071</td>\n",
       "      <td>0.643729</td>\n",
       "      <td>0.639106</td>\n",
       "      <td>0.631836</td>\n",
       "      <td>0.009856</td>\n",
       "      <td>29</td>\n",
       "      <td>0.561386</td>\n",
       "      <td>0.572144</td>\n",
       "      <td>0.557576</td>\n",
       "      <td>0.580581</td>\n",
       "      <td>0.579534</td>\n",
       "      <td>0.570244</td>\n",
       "      <td>0.009335</td>\n",
       "      <td>46</td>\n",
       "      <td>0.706983</td>\n",
       "      <td>0.711970</td>\n",
       "      <td>0.688279</td>\n",
       "      <td>0.722291</td>\n",
       "      <td>0.712329</td>\n",
       "      <td>0.708370</td>\n",
       "      <td>0.011207</td>\n",
       "      <td>11</td>\n",
       "      <td>0.576250</td>\n",
       "      <td>0.588750</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.598750</td>\n",
       "      <td>0.596250</td>\n",
       "      <td>0.586000</td>\n",
       "      <td>0.011192</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.752681</td>\n",
       "      <td>0.015431</td>\n",
       "      <td>0.155304</td>\n",
       "      <td>0.026791</td>\n",
       "      <td>gini</td>\n",
       "      <td>8</td>\n",
       "      <td>auto</td>\n",
       "      <td>200</td>\n",
       "      <td>{'classifier__criterion': 'gini', 'classifier_...</td>\n",
       "      <td>0.617415</td>\n",
       "      <td>0.649660</td>\n",
       "      <td>0.615063</td>\n",
       "      <td>0.637617</td>\n",
       "      <td>0.639286</td>\n",
       "      <td>0.631808</td>\n",
       "      <td>0.013386</td>\n",
       "      <td>1</td>\n",
       "      <td>0.616601</td>\n",
       "      <td>0.641187</td>\n",
       "      <td>0.616438</td>\n",
       "      <td>0.647746</td>\n",
       "      <td>0.629930</td>\n",
       "      <td>0.630380</td>\n",
       "      <td>0.012671</td>\n",
       "      <td>35</td>\n",
       "      <td>0.563467</td>\n",
       "      <td>0.590957</td>\n",
       "      <td>0.568421</td>\n",
       "      <td>0.585513</td>\n",
       "      <td>0.589577</td>\n",
       "      <td>0.579587</td>\n",
       "      <td>0.011390</td>\n",
       "      <td>7</td>\n",
       "      <td>0.680798</td>\n",
       "      <td>0.700748</td>\n",
       "      <td>0.673317</td>\n",
       "      <td>0.724782</td>\n",
       "      <td>0.676214</td>\n",
       "      <td>0.691172</td>\n",
       "      <td>0.019347</td>\n",
       "      <td>51</td>\n",
       "      <td>0.575625</td>\n",
       "      <td>0.606875</td>\n",
       "      <td>0.580000</td>\n",
       "      <td>0.604375</td>\n",
       "      <td>0.601250</td>\n",
       "      <td>0.593625</td>\n",
       "      <td>0.013107</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.738606</td>\n",
       "      <td>0.009045</td>\n",
       "      <td>0.307554</td>\n",
       "      <td>0.001427</td>\n",
       "      <td>gini</td>\n",
       "      <td>8</td>\n",
       "      <td>auto</td>\n",
       "      <td>500</td>\n",
       "      <td>{'classifier__criterion': 'gini', 'classifier_...</td>\n",
       "      <td>0.615927</td>\n",
       "      <td>0.646971</td>\n",
       "      <td>0.617404</td>\n",
       "      <td>0.633285</td>\n",
       "      <td>0.643678</td>\n",
       "      <td>0.631453</td>\n",
       "      <td>0.012900</td>\n",
       "      <td>7</td>\n",
       "      <td>0.623245</td>\n",
       "      <td>0.642327</td>\n",
       "      <td>0.614857</td>\n",
       "      <td>0.635334</td>\n",
       "      <td>0.639816</td>\n",
       "      <td>0.631116</td>\n",
       "      <td>0.010445</td>\n",
       "      <td>32</td>\n",
       "      <td>0.566905</td>\n",
       "      <td>0.592008</td>\n",
       "      <td>0.567511</td>\n",
       "      <td>0.582555</td>\n",
       "      <td>0.594652</td>\n",
       "      <td>0.580726</td>\n",
       "      <td>0.011749</td>\n",
       "      <td>5</td>\n",
       "      <td>0.692020</td>\n",
       "      <td>0.701995</td>\n",
       "      <td>0.670823</td>\n",
       "      <td>0.698630</td>\n",
       "      <td>0.692403</td>\n",
       "      <td>0.691174</td>\n",
       "      <td>0.010854</td>\n",
       "      <td>49</td>\n",
       "      <td>0.580625</td>\n",
       "      <td>0.608125</td>\n",
       "      <td>0.578750</td>\n",
       "      <td>0.597500</td>\n",
       "      <td>0.608750</td>\n",
       "      <td>0.594750</td>\n",
       "      <td>0.012946</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.747338</td>\n",
       "      <td>0.009881</td>\n",
       "      <td>0.144510</td>\n",
       "      <td>0.002415</td>\n",
       "      <td>gini</td>\n",
       "      <td>8</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>200</td>\n",
       "      <td>{'classifier__criterion': 'gini', 'classifier_...</td>\n",
       "      <td>0.617415</td>\n",
       "      <td>0.649660</td>\n",
       "      <td>0.615063</td>\n",
       "      <td>0.637617</td>\n",
       "      <td>0.639286</td>\n",
       "      <td>0.631808</td>\n",
       "      <td>0.013386</td>\n",
       "      <td>1</td>\n",
       "      <td>0.616601</td>\n",
       "      <td>0.641187</td>\n",
       "      <td>0.616438</td>\n",
       "      <td>0.647746</td>\n",
       "      <td>0.629930</td>\n",
       "      <td>0.630380</td>\n",
       "      <td>0.012671</td>\n",
       "      <td>35</td>\n",
       "      <td>0.563467</td>\n",
       "      <td>0.590957</td>\n",
       "      <td>0.568421</td>\n",
       "      <td>0.585513</td>\n",
       "      <td>0.589577</td>\n",
       "      <td>0.579587</td>\n",
       "      <td>0.011390</td>\n",
       "      <td>7</td>\n",
       "      <td>0.680798</td>\n",
       "      <td>0.700748</td>\n",
       "      <td>0.673317</td>\n",
       "      <td>0.724782</td>\n",
       "      <td>0.676214</td>\n",
       "      <td>0.691172</td>\n",
       "      <td>0.019347</td>\n",
       "      <td>51</td>\n",
       "      <td>0.575625</td>\n",
       "      <td>0.606875</td>\n",
       "      <td>0.580000</td>\n",
       "      <td>0.604375</td>\n",
       "      <td>0.601250</td>\n",
       "      <td>0.593625</td>\n",
       "      <td>0.013107</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.749946</td>\n",
       "      <td>0.021520</td>\n",
       "      <td>0.307032</td>\n",
       "      <td>0.002393</td>\n",
       "      <td>gini</td>\n",
       "      <td>8</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>500</td>\n",
       "      <td>{'classifier__criterion': 'gini', 'classifier_...</td>\n",
       "      <td>0.615927</td>\n",
       "      <td>0.646971</td>\n",
       "      <td>0.617404</td>\n",
       "      <td>0.633285</td>\n",
       "      <td>0.643678</td>\n",
       "      <td>0.631453</td>\n",
       "      <td>0.012900</td>\n",
       "      <td>7</td>\n",
       "      <td>0.623245</td>\n",
       "      <td>0.642327</td>\n",
       "      <td>0.614857</td>\n",
       "      <td>0.635334</td>\n",
       "      <td>0.639816</td>\n",
       "      <td>0.631116</td>\n",
       "      <td>0.010445</td>\n",
       "      <td>32</td>\n",
       "      <td>0.566905</td>\n",
       "      <td>0.592008</td>\n",
       "      <td>0.567511</td>\n",
       "      <td>0.582555</td>\n",
       "      <td>0.594652</td>\n",
       "      <td>0.580726</td>\n",
       "      <td>0.011749</td>\n",
       "      <td>5</td>\n",
       "      <td>0.692020</td>\n",
       "      <td>0.701995</td>\n",
       "      <td>0.670823</td>\n",
       "      <td>0.698630</td>\n",
       "      <td>0.692403</td>\n",
       "      <td>0.691174</td>\n",
       "      <td>0.010854</td>\n",
       "      <td>49</td>\n",
       "      <td>0.580625</td>\n",
       "      <td>0.608125</td>\n",
       "      <td>0.578750</td>\n",
       "      <td>0.597500</td>\n",
       "      <td>0.608750</td>\n",
       "      <td>0.594750</td>\n",
       "      <td>0.012946</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.630612</td>\n",
       "      <td>0.051648</td>\n",
       "      <td>0.147145</td>\n",
       "      <td>0.010985</td>\n",
       "      <td>gini</td>\n",
       "      <td>8</td>\n",
       "      <td>log2</td>\n",
       "      <td>200</td>\n",
       "      <td>{'classifier__criterion': 'gini', 'classifier_...</td>\n",
       "      <td>0.588041</td>\n",
       "      <td>0.630738</td>\n",
       "      <td>0.590147</td>\n",
       "      <td>0.622369</td>\n",
       "      <td>0.626515</td>\n",
       "      <td>0.611562</td>\n",
       "      <td>0.018547</td>\n",
       "      <td>51</td>\n",
       "      <td>0.593732</td>\n",
       "      <td>0.630926</td>\n",
       "      <td>0.618304</td>\n",
       "      <td>0.646700</td>\n",
       "      <td>0.617580</td>\n",
       "      <td>0.621448</td>\n",
       "      <td>0.017438</td>\n",
       "      <td>60</td>\n",
       "      <td>0.546695</td>\n",
       "      <td>0.576289</td>\n",
       "      <td>0.559596</td>\n",
       "      <td>0.583000</td>\n",
       "      <td>0.570074</td>\n",
       "      <td>0.567131</td>\n",
       "      <td>0.012797</td>\n",
       "      <td>53</td>\n",
       "      <td>0.649626</td>\n",
       "      <td>0.697007</td>\n",
       "      <td>0.690773</td>\n",
       "      <td>0.726027</td>\n",
       "      <td>0.673724</td>\n",
       "      <td>0.687431</td>\n",
       "      <td>0.025336</td>\n",
       "      <td>60</td>\n",
       "      <td>0.554375</td>\n",
       "      <td>0.591250</td>\n",
       "      <td>0.572500</td>\n",
       "      <td>0.601875</td>\n",
       "      <td>0.581250</td>\n",
       "      <td>0.580250</td>\n",
       "      <td>0.016243</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.341653</td>\n",
       "      <td>0.018557</td>\n",
       "      <td>0.300035</td>\n",
       "      <td>0.010630</td>\n",
       "      <td>gini</td>\n",
       "      <td>8</td>\n",
       "      <td>log2</td>\n",
       "      <td>500</td>\n",
       "      <td>{'classifier__criterion': 'gini', 'classifier_...</td>\n",
       "      <td>0.603474</td>\n",
       "      <td>0.639674</td>\n",
       "      <td>0.597632</td>\n",
       "      <td>0.629960</td>\n",
       "      <td>0.637565</td>\n",
       "      <td>0.621661</td>\n",
       "      <td>0.017632</td>\n",
       "      <td>41</td>\n",
       "      <td>0.626039</td>\n",
       "      <td>0.629797</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>0.635903</td>\n",
       "      <td>0.643820</td>\n",
       "      <td>0.630445</td>\n",
       "      <td>0.009147</td>\n",
       "      <td>34</td>\n",
       "      <td>0.563310</td>\n",
       "      <td>0.575258</td>\n",
       "      <td>0.556112</td>\n",
       "      <td>0.580082</td>\n",
       "      <td>0.586489</td>\n",
       "      <td>0.572250</td>\n",
       "      <td>0.011077</td>\n",
       "      <td>44</td>\n",
       "      <td>0.704489</td>\n",
       "      <td>0.695761</td>\n",
       "      <td>0.692020</td>\n",
       "      <td>0.703611</td>\n",
       "      <td>0.713574</td>\n",
       "      <td>0.701891</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>15</td>\n",
       "      <td>0.578125</td>\n",
       "      <td>0.590000</td>\n",
       "      <td>0.568750</td>\n",
       "      <td>0.595625</td>\n",
       "      <td>0.603750</td>\n",
       "      <td>0.587250</td>\n",
       "      <td>0.012454</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.613221</td>\n",
       "      <td>0.010378</td>\n",
       "      <td>0.135915</td>\n",
       "      <td>0.003630</td>\n",
       "      <td>entropy</td>\n",
       "      <td>4</td>\n",
       "      <td>auto</td>\n",
       "      <td>200</td>\n",
       "      <td>{'classifier__criterion': 'entropy', 'classifi...</td>\n",
       "      <td>0.610620</td>\n",
       "      <td>0.641967</td>\n",
       "      <td>0.608073</td>\n",
       "      <td>0.627693</td>\n",
       "      <td>0.639708</td>\n",
       "      <td>0.625612</td>\n",
       "      <td>0.014163</td>\n",
       "      <td>33</td>\n",
       "      <td>0.622074</td>\n",
       "      <td>0.642534</td>\n",
       "      <td>0.617997</td>\n",
       "      <td>0.652103</td>\n",
       "      <td>0.626866</td>\n",
       "      <td>0.632314</td>\n",
       "      <td>0.012927</td>\n",
       "      <td>19</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.587992</td>\n",
       "      <td>0.565803</td>\n",
       "      <td>0.580739</td>\n",
       "      <td>0.581470</td>\n",
       "      <td>0.575701</td>\n",
       "      <td>0.009818</td>\n",
       "      <td>37</td>\n",
       "      <td>0.695761</td>\n",
       "      <td>0.708229</td>\n",
       "      <td>0.680798</td>\n",
       "      <td>0.743462</td>\n",
       "      <td>0.679950</td>\n",
       "      <td>0.701640</td>\n",
       "      <td>0.023375</td>\n",
       "      <td>16</td>\n",
       "      <td>0.576250</td>\n",
       "      <td>0.605000</td>\n",
       "      <td>0.578125</td>\n",
       "      <td>0.601875</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>0.591000</td>\n",
       "      <td>0.011876</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.526328</td>\n",
       "      <td>0.151030</td>\n",
       "      <td>0.295402</td>\n",
       "      <td>0.015606</td>\n",
       "      <td>entropy</td>\n",
       "      <td>4</td>\n",
       "      <td>auto</td>\n",
       "      <td>500</td>\n",
       "      <td>{'classifier__criterion': 'entropy', 'classifi...</td>\n",
       "      <td>0.610098</td>\n",
       "      <td>0.641238</td>\n",
       "      <td>0.610259</td>\n",
       "      <td>0.622706</td>\n",
       "      <td>0.637282</td>\n",
       "      <td>0.624316</td>\n",
       "      <td>0.013091</td>\n",
       "      <td>39</td>\n",
       "      <td>0.625139</td>\n",
       "      <td>0.637188</td>\n",
       "      <td>0.625282</td>\n",
       "      <td>0.643094</td>\n",
       "      <td>0.629758</td>\n",
       "      <td>0.632092</td>\n",
       "      <td>0.007031</td>\n",
       "      <td>23</td>\n",
       "      <td>0.564257</td>\n",
       "      <td>0.584200</td>\n",
       "      <td>0.571134</td>\n",
       "      <td>0.577954</td>\n",
       "      <td>0.586466</td>\n",
       "      <td>0.576802</td>\n",
       "      <td>0.008233</td>\n",
       "      <td>33</td>\n",
       "      <td>0.700748</td>\n",
       "      <td>0.700748</td>\n",
       "      <td>0.690773</td>\n",
       "      <td>0.724782</td>\n",
       "      <td>0.679950</td>\n",
       "      <td>0.699400</td>\n",
       "      <td>0.014836</td>\n",
       "      <td>21</td>\n",
       "      <td>0.578750</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.585000</td>\n",
       "      <td>0.596250</td>\n",
       "      <td>0.598750</td>\n",
       "      <td>0.591750</td>\n",
       "      <td>0.008389</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.625724</td>\n",
       "      <td>0.022259</td>\n",
       "      <td>0.136520</td>\n",
       "      <td>0.001639</td>\n",
       "      <td>entropy</td>\n",
       "      <td>4</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>200</td>\n",
       "      <td>{'classifier__criterion': 'entropy', 'classifi...</td>\n",
       "      <td>0.610620</td>\n",
       "      <td>0.641967</td>\n",
       "      <td>0.608073</td>\n",
       "      <td>0.627693</td>\n",
       "      <td>0.639708</td>\n",
       "      <td>0.625612</td>\n",
       "      <td>0.014163</td>\n",
       "      <td>33</td>\n",
       "      <td>0.622074</td>\n",
       "      <td>0.642534</td>\n",
       "      <td>0.617997</td>\n",
       "      <td>0.652103</td>\n",
       "      <td>0.626866</td>\n",
       "      <td>0.632314</td>\n",
       "      <td>0.012927</td>\n",
       "      <td>19</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.587992</td>\n",
       "      <td>0.565803</td>\n",
       "      <td>0.580739</td>\n",
       "      <td>0.581470</td>\n",
       "      <td>0.575701</td>\n",
       "      <td>0.009818</td>\n",
       "      <td>37</td>\n",
       "      <td>0.695761</td>\n",
       "      <td>0.708229</td>\n",
       "      <td>0.680798</td>\n",
       "      <td>0.743462</td>\n",
       "      <td>0.679950</td>\n",
       "      <td>0.701640</td>\n",
       "      <td>0.023375</td>\n",
       "      <td>16</td>\n",
       "      <td>0.576250</td>\n",
       "      <td>0.605000</td>\n",
       "      <td>0.578125</td>\n",
       "      <td>0.601875</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>0.591000</td>\n",
       "      <td>0.011876</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.434417</td>\n",
       "      <td>0.021688</td>\n",
       "      <td>0.280452</td>\n",
       "      <td>0.001503</td>\n",
       "      <td>entropy</td>\n",
       "      <td>4</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>500</td>\n",
       "      <td>{'classifier__criterion': 'entropy', 'classifi...</td>\n",
       "      <td>0.610098</td>\n",
       "      <td>0.641238</td>\n",
       "      <td>0.610259</td>\n",
       "      <td>0.622706</td>\n",
       "      <td>0.637282</td>\n",
       "      <td>0.624316</td>\n",
       "      <td>0.013091</td>\n",
       "      <td>39</td>\n",
       "      <td>0.625139</td>\n",
       "      <td>0.637188</td>\n",
       "      <td>0.625282</td>\n",
       "      <td>0.643094</td>\n",
       "      <td>0.629758</td>\n",
       "      <td>0.632092</td>\n",
       "      <td>0.007031</td>\n",
       "      <td>23</td>\n",
       "      <td>0.564257</td>\n",
       "      <td>0.584200</td>\n",
       "      <td>0.571134</td>\n",
       "      <td>0.577954</td>\n",
       "      <td>0.586466</td>\n",
       "      <td>0.576802</td>\n",
       "      <td>0.008233</td>\n",
       "      <td>33</td>\n",
       "      <td>0.700748</td>\n",
       "      <td>0.700748</td>\n",
       "      <td>0.690773</td>\n",
       "      <td>0.724782</td>\n",
       "      <td>0.679950</td>\n",
       "      <td>0.699400</td>\n",
       "      <td>0.014836</td>\n",
       "      <td>21</td>\n",
       "      <td>0.578750</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.585000</td>\n",
       "      <td>0.596250</td>\n",
       "      <td>0.598750</td>\n",
       "      <td>0.591750</td>\n",
       "      <td>0.008389</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.551060</td>\n",
       "      <td>0.013857</td>\n",
       "      <td>0.133317</td>\n",
       "      <td>0.001624</td>\n",
       "      <td>entropy</td>\n",
       "      <td>4</td>\n",
       "      <td>log2</td>\n",
       "      <td>200</td>\n",
       "      <td>{'classifier__criterion': 'entropy', 'classifi...</td>\n",
       "      <td>0.587026</td>\n",
       "      <td>0.622221</td>\n",
       "      <td>0.589951</td>\n",
       "      <td>0.615048</td>\n",
       "      <td>0.611066</td>\n",
       "      <td>0.605062</td>\n",
       "      <td>0.014028</td>\n",
       "      <td>58</td>\n",
       "      <td>0.621111</td>\n",
       "      <td>0.648796</td>\n",
       "      <td>0.636704</td>\n",
       "      <td>0.654526</td>\n",
       "      <td>0.624793</td>\n",
       "      <td>0.637186</td>\n",
       "      <td>0.013021</td>\n",
       "      <td>7</td>\n",
       "      <td>0.560120</td>\n",
       "      <td>0.577973</td>\n",
       "      <td>0.557638</td>\n",
       "      <td>0.574248</td>\n",
       "      <td>0.560277</td>\n",
       "      <td>0.566051</td>\n",
       "      <td>0.008350</td>\n",
       "      <td>56</td>\n",
       "      <td>0.697007</td>\n",
       "      <td>0.739401</td>\n",
       "      <td>0.741895</td>\n",
       "      <td>0.760897</td>\n",
       "      <td>0.706102</td>\n",
       "      <td>0.729061</td>\n",
       "      <td>0.023831</td>\n",
       "      <td>5</td>\n",
       "      <td>0.573750</td>\n",
       "      <td>0.598750</td>\n",
       "      <td>0.575625</td>\n",
       "      <td>0.596875</td>\n",
       "      <td>0.574375</td>\n",
       "      <td>0.583875</td>\n",
       "      <td>0.011411</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.246591</td>\n",
       "      <td>0.015093</td>\n",
       "      <td>0.274043</td>\n",
       "      <td>0.004847</td>\n",
       "      <td>entropy</td>\n",
       "      <td>4</td>\n",
       "      <td>log2</td>\n",
       "      <td>500</td>\n",
       "      <td>{'classifier__criterion': 'entropy', 'classifi...</td>\n",
       "      <td>0.599634</td>\n",
       "      <td>0.632319</td>\n",
       "      <td>0.599658</td>\n",
       "      <td>0.620578</td>\n",
       "      <td>0.619765</td>\n",
       "      <td>0.614391</td>\n",
       "      <td>0.012833</td>\n",
       "      <td>48</td>\n",
       "      <td>0.634989</td>\n",
       "      <td>0.663462</td>\n",
       "      <td>0.627408</td>\n",
       "      <td>0.651491</td>\n",
       "      <td>0.656811</td>\n",
       "      <td>0.646832</td>\n",
       "      <td>0.013531</td>\n",
       "      <td>1</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.580374</td>\n",
       "      <td>0.561576</td>\n",
       "      <td>0.576775</td>\n",
       "      <td>0.570119</td>\n",
       "      <td>0.569769</td>\n",
       "      <td>0.008053</td>\n",
       "      <td>47</td>\n",
       "      <td>0.733167</td>\n",
       "      <td>0.774314</td>\n",
       "      <td>0.710723</td>\n",
       "      <td>0.748443</td>\n",
       "      <td>0.774595</td>\n",
       "      <td>0.748249</td>\n",
       "      <td>0.024532</td>\n",
       "      <td>1</td>\n",
       "      <td>0.577500</td>\n",
       "      <td>0.606250</td>\n",
       "      <td>0.576875</td>\n",
       "      <td>0.598125</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>0.590500</td>\n",
       "      <td>0.011588</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.640549</td>\n",
       "      <td>0.004267</td>\n",
       "      <td>0.136350</td>\n",
       "      <td>0.002478</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>auto</td>\n",
       "      <td>200</td>\n",
       "      <td>{'classifier__criterion': 'entropy', 'classifi...</td>\n",
       "      <td>0.611043</td>\n",
       "      <td>0.643271</td>\n",
       "      <td>0.609419</td>\n",
       "      <td>0.630101</td>\n",
       "      <td>0.644673</td>\n",
       "      <td>0.627702</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>25</td>\n",
       "      <td>0.620805</td>\n",
       "      <td>0.641934</td>\n",
       "      <td>0.623172</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.630422</td>\n",
       "      <td>0.632678</td>\n",
       "      <td>0.010285</td>\n",
       "      <td>15</td>\n",
       "      <td>0.562880</td>\n",
       "      <td>0.584442</td>\n",
       "      <td>0.567623</td>\n",
       "      <td>0.583584</td>\n",
       "      <td>0.588553</td>\n",
       "      <td>0.577416</td>\n",
       "      <td>0.010185</td>\n",
       "      <td>29</td>\n",
       "      <td>0.692020</td>\n",
       "      <td>0.711970</td>\n",
       "      <td>0.690773</td>\n",
       "      <td>0.726027</td>\n",
       "      <td>0.678705</td>\n",
       "      <td>0.699899</td>\n",
       "      <td>0.016870</td>\n",
       "      <td>19</td>\n",
       "      <td>0.576250</td>\n",
       "      <td>0.601875</td>\n",
       "      <td>0.581250</td>\n",
       "      <td>0.602500</td>\n",
       "      <td>0.600625</td>\n",
       "      <td>0.592500</td>\n",
       "      <td>0.011354</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.495935</td>\n",
       "      <td>0.007676</td>\n",
       "      <td>0.298606</td>\n",
       "      <td>0.022701</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>auto</td>\n",
       "      <td>500</td>\n",
       "      <td>{'classifier__criterion': 'entropy', 'classifi...</td>\n",
       "      <td>0.610159</td>\n",
       "      <td>0.645059</td>\n",
       "      <td>0.613160</td>\n",
       "      <td>0.626581</td>\n",
       "      <td>0.640397</td>\n",
       "      <td>0.627071</td>\n",
       "      <td>0.014007</td>\n",
       "      <td>29</td>\n",
       "      <td>0.622197</td>\n",
       "      <td>0.641040</td>\n",
       "      <td>0.622673</td>\n",
       "      <td>0.644199</td>\n",
       "      <td>0.629758</td>\n",
       "      <td>0.631974</td>\n",
       "      <td>0.009150</td>\n",
       "      <td>25</td>\n",
       "      <td>0.565173</td>\n",
       "      <td>0.586350</td>\n",
       "      <td>0.568486</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.586466</td>\n",
       "      <td>0.577084</td>\n",
       "      <td>0.008867</td>\n",
       "      <td>31</td>\n",
       "      <td>0.692020</td>\n",
       "      <td>0.706983</td>\n",
       "      <td>0.688279</td>\n",
       "      <td>0.726027</td>\n",
       "      <td>0.679950</td>\n",
       "      <td>0.698652</td>\n",
       "      <td>0.016249</td>\n",
       "      <td>23</td>\n",
       "      <td>0.578750</td>\n",
       "      <td>0.603125</td>\n",
       "      <td>0.581875</td>\n",
       "      <td>0.597500</td>\n",
       "      <td>0.598750</td>\n",
       "      <td>0.592000</td>\n",
       "      <td>0.009774</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.644530</td>\n",
       "      <td>0.011714</td>\n",
       "      <td>0.141959</td>\n",
       "      <td>0.005716</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>200</td>\n",
       "      <td>{'classifier__criterion': 'entropy', 'classifi...</td>\n",
       "      <td>0.611043</td>\n",
       "      <td>0.643271</td>\n",
       "      <td>0.609419</td>\n",
       "      <td>0.630101</td>\n",
       "      <td>0.644673</td>\n",
       "      <td>0.627702</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>25</td>\n",
       "      <td>0.620805</td>\n",
       "      <td>0.641934</td>\n",
       "      <td>0.623172</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.630422</td>\n",
       "      <td>0.632678</td>\n",
       "      <td>0.010285</td>\n",
       "      <td>15</td>\n",
       "      <td>0.562880</td>\n",
       "      <td>0.584442</td>\n",
       "      <td>0.567623</td>\n",
       "      <td>0.583584</td>\n",
       "      <td>0.588553</td>\n",
       "      <td>0.577416</td>\n",
       "      <td>0.010185</td>\n",
       "      <td>29</td>\n",
       "      <td>0.692020</td>\n",
       "      <td>0.711970</td>\n",
       "      <td>0.690773</td>\n",
       "      <td>0.726027</td>\n",
       "      <td>0.678705</td>\n",
       "      <td>0.699899</td>\n",
       "      <td>0.016870</td>\n",
       "      <td>19</td>\n",
       "      <td>0.576250</td>\n",
       "      <td>0.601875</td>\n",
       "      <td>0.581250</td>\n",
       "      <td>0.602500</td>\n",
       "      <td>0.600625</td>\n",
       "      <td>0.592500</td>\n",
       "      <td>0.011354</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1.497159</td>\n",
       "      <td>0.025898</td>\n",
       "      <td>0.286808</td>\n",
       "      <td>0.001830</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>500</td>\n",
       "      <td>{'classifier__criterion': 'entropy', 'classifi...</td>\n",
       "      <td>0.610159</td>\n",
       "      <td>0.645059</td>\n",
       "      <td>0.613160</td>\n",
       "      <td>0.626581</td>\n",
       "      <td>0.640397</td>\n",
       "      <td>0.627071</td>\n",
       "      <td>0.014007</td>\n",
       "      <td>29</td>\n",
       "      <td>0.622197</td>\n",
       "      <td>0.641040</td>\n",
       "      <td>0.622673</td>\n",
       "      <td>0.644199</td>\n",
       "      <td>0.629758</td>\n",
       "      <td>0.631974</td>\n",
       "      <td>0.009150</td>\n",
       "      <td>25</td>\n",
       "      <td>0.565173</td>\n",
       "      <td>0.586350</td>\n",
       "      <td>0.568486</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.586466</td>\n",
       "      <td>0.577084</td>\n",
       "      <td>0.008867</td>\n",
       "      <td>31</td>\n",
       "      <td>0.692020</td>\n",
       "      <td>0.706983</td>\n",
       "      <td>0.688279</td>\n",
       "      <td>0.726027</td>\n",
       "      <td>0.679950</td>\n",
       "      <td>0.698652</td>\n",
       "      <td>0.016249</td>\n",
       "      <td>23</td>\n",
       "      <td>0.578750</td>\n",
       "      <td>0.603125</td>\n",
       "      <td>0.581875</td>\n",
       "      <td>0.597500</td>\n",
       "      <td>0.598750</td>\n",
       "      <td>0.592000</td>\n",
       "      <td>0.009774</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.550502</td>\n",
       "      <td>0.005591</td>\n",
       "      <td>0.133316</td>\n",
       "      <td>0.001624</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>log2</td>\n",
       "      <td>200</td>\n",
       "      <td>{'classifier__criterion': 'entropy', 'classifi...</td>\n",
       "      <td>0.584136</td>\n",
       "      <td>0.623679</td>\n",
       "      <td>0.588006</td>\n",
       "      <td>0.610994</td>\n",
       "      <td>0.614771</td>\n",
       "      <td>0.604317</td>\n",
       "      <td>0.015505</td>\n",
       "      <td>59</td>\n",
       "      <td>0.609905</td>\n",
       "      <td>0.646355</td>\n",
       "      <td>0.620804</td>\n",
       "      <td>0.640787</td>\n",
       "      <td>0.619890</td>\n",
       "      <td>0.627548</td>\n",
       "      <td>0.013743</td>\n",
       "      <td>54</td>\n",
       "      <td>0.550754</td>\n",
       "      <td>0.573359</td>\n",
       "      <td>0.555665</td>\n",
       "      <td>0.571150</td>\n",
       "      <td>0.557100</td>\n",
       "      <td>0.561606</td>\n",
       "      <td>0.008973</td>\n",
       "      <td>60</td>\n",
       "      <td>0.683292</td>\n",
       "      <td>0.740648</td>\n",
       "      <td>0.703242</td>\n",
       "      <td>0.729763</td>\n",
       "      <td>0.698630</td>\n",
       "      <td>0.711115</td>\n",
       "      <td>0.021033</td>\n",
       "      <td>9</td>\n",
       "      <td>0.561875</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>0.569375</td>\n",
       "      <td>0.589375</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.576875</td>\n",
       "      <td>0.012406</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1.272258</td>\n",
       "      <td>0.010112</td>\n",
       "      <td>0.280375</td>\n",
       "      <td>0.002682</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>log2</td>\n",
       "      <td>500</td>\n",
       "      <td>{'classifier__criterion': 'entropy', 'classifi...</td>\n",
       "      <td>0.595471</td>\n",
       "      <td>0.628957</td>\n",
       "      <td>0.595458</td>\n",
       "      <td>0.620941</td>\n",
       "      <td>0.621620</td>\n",
       "      <td>0.612490</td>\n",
       "      <td>0.014182</td>\n",
       "      <td>50</td>\n",
       "      <td>0.629650</td>\n",
       "      <td>0.648796</td>\n",
       "      <td>0.619656</td>\n",
       "      <td>0.647027</td>\n",
       "      <td>0.646681</td>\n",
       "      <td>0.638362</td>\n",
       "      <td>0.011653</td>\n",
       "      <td>4</td>\n",
       "      <td>0.554606</td>\n",
       "      <td>0.577973</td>\n",
       "      <td>0.558559</td>\n",
       "      <td>0.575728</td>\n",
       "      <td>0.567136</td>\n",
       "      <td>0.566800</td>\n",
       "      <td>0.009179</td>\n",
       "      <td>54</td>\n",
       "      <td>0.728180</td>\n",
       "      <td>0.739401</td>\n",
       "      <td>0.695761</td>\n",
       "      <td>0.738481</td>\n",
       "      <td>0.752179</td>\n",
       "      <td>0.730800</td>\n",
       "      <td>0.019103</td>\n",
       "      <td>3</td>\n",
       "      <td>0.570625</td>\n",
       "      <td>0.598750</td>\n",
       "      <td>0.571875</td>\n",
       "      <td>0.595625</td>\n",
       "      <td>0.587500</td>\n",
       "      <td>0.584875</td>\n",
       "      <td>0.011722</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.675286</td>\n",
       "      <td>0.007947</td>\n",
       "      <td>0.140710</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>entropy</td>\n",
       "      <td>6</td>\n",
       "      <td>auto</td>\n",
       "      <td>200</td>\n",
       "      <td>{'classifier__criterion': 'entropy', 'classifi...</td>\n",
       "      <td>0.614193</td>\n",
       "      <td>0.637045</td>\n",
       "      <td>0.610846</td>\n",
       "      <td>0.626457</td>\n",
       "      <td>0.642575</td>\n",
       "      <td>0.626223</td>\n",
       "      <td>0.012375</td>\n",
       "      <td>31</td>\n",
       "      <td>0.623245</td>\n",
       "      <td>0.637648</td>\n",
       "      <td>0.623099</td>\n",
       "      <td>0.640798</td>\n",
       "      <td>0.622642</td>\n",
       "      <td>0.629486</td>\n",
       "      <td>0.008015</td>\n",
       "      <td>45</td>\n",
       "      <td>0.566905</td>\n",
       "      <td>0.583247</td>\n",
       "      <td>0.568345</td>\n",
       "      <td>0.577423</td>\n",
       "      <td>0.591265</td>\n",
       "      <td>0.577437</td>\n",
       "      <td>0.009149</td>\n",
       "      <td>27</td>\n",
       "      <td>0.692020</td>\n",
       "      <td>0.703242</td>\n",
       "      <td>0.689526</td>\n",
       "      <td>0.719801</td>\n",
       "      <td>0.657534</td>\n",
       "      <td>0.692425</td>\n",
       "      <td>0.020457</td>\n",
       "      <td>41</td>\n",
       "      <td>0.580625</td>\n",
       "      <td>0.599375</td>\n",
       "      <td>0.581875</td>\n",
       "      <td>0.595000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.591375</td>\n",
       "      <td>0.008454</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1.587962</td>\n",
       "      <td>0.009509</td>\n",
       "      <td>0.298216</td>\n",
       "      <td>0.011335</td>\n",
       "      <td>entropy</td>\n",
       "      <td>6</td>\n",
       "      <td>auto</td>\n",
       "      <td>500</td>\n",
       "      <td>{'classifier__criterion': 'entropy', 'classifi...</td>\n",
       "      <td>0.613637</td>\n",
       "      <td>0.639593</td>\n",
       "      <td>0.612171</td>\n",
       "      <td>0.629049</td>\n",
       "      <td>0.642128</td>\n",
       "      <td>0.627316</td>\n",
       "      <td>0.012567</td>\n",
       "      <td>27</td>\n",
       "      <td>0.619449</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.621041</td>\n",
       "      <td>0.639554</td>\n",
       "      <td>0.630966</td>\n",
       "      <td>0.629475</td>\n",
       "      <td>0.008036</td>\n",
       "      <td>47</td>\n",
       "      <td>0.563971</td>\n",
       "      <td>0.584551</td>\n",
       "      <td>0.568323</td>\n",
       "      <td>0.578629</td>\n",
       "      <td>0.592350</td>\n",
       "      <td>0.577565</td>\n",
       "      <td>0.010380</td>\n",
       "      <td>25</td>\n",
       "      <td>0.687032</td>\n",
       "      <td>0.698254</td>\n",
       "      <td>0.684539</td>\n",
       "      <td>0.714819</td>\n",
       "      <td>0.674969</td>\n",
       "      <td>0.691923</td>\n",
       "      <td>0.013640</td>\n",
       "      <td>46</td>\n",
       "      <td>0.576875</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.581250</td>\n",
       "      <td>0.595625</td>\n",
       "      <td>0.603750</td>\n",
       "      <td>0.591500</td>\n",
       "      <td>0.010567</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.678442</td>\n",
       "      <td>0.007689</td>\n",
       "      <td>0.138314</td>\n",
       "      <td>0.002798</td>\n",
       "      <td>entropy</td>\n",
       "      <td>6</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>200</td>\n",
       "      <td>{'classifier__criterion': 'entropy', 'classifi...</td>\n",
       "      <td>0.614193</td>\n",
       "      <td>0.637045</td>\n",
       "      <td>0.610846</td>\n",
       "      <td>0.626457</td>\n",
       "      <td>0.642575</td>\n",
       "      <td>0.626223</td>\n",
       "      <td>0.012375</td>\n",
       "      <td>31</td>\n",
       "      <td>0.623245</td>\n",
       "      <td>0.637648</td>\n",
       "      <td>0.623099</td>\n",
       "      <td>0.640798</td>\n",
       "      <td>0.622642</td>\n",
       "      <td>0.629486</td>\n",
       "      <td>0.008015</td>\n",
       "      <td>45</td>\n",
       "      <td>0.566905</td>\n",
       "      <td>0.583247</td>\n",
       "      <td>0.568345</td>\n",
       "      <td>0.577423</td>\n",
       "      <td>0.591265</td>\n",
       "      <td>0.577437</td>\n",
       "      <td>0.009149</td>\n",
       "      <td>27</td>\n",
       "      <td>0.692020</td>\n",
       "      <td>0.703242</td>\n",
       "      <td>0.689526</td>\n",
       "      <td>0.719801</td>\n",
       "      <td>0.657534</td>\n",
       "      <td>0.692425</td>\n",
       "      <td>0.020457</td>\n",
       "      <td>41</td>\n",
       "      <td>0.580625</td>\n",
       "      <td>0.599375</td>\n",
       "      <td>0.581875</td>\n",
       "      <td>0.595000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.591375</td>\n",
       "      <td>0.008454</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1.565696</td>\n",
       "      <td>0.020065</td>\n",
       "      <td>0.298015</td>\n",
       "      <td>0.008926</td>\n",
       "      <td>entropy</td>\n",
       "      <td>6</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>500</td>\n",
       "      <td>{'classifier__criterion': 'entropy', 'classifi...</td>\n",
       "      <td>0.613637</td>\n",
       "      <td>0.639593</td>\n",
       "      <td>0.612171</td>\n",
       "      <td>0.629049</td>\n",
       "      <td>0.642128</td>\n",
       "      <td>0.627316</td>\n",
       "      <td>0.012567</td>\n",
       "      <td>27</td>\n",
       "      <td>0.619449</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.621041</td>\n",
       "      <td>0.639554</td>\n",
       "      <td>0.630966</td>\n",
       "      <td>0.629475</td>\n",
       "      <td>0.008036</td>\n",
       "      <td>47</td>\n",
       "      <td>0.563971</td>\n",
       "      <td>0.584551</td>\n",
       "      <td>0.568323</td>\n",
       "      <td>0.578629</td>\n",
       "      <td>0.592350</td>\n",
       "      <td>0.577565</td>\n",
       "      <td>0.010380</td>\n",
       "      <td>25</td>\n",
       "      <td>0.687032</td>\n",
       "      <td>0.698254</td>\n",
       "      <td>0.684539</td>\n",
       "      <td>0.714819</td>\n",
       "      <td>0.674969</td>\n",
       "      <td>0.691923</td>\n",
       "      <td>0.013640</td>\n",
       "      <td>46</td>\n",
       "      <td>0.576875</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.581250</td>\n",
       "      <td>0.595625</td>\n",
       "      <td>0.603750</td>\n",
       "      <td>0.591500</td>\n",
       "      <td>0.010567</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.561484</td>\n",
       "      <td>0.004388</td>\n",
       "      <td>0.136144</td>\n",
       "      <td>0.001466</td>\n",
       "      <td>entropy</td>\n",
       "      <td>6</td>\n",
       "      <td>log2</td>\n",
       "      <td>200</td>\n",
       "      <td>{'classifier__criterion': 'entropy', 'classifi...</td>\n",
       "      <td>0.584013</td>\n",
       "      <td>0.626254</td>\n",
       "      <td>0.598982</td>\n",
       "      <td>0.615970</td>\n",
       "      <td>0.621575</td>\n",
       "      <td>0.609359</td>\n",
       "      <td>0.015675</td>\n",
       "      <td>54</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.640089</td>\n",
       "      <td>0.618545</td>\n",
       "      <td>0.641364</td>\n",
       "      <td>0.632940</td>\n",
       "      <td>0.626588</td>\n",
       "      <td>0.015573</td>\n",
       "      <td>56</td>\n",
       "      <td>0.548554</td>\n",
       "      <td>0.575697</td>\n",
       "      <td>0.557558</td>\n",
       "      <td>0.574384</td>\n",
       "      <td>0.576844</td>\n",
       "      <td>0.566607</td>\n",
       "      <td>0.011452</td>\n",
       "      <td>55</td>\n",
       "      <td>0.662095</td>\n",
       "      <td>0.720698</td>\n",
       "      <td>0.694514</td>\n",
       "      <td>0.726027</td>\n",
       "      <td>0.701121</td>\n",
       "      <td>0.700891</td>\n",
       "      <td>0.022672</td>\n",
       "      <td>18</td>\n",
       "      <td>0.557500</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>0.570625</td>\n",
       "      <td>0.592500</td>\n",
       "      <td>0.591875</td>\n",
       "      <td>0.581250</td>\n",
       "      <td>0.014647</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1.323233</td>\n",
       "      <td>0.098895</td>\n",
       "      <td>0.299442</td>\n",
       "      <td>0.025879</td>\n",
       "      <td>entropy</td>\n",
       "      <td>6</td>\n",
       "      <td>log2</td>\n",
       "      <td>500</td>\n",
       "      <td>{'classifier__criterion': 'entropy', 'classifi...</td>\n",
       "      <td>0.597740</td>\n",
       "      <td>0.629822</td>\n",
       "      <td>0.598590</td>\n",
       "      <td>0.625240</td>\n",
       "      <td>0.627358</td>\n",
       "      <td>0.615750</td>\n",
       "      <td>0.014434</td>\n",
       "      <td>46</td>\n",
       "      <td>0.634332</td>\n",
       "      <td>0.644162</td>\n",
       "      <td>0.620190</td>\n",
       "      <td>0.645695</td>\n",
       "      <td>0.651599</td>\n",
       "      <td>0.639196</td>\n",
       "      <td>0.011006</td>\n",
       "      <td>3</td>\n",
       "      <td>0.563408</td>\n",
       "      <td>0.579104</td>\n",
       "      <td>0.561049</td>\n",
       "      <td>0.579782</td>\n",
       "      <td>0.584570</td>\n",
       "      <td>0.573583</td>\n",
       "      <td>0.009489</td>\n",
       "      <td>42</td>\n",
       "      <td>0.725686</td>\n",
       "      <td>0.725686</td>\n",
       "      <td>0.693267</td>\n",
       "      <td>0.728518</td>\n",
       "      <td>0.735990</td>\n",
       "      <td>0.721829</td>\n",
       "      <td>0.014770</td>\n",
       "      <td>8</td>\n",
       "      <td>0.580625</td>\n",
       "      <td>0.598125</td>\n",
       "      <td>0.574375</td>\n",
       "      <td>0.598750</td>\n",
       "      <td>0.605000</td>\n",
       "      <td>0.591375</td>\n",
       "      <td>0.011749</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.715989</td>\n",
       "      <td>0.009716</td>\n",
       "      <td>0.138914</td>\n",
       "      <td>0.002754</td>\n",
       "      <td>entropy</td>\n",
       "      <td>7</td>\n",
       "      <td>auto</td>\n",
       "      <td>200</td>\n",
       "      <td>{'classifier__criterion': 'entropy', 'classifi...</td>\n",
       "      <td>0.624007</td>\n",
       "      <td>0.646267</td>\n",
       "      <td>0.614613</td>\n",
       "      <td>0.630735</td>\n",
       "      <td>0.643333</td>\n",
       "      <td>0.631791</td>\n",
       "      <td>0.011828</td>\n",
       "      <td>3</td>\n",
       "      <td>0.617895</td>\n",
       "      <td>0.647856</td>\n",
       "      <td>0.621469</td>\n",
       "      <td>0.644124</td>\n",
       "      <td>0.630610</td>\n",
       "      <td>0.632391</td>\n",
       "      <td>0.011911</td>\n",
       "      <td>17</td>\n",
       "      <td>0.563077</td>\n",
       "      <td>0.591753</td>\n",
       "      <td>0.568182</td>\n",
       "      <td>0.580420</td>\n",
       "      <td>0.586096</td>\n",
       "      <td>0.577905</td>\n",
       "      <td>0.010766</td>\n",
       "      <td>23</td>\n",
       "      <td>0.684539</td>\n",
       "      <td>0.715711</td>\n",
       "      <td>0.685786</td>\n",
       "      <td>0.723537</td>\n",
       "      <td>0.682441</td>\n",
       "      <td>0.698402</td>\n",
       "      <td>0.017536</td>\n",
       "      <td>25</td>\n",
       "      <td>0.575625</td>\n",
       "      <td>0.610000</td>\n",
       "      <td>0.581250</td>\n",
       "      <td>0.598750</td>\n",
       "      <td>0.598750</td>\n",
       "      <td>0.592875</td>\n",
       "      <td>0.012610</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1.648106</td>\n",
       "      <td>0.016644</td>\n",
       "      <td>0.297016</td>\n",
       "      <td>0.003653</td>\n",
       "      <td>entropy</td>\n",
       "      <td>7</td>\n",
       "      <td>auto</td>\n",
       "      <td>500</td>\n",
       "      <td>{'classifier__criterion': 'entropy', 'classifi...</td>\n",
       "      <td>0.618223</td>\n",
       "      <td>0.646356</td>\n",
       "      <td>0.615780</td>\n",
       "      <td>0.626248</td>\n",
       "      <td>0.642648</td>\n",
       "      <td>0.629851</td>\n",
       "      <td>0.012509</td>\n",
       "      <td>15</td>\n",
       "      <td>0.624018</td>\n",
       "      <td>0.639727</td>\n",
       "      <td>0.621469</td>\n",
       "      <td>0.639776</td>\n",
       "      <td>0.634371</td>\n",
       "      <td>0.631872</td>\n",
       "      <td>0.007750</td>\n",
       "      <td>27</td>\n",
       "      <td>0.567347</td>\n",
       "      <td>0.588482</td>\n",
       "      <td>0.568182</td>\n",
       "      <td>0.581466</td>\n",
       "      <td>0.590763</td>\n",
       "      <td>0.579248</td>\n",
       "      <td>0.009868</td>\n",
       "      <td>11</td>\n",
       "      <td>0.693267</td>\n",
       "      <td>0.700748</td>\n",
       "      <td>0.685786</td>\n",
       "      <td>0.711083</td>\n",
       "      <td>0.684932</td>\n",
       "      <td>0.695163</td>\n",
       "      <td>0.009807</td>\n",
       "      <td>37</td>\n",
       "      <td>0.581250</td>\n",
       "      <td>0.604375</td>\n",
       "      <td>0.581250</td>\n",
       "      <td>0.598125</td>\n",
       "      <td>0.603750</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>0.010436</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.709792</td>\n",
       "      <td>0.009436</td>\n",
       "      <td>0.141312</td>\n",
       "      <td>0.002244</td>\n",
       "      <td>entropy</td>\n",
       "      <td>7</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>200</td>\n",
       "      <td>{'classifier__criterion': 'entropy', 'classifi...</td>\n",
       "      <td>0.624007</td>\n",
       "      <td>0.646267</td>\n",
       "      <td>0.614613</td>\n",
       "      <td>0.630735</td>\n",
       "      <td>0.643333</td>\n",
       "      <td>0.631791</td>\n",
       "      <td>0.011828</td>\n",
       "      <td>3</td>\n",
       "      <td>0.617895</td>\n",
       "      <td>0.647856</td>\n",
       "      <td>0.621469</td>\n",
       "      <td>0.644124</td>\n",
       "      <td>0.630610</td>\n",
       "      <td>0.632391</td>\n",
       "      <td>0.011911</td>\n",
       "      <td>17</td>\n",
       "      <td>0.563077</td>\n",
       "      <td>0.591753</td>\n",
       "      <td>0.568182</td>\n",
       "      <td>0.580420</td>\n",
       "      <td>0.586096</td>\n",
       "      <td>0.577905</td>\n",
       "      <td>0.010766</td>\n",
       "      <td>23</td>\n",
       "      <td>0.684539</td>\n",
       "      <td>0.715711</td>\n",
       "      <td>0.685786</td>\n",
       "      <td>0.723537</td>\n",
       "      <td>0.682441</td>\n",
       "      <td>0.698402</td>\n",
       "      <td>0.017536</td>\n",
       "      <td>25</td>\n",
       "      <td>0.575625</td>\n",
       "      <td>0.610000</td>\n",
       "      <td>0.581250</td>\n",
       "      <td>0.598750</td>\n",
       "      <td>0.598750</td>\n",
       "      <td>0.592875</td>\n",
       "      <td>0.012610</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1.703604</td>\n",
       "      <td>0.045909</td>\n",
       "      <td>0.296217</td>\n",
       "      <td>0.002331</td>\n",
       "      <td>entropy</td>\n",
       "      <td>7</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>500</td>\n",
       "      <td>{'classifier__criterion': 'entropy', 'classifi...</td>\n",
       "      <td>0.618223</td>\n",
       "      <td>0.646356</td>\n",
       "      <td>0.615780</td>\n",
       "      <td>0.626248</td>\n",
       "      <td>0.642648</td>\n",
       "      <td>0.629851</td>\n",
       "      <td>0.012509</td>\n",
       "      <td>15</td>\n",
       "      <td>0.624018</td>\n",
       "      <td>0.639727</td>\n",
       "      <td>0.621469</td>\n",
       "      <td>0.639776</td>\n",
       "      <td>0.634371</td>\n",
       "      <td>0.631872</td>\n",
       "      <td>0.007750</td>\n",
       "      <td>27</td>\n",
       "      <td>0.567347</td>\n",
       "      <td>0.588482</td>\n",
       "      <td>0.568182</td>\n",
       "      <td>0.581466</td>\n",
       "      <td>0.590763</td>\n",
       "      <td>0.579248</td>\n",
       "      <td>0.009868</td>\n",
       "      <td>11</td>\n",
       "      <td>0.693267</td>\n",
       "      <td>0.700748</td>\n",
       "      <td>0.685786</td>\n",
       "      <td>0.711083</td>\n",
       "      <td>0.684932</td>\n",
       "      <td>0.695163</td>\n",
       "      <td>0.009807</td>\n",
       "      <td>37</td>\n",
       "      <td>0.581250</td>\n",
       "      <td>0.604375</td>\n",
       "      <td>0.581250</td>\n",
       "      <td>0.598125</td>\n",
       "      <td>0.603750</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>0.010436</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.581070</td>\n",
       "      <td>0.025540</td>\n",
       "      <td>0.136115</td>\n",
       "      <td>0.001599</td>\n",
       "      <td>entropy</td>\n",
       "      <td>7</td>\n",
       "      <td>log2</td>\n",
       "      <td>200</td>\n",
       "      <td>{'classifier__criterion': 'entropy', 'classifi...</td>\n",
       "      <td>0.578760</td>\n",
       "      <td>0.621685</td>\n",
       "      <td>0.593742</td>\n",
       "      <td>0.623523</td>\n",
       "      <td>0.622679</td>\n",
       "      <td>0.608078</td>\n",
       "      <td>0.018450</td>\n",
       "      <td>56</td>\n",
       "      <td>0.595914</td>\n",
       "      <td>0.632997</td>\n",
       "      <td>0.622197</td>\n",
       "      <td>0.649007</td>\n",
       "      <td>0.619509</td>\n",
       "      <td>0.623925</td>\n",
       "      <td>0.017427</td>\n",
       "      <td>57</td>\n",
       "      <td>0.546875</td>\n",
       "      <td>0.575510</td>\n",
       "      <td>0.565173</td>\n",
       "      <td>0.582755</td>\n",
       "      <td>0.571579</td>\n",
       "      <td>0.568378</td>\n",
       "      <td>0.012169</td>\n",
       "      <td>49</td>\n",
       "      <td>0.654613</td>\n",
       "      <td>0.703242</td>\n",
       "      <td>0.692020</td>\n",
       "      <td>0.732254</td>\n",
       "      <td>0.676214</td>\n",
       "      <td>0.691669</td>\n",
       "      <td>0.026051</td>\n",
       "      <td>48</td>\n",
       "      <td>0.555000</td>\n",
       "      <td>0.591250</td>\n",
       "      <td>0.578750</td>\n",
       "      <td>0.602500</td>\n",
       "      <td>0.583125</td>\n",
       "      <td>0.582125</td>\n",
       "      <td>0.015790</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1.326977</td>\n",
       "      <td>0.015070</td>\n",
       "      <td>0.300233</td>\n",
       "      <td>0.024038</td>\n",
       "      <td>entropy</td>\n",
       "      <td>7</td>\n",
       "      <td>log2</td>\n",
       "      <td>500</td>\n",
       "      <td>{'classifier__criterion': 'entropy', 'classifi...</td>\n",
       "      <td>0.598353</td>\n",
       "      <td>0.628098</td>\n",
       "      <td>0.597032</td>\n",
       "      <td>0.628517</td>\n",
       "      <td>0.628573</td>\n",
       "      <td>0.616114</td>\n",
       "      <td>0.015048</td>\n",
       "      <td>45</td>\n",
       "      <td>0.630939</td>\n",
       "      <td>0.634228</td>\n",
       "      <td>0.616331</td>\n",
       "      <td>0.644875</td>\n",
       "      <td>0.640896</td>\n",
       "      <td>0.633454</td>\n",
       "      <td>0.009859</td>\n",
       "      <td>11</td>\n",
       "      <td>0.566468</td>\n",
       "      <td>0.575051</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.580838</td>\n",
       "      <td>0.582485</td>\n",
       "      <td>0.572733</td>\n",
       "      <td>0.008932</td>\n",
       "      <td>43</td>\n",
       "      <td>0.711970</td>\n",
       "      <td>0.706983</td>\n",
       "      <td>0.687032</td>\n",
       "      <td>0.724782</td>\n",
       "      <td>0.712329</td>\n",
       "      <td>0.708619</td>\n",
       "      <td>0.012287</td>\n",
       "      <td>10</td>\n",
       "      <td>0.582500</td>\n",
       "      <td>0.591250</td>\n",
       "      <td>0.571250</td>\n",
       "      <td>0.599375</td>\n",
       "      <td>0.599375</td>\n",
       "      <td>0.588750</td>\n",
       "      <td>0.010746</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.752375</td>\n",
       "      <td>0.013831</td>\n",
       "      <td>0.143706</td>\n",
       "      <td>0.001325</td>\n",
       "      <td>entropy</td>\n",
       "      <td>8</td>\n",
       "      <td>auto</td>\n",
       "      <td>200</td>\n",
       "      <td>{'classifier__criterion': 'entropy', 'classifi...</td>\n",
       "      <td>0.622409</td>\n",
       "      <td>0.644774</td>\n",
       "      <td>0.613084</td>\n",
       "      <td>0.632890</td>\n",
       "      <td>0.639440</td>\n",
       "      <td>0.630519</td>\n",
       "      <td>0.011473</td>\n",
       "      <td>9</td>\n",
       "      <td>0.624297</td>\n",
       "      <td>0.645491</td>\n",
       "      <td>0.619830</td>\n",
       "      <td>0.651422</td>\n",
       "      <td>0.629265</td>\n",
       "      <td>0.634061</td>\n",
       "      <td>0.012271</td>\n",
       "      <td>9</td>\n",
       "      <td>0.568648</td>\n",
       "      <td>0.592092</td>\n",
       "      <td>0.568017</td>\n",
       "      <td>0.589899</td>\n",
       "      <td>0.587473</td>\n",
       "      <td>0.581226</td>\n",
       "      <td>0.010630</td>\n",
       "      <td>1</td>\n",
       "      <td>0.692020</td>\n",
       "      <td>0.709476</td>\n",
       "      <td>0.682045</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.677460</td>\n",
       "      <td>0.697655</td>\n",
       "      <td>0.018440</td>\n",
       "      <td>27</td>\n",
       "      <td>0.582500</td>\n",
       "      <td>0.609375</td>\n",
       "      <td>0.580625</td>\n",
       "      <td>0.609375</td>\n",
       "      <td>0.599375</td>\n",
       "      <td>0.596250</td>\n",
       "      <td>0.012550</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1.746635</td>\n",
       "      <td>0.019922</td>\n",
       "      <td>0.305611</td>\n",
       "      <td>0.001599</td>\n",
       "      <td>entropy</td>\n",
       "      <td>8</td>\n",
       "      <td>auto</td>\n",
       "      <td>500</td>\n",
       "      <td>{'classifier__criterion': 'entropy', 'classifi...</td>\n",
       "      <td>0.615793</td>\n",
       "      <td>0.644985</td>\n",
       "      <td>0.616421</td>\n",
       "      <td>0.632370</td>\n",
       "      <td>0.641565</td>\n",
       "      <td>0.630227</td>\n",
       "      <td>0.012247</td>\n",
       "      <td>13</td>\n",
       "      <td>0.621896</td>\n",
       "      <td>0.644356</td>\n",
       "      <td>0.622802</td>\n",
       "      <td>0.644295</td>\n",
       "      <td>0.631214</td>\n",
       "      <td>0.632913</td>\n",
       "      <td>0.009869</td>\n",
       "      <td>12</td>\n",
       "      <td>0.568041</td>\n",
       "      <td>0.591051</td>\n",
       "      <td>0.571280</td>\n",
       "      <td>0.584772</td>\n",
       "      <td>0.588997</td>\n",
       "      <td>0.580828</td>\n",
       "      <td>0.009396</td>\n",
       "      <td>3</td>\n",
       "      <td>0.687032</td>\n",
       "      <td>0.708229</td>\n",
       "      <td>0.684539</td>\n",
       "      <td>0.717310</td>\n",
       "      <td>0.679950</td>\n",
       "      <td>0.695412</td>\n",
       "      <td>0.014638</td>\n",
       "      <td>35</td>\n",
       "      <td>0.581250</td>\n",
       "      <td>0.608125</td>\n",
       "      <td>0.584375</td>\n",
       "      <td>0.602500</td>\n",
       "      <td>0.601250</td>\n",
       "      <td>0.595500</td>\n",
       "      <td>0.010661</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.742802</td>\n",
       "      <td>0.010777</td>\n",
       "      <td>0.144337</td>\n",
       "      <td>0.003265</td>\n",
       "      <td>entropy</td>\n",
       "      <td>8</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>200</td>\n",
       "      <td>{'classifier__criterion': 'entropy', 'classifi...</td>\n",
       "      <td>0.622409</td>\n",
       "      <td>0.644774</td>\n",
       "      <td>0.613084</td>\n",
       "      <td>0.632890</td>\n",
       "      <td>0.639440</td>\n",
       "      <td>0.630519</td>\n",
       "      <td>0.011473</td>\n",
       "      <td>9</td>\n",
       "      <td>0.624297</td>\n",
       "      <td>0.645491</td>\n",
       "      <td>0.619830</td>\n",
       "      <td>0.651422</td>\n",
       "      <td>0.629265</td>\n",
       "      <td>0.634061</td>\n",
       "      <td>0.012271</td>\n",
       "      <td>9</td>\n",
       "      <td>0.568648</td>\n",
       "      <td>0.592092</td>\n",
       "      <td>0.568017</td>\n",
       "      <td>0.589899</td>\n",
       "      <td>0.587473</td>\n",
       "      <td>0.581226</td>\n",
       "      <td>0.010630</td>\n",
       "      <td>1</td>\n",
       "      <td>0.692020</td>\n",
       "      <td>0.709476</td>\n",
       "      <td>0.682045</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.677460</td>\n",
       "      <td>0.697655</td>\n",
       "      <td>0.018440</td>\n",
       "      <td>27</td>\n",
       "      <td>0.582500</td>\n",
       "      <td>0.609375</td>\n",
       "      <td>0.580625</td>\n",
       "      <td>0.609375</td>\n",
       "      <td>0.599375</td>\n",
       "      <td>0.596250</td>\n",
       "      <td>0.012550</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>1.751150</td>\n",
       "      <td>0.017356</td>\n",
       "      <td>0.305411</td>\n",
       "      <td>0.005159</td>\n",
       "      <td>entropy</td>\n",
       "      <td>8</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>500</td>\n",
       "      <td>{'classifier__criterion': 'entropy', 'classifi...</td>\n",
       "      <td>0.615793</td>\n",
       "      <td>0.644985</td>\n",
       "      <td>0.616421</td>\n",
       "      <td>0.632370</td>\n",
       "      <td>0.641565</td>\n",
       "      <td>0.630227</td>\n",
       "      <td>0.012247</td>\n",
       "      <td>13</td>\n",
       "      <td>0.621896</td>\n",
       "      <td>0.644356</td>\n",
       "      <td>0.622802</td>\n",
       "      <td>0.644295</td>\n",
       "      <td>0.631214</td>\n",
       "      <td>0.632913</td>\n",
       "      <td>0.009869</td>\n",
       "      <td>12</td>\n",
       "      <td>0.568041</td>\n",
       "      <td>0.591051</td>\n",
       "      <td>0.571280</td>\n",
       "      <td>0.584772</td>\n",
       "      <td>0.588997</td>\n",
       "      <td>0.580828</td>\n",
       "      <td>0.009396</td>\n",
       "      <td>3</td>\n",
       "      <td>0.687032</td>\n",
       "      <td>0.708229</td>\n",
       "      <td>0.684539</td>\n",
       "      <td>0.717310</td>\n",
       "      <td>0.679950</td>\n",
       "      <td>0.695412</td>\n",
       "      <td>0.014638</td>\n",
       "      <td>35</td>\n",
       "      <td>0.581250</td>\n",
       "      <td>0.608125</td>\n",
       "      <td>0.584375</td>\n",
       "      <td>0.602500</td>\n",
       "      <td>0.601250</td>\n",
       "      <td>0.595500</td>\n",
       "      <td>0.010661</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.587910</td>\n",
       "      <td>0.004649</td>\n",
       "      <td>0.138719</td>\n",
       "      <td>0.000749</td>\n",
       "      <td>entropy</td>\n",
       "      <td>8</td>\n",
       "      <td>log2</td>\n",
       "      <td>200</td>\n",
       "      <td>{'classifier__criterion': 'entropy', 'classifi...</td>\n",
       "      <td>0.585329</td>\n",
       "      <td>0.630302</td>\n",
       "      <td>0.590726</td>\n",
       "      <td>0.621010</td>\n",
       "      <td>0.627847</td>\n",
       "      <td>0.611043</td>\n",
       "      <td>0.019113</td>\n",
       "      <td>52</td>\n",
       "      <td>0.593607</td>\n",
       "      <td>0.636821</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>0.641196</td>\n",
       "      <td>0.626205</td>\n",
       "      <td>0.623704</td>\n",
       "      <td>0.016728</td>\n",
       "      <td>58</td>\n",
       "      <td>0.547368</td>\n",
       "      <td>0.577665</td>\n",
       "      <td>0.560241</td>\n",
       "      <td>0.577268</td>\n",
       "      <td>0.575000</td>\n",
       "      <td>0.567509</td>\n",
       "      <td>0.011941</td>\n",
       "      <td>51</td>\n",
       "      <td>0.648379</td>\n",
       "      <td>0.709476</td>\n",
       "      <td>0.695761</td>\n",
       "      <td>0.721046</td>\n",
       "      <td>0.687422</td>\n",
       "      <td>0.692417</td>\n",
       "      <td>0.024844</td>\n",
       "      <td>43</td>\n",
       "      <td>0.555000</td>\n",
       "      <td>0.594375</td>\n",
       "      <td>0.573750</td>\n",
       "      <td>0.595000</td>\n",
       "      <td>0.588125</td>\n",
       "      <td>0.581250</td>\n",
       "      <td>0.015191</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>1.373012</td>\n",
       "      <td>0.087737</td>\n",
       "      <td>0.297250</td>\n",
       "      <td>0.003606</td>\n",
       "      <td>entropy</td>\n",
       "      <td>8</td>\n",
       "      <td>log2</td>\n",
       "      <td>500</td>\n",
       "      <td>{'classifier__criterion': 'entropy', 'classifi...</td>\n",
       "      <td>0.602186</td>\n",
       "      <td>0.636879</td>\n",
       "      <td>0.597138</td>\n",
       "      <td>0.629051</td>\n",
       "      <td>0.633842</td>\n",
       "      <td>0.619819</td>\n",
       "      <td>0.016723</td>\n",
       "      <td>42</td>\n",
       "      <td>0.624303</td>\n",
       "      <td>0.634670</td>\n",
       "      <td>0.619233</td>\n",
       "      <td>0.636721</td>\n",
       "      <td>0.648709</td>\n",
       "      <td>0.632727</td>\n",
       "      <td>0.010278</td>\n",
       "      <td>14</td>\n",
       "      <td>0.564516</td>\n",
       "      <td>0.579979</td>\n",
       "      <td>0.558676</td>\n",
       "      <td>0.579755</td>\n",
       "      <td>0.590398</td>\n",
       "      <td>0.574665</td>\n",
       "      <td>0.011492</td>\n",
       "      <td>41</td>\n",
       "      <td>0.698254</td>\n",
       "      <td>0.700748</td>\n",
       "      <td>0.694514</td>\n",
       "      <td>0.706102</td>\n",
       "      <td>0.719801</td>\n",
       "      <td>0.703884</td>\n",
       "      <td>0.008804</td>\n",
       "      <td>13</td>\n",
       "      <td>0.578750</td>\n",
       "      <td>0.595625</td>\n",
       "      <td>0.571875</td>\n",
       "      <td>0.595625</td>\n",
       "      <td>0.608750</td>\n",
       "      <td>0.590125</td>\n",
       "      <td>0.013190</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        0.614637      0.012429         0.133190        0.000975   \n",
       "1        1.414895      0.052682         0.278257        0.001220   \n",
       "2        0.604426      0.002314         0.133517        0.001019   \n",
       "3        1.405861      0.033879         0.277828        0.002096   \n",
       "4        0.552373      0.027854         0.132140        0.003648   \n",
       "5        1.248124      0.031093         0.289824        0.031066   \n",
       "6        0.637909      0.006190         0.134833        0.000673   \n",
       "7        1.483945      0.025338         0.285308        0.003228   \n",
       "8        0.634473      0.003329         0.135550        0.001229   \n",
       "9        1.492746      0.044471         0.293618        0.011679   \n",
       "10       0.544089      0.003051         0.132517        0.000799   \n",
       "11       1.261127      0.014927         0.288621        0.018334   \n",
       "12       0.665351      0.004558         0.139602        0.002534   \n",
       "13       1.606794      0.133745         0.317934        0.033516   \n",
       "14       0.678823      0.010622         0.137115        0.000979   \n",
       "15       1.574796      0.017881         0.302278        0.018876   \n",
       "16       0.559676      0.003329         0.134636        0.000628   \n",
       "17       1.293071      0.018681         0.284031        0.002590   \n",
       "18       0.716295      0.015048         0.140513        0.001743   \n",
       "19       1.655672      0.018399         0.298640        0.001979   \n",
       "20       0.715739      0.017276         0.140439        0.001342   \n",
       "21       1.671541      0.033684         0.302612        0.004618   \n",
       "22       0.567735      0.004235         0.137315        0.002243   \n",
       "23       1.305767      0.008277         0.293613        0.005774   \n",
       "24       0.752681      0.015431         0.155304        0.026791   \n",
       "25       1.738606      0.009045         0.307554        0.001427   \n",
       "26       0.747338      0.009881         0.144510        0.002415   \n",
       "27       1.749946      0.021520         0.307032        0.002393   \n",
       "28       0.630612      0.051648         0.147145        0.010985   \n",
       "29       1.341653      0.018557         0.300035        0.010630   \n",
       "30       0.613221      0.010378         0.135915        0.003630   \n",
       "31       1.526328      0.151030         0.295402        0.015606   \n",
       "32       0.625724      0.022259         0.136520        0.001639   \n",
       "33       1.434417      0.021688         0.280452        0.001503   \n",
       "34       0.551060      0.013857         0.133317        0.001624   \n",
       "35       1.246591      0.015093         0.274043        0.004847   \n",
       "36       0.640549      0.004267         0.136350        0.002478   \n",
       "37       1.495935      0.007676         0.298606        0.022701   \n",
       "38       0.644530      0.011714         0.141959        0.005716   \n",
       "39       1.497159      0.025898         0.286808        0.001830   \n",
       "40       0.550502      0.005591         0.133316        0.001624   \n",
       "41       1.272258      0.010112         0.280375        0.002682   \n",
       "42       0.675286      0.007947         0.140710        0.002700   \n",
       "43       1.587962      0.009509         0.298216        0.011335   \n",
       "44       0.678442      0.007689         0.138314        0.002798   \n",
       "45       1.565696      0.020065         0.298015        0.008926   \n",
       "46       0.561484      0.004388         0.136144        0.001466   \n",
       "47       1.323233      0.098895         0.299442        0.025879   \n",
       "48       0.715989      0.009716         0.138914        0.002754   \n",
       "49       1.648106      0.016644         0.297016        0.003653   \n",
       "50       0.709792      0.009436         0.141312        0.002244   \n",
       "51       1.703604      0.045909         0.296217        0.002331   \n",
       "52       0.581070      0.025540         0.136115        0.001599   \n",
       "53       1.326977      0.015070         0.300233        0.024038   \n",
       "54       0.752375      0.013831         0.143706        0.001325   \n",
       "55       1.746635      0.019922         0.305611        0.001599   \n",
       "56       0.742802      0.010777         0.144337        0.003265   \n",
       "57       1.751150      0.017356         0.305411        0.005159   \n",
       "58       0.587910      0.004649         0.138719        0.000749   \n",
       "59       1.373012      0.087737         0.297250        0.003606   \n",
       "\n",
       "   param_classifier__criterion param_classifier__max_depth  \\\n",
       "0                         gini                           4   \n",
       "1                         gini                           4   \n",
       "2                         gini                           4   \n",
       "3                         gini                           4   \n",
       "4                         gini                           4   \n",
       "5                         gini                           4   \n",
       "6                         gini                           5   \n",
       "7                         gini                           5   \n",
       "8                         gini                           5   \n",
       "9                         gini                           5   \n",
       "10                        gini                           5   \n",
       "11                        gini                           5   \n",
       "12                        gini                           6   \n",
       "13                        gini                           6   \n",
       "14                        gini                           6   \n",
       "15                        gini                           6   \n",
       "16                        gini                           6   \n",
       "17                        gini                           6   \n",
       "18                        gini                           7   \n",
       "19                        gini                           7   \n",
       "20                        gini                           7   \n",
       "21                        gini                           7   \n",
       "22                        gini                           7   \n",
       "23                        gini                           7   \n",
       "24                        gini                           8   \n",
       "25                        gini                           8   \n",
       "26                        gini                           8   \n",
       "27                        gini                           8   \n",
       "28                        gini                           8   \n",
       "29                        gini                           8   \n",
       "30                     entropy                           4   \n",
       "31                     entropy                           4   \n",
       "32                     entropy                           4   \n",
       "33                     entropy                           4   \n",
       "34                     entropy                           4   \n",
       "35                     entropy                           4   \n",
       "36                     entropy                           5   \n",
       "37                     entropy                           5   \n",
       "38                     entropy                           5   \n",
       "39                     entropy                           5   \n",
       "40                     entropy                           5   \n",
       "41                     entropy                           5   \n",
       "42                     entropy                           6   \n",
       "43                     entropy                           6   \n",
       "44                     entropy                           6   \n",
       "45                     entropy                           6   \n",
       "46                     entropy                           6   \n",
       "47                     entropy                           6   \n",
       "48                     entropy                           7   \n",
       "49                     entropy                           7   \n",
       "50                     entropy                           7   \n",
       "51                     entropy                           7   \n",
       "52                     entropy                           7   \n",
       "53                     entropy                           7   \n",
       "54                     entropy                           8   \n",
       "55                     entropy                           8   \n",
       "56                     entropy                           8   \n",
       "57                     entropy                           8   \n",
       "58                     entropy                           8   \n",
       "59                     entropy                           8   \n",
       "\n",
       "   param_classifier__max_features param_classifier__n_estimators  \\\n",
       "0                            auto                            200   \n",
       "1                            auto                            500   \n",
       "2                            sqrt                            200   \n",
       "3                            sqrt                            500   \n",
       "4                            log2                            200   \n",
       "5                            log2                            500   \n",
       "6                            auto                            200   \n",
       "7                            auto                            500   \n",
       "8                            sqrt                            200   \n",
       "9                            sqrt                            500   \n",
       "10                           log2                            200   \n",
       "11                           log2                            500   \n",
       "12                           auto                            200   \n",
       "13                           auto                            500   \n",
       "14                           sqrt                            200   \n",
       "15                           sqrt                            500   \n",
       "16                           log2                            200   \n",
       "17                           log2                            500   \n",
       "18                           auto                            200   \n",
       "19                           auto                            500   \n",
       "20                           sqrt                            200   \n",
       "21                           sqrt                            500   \n",
       "22                           log2                            200   \n",
       "23                           log2                            500   \n",
       "24                           auto                            200   \n",
       "25                           auto                            500   \n",
       "26                           sqrt                            200   \n",
       "27                           sqrt                            500   \n",
       "28                           log2                            200   \n",
       "29                           log2                            500   \n",
       "30                           auto                            200   \n",
       "31                           auto                            500   \n",
       "32                           sqrt                            200   \n",
       "33                           sqrt                            500   \n",
       "34                           log2                            200   \n",
       "35                           log2                            500   \n",
       "36                           auto                            200   \n",
       "37                           auto                            500   \n",
       "38                           sqrt                            200   \n",
       "39                           sqrt                            500   \n",
       "40                           log2                            200   \n",
       "41                           log2                            500   \n",
       "42                           auto                            200   \n",
       "43                           auto                            500   \n",
       "44                           sqrt                            200   \n",
       "45                           sqrt                            500   \n",
       "46                           log2                            200   \n",
       "47                           log2                            500   \n",
       "48                           auto                            200   \n",
       "49                           auto                            500   \n",
       "50                           sqrt                            200   \n",
       "51                           sqrt                            500   \n",
       "52                           log2                            200   \n",
       "53                           log2                            500   \n",
       "54                           auto                            200   \n",
       "55                           auto                            500   \n",
       "56                           sqrt                            200   \n",
       "57                           sqrt                            500   \n",
       "58                           log2                            200   \n",
       "59                           log2                            500   \n",
       "\n",
       "                                               params  split0_test_AUC  \\\n",
       "0   {'classifier__criterion': 'gini', 'classifier_...         0.613044   \n",
       "1   {'classifier__criterion': 'gini', 'classifier_...         0.611952   \n",
       "2   {'classifier__criterion': 'gini', 'classifier_...         0.613044   \n",
       "3   {'classifier__criterion': 'gini', 'classifier_...         0.611952   \n",
       "4   {'classifier__criterion': 'gini', 'classifier_...         0.591492   \n",
       "5   {'classifier__criterion': 'gini', 'classifier_...         0.601905   \n",
       "6   {'classifier__criterion': 'gini', 'classifier_...         0.611729   \n",
       "7   {'classifier__criterion': 'gini', 'classifier_...         0.612840   \n",
       "8   {'classifier__criterion': 'gini', 'classifier_...         0.611729   \n",
       "9   {'classifier__criterion': 'gini', 'classifier_...         0.612840   \n",
       "10  {'classifier__criterion': 'gini', 'classifier_...         0.586392   \n",
       "11  {'classifier__criterion': 'gini', 'classifier_...         0.597817   \n",
       "12  {'classifier__criterion': 'gini', 'classifier_...         0.617146   \n",
       "13  {'classifier__criterion': 'gini', 'classifier_...         0.615787   \n",
       "14  {'classifier__criterion': 'gini', 'classifier_...         0.617146   \n",
       "15  {'classifier__criterion': 'gini', 'classifier_...         0.615787   \n",
       "16  {'classifier__criterion': 'gini', 'classifier_...         0.586279   \n",
       "17  {'classifier__criterion': 'gini', 'classifier_...         0.597600   \n",
       "18  {'classifier__criterion': 'gini', 'classifier_...         0.619519   \n",
       "19  {'classifier__criterion': 'gini', 'classifier_...         0.616366   \n",
       "20  {'classifier__criterion': 'gini', 'classifier_...         0.619519   \n",
       "21  {'classifier__criterion': 'gini', 'classifier_...         0.616366   \n",
       "22  {'classifier__criterion': 'gini', 'classifier_...         0.584013   \n",
       "23  {'classifier__criterion': 'gini', 'classifier_...         0.599644   \n",
       "24  {'classifier__criterion': 'gini', 'classifier_...         0.617415   \n",
       "25  {'classifier__criterion': 'gini', 'classifier_...         0.615927   \n",
       "26  {'classifier__criterion': 'gini', 'classifier_...         0.617415   \n",
       "27  {'classifier__criterion': 'gini', 'classifier_...         0.615927   \n",
       "28  {'classifier__criterion': 'gini', 'classifier_...         0.588041   \n",
       "29  {'classifier__criterion': 'gini', 'classifier_...         0.603474   \n",
       "30  {'classifier__criterion': 'entropy', 'classifi...         0.610620   \n",
       "31  {'classifier__criterion': 'entropy', 'classifi...         0.610098   \n",
       "32  {'classifier__criterion': 'entropy', 'classifi...         0.610620   \n",
       "33  {'classifier__criterion': 'entropy', 'classifi...         0.610098   \n",
       "34  {'classifier__criterion': 'entropy', 'classifi...         0.587026   \n",
       "35  {'classifier__criterion': 'entropy', 'classifi...         0.599634   \n",
       "36  {'classifier__criterion': 'entropy', 'classifi...         0.611043   \n",
       "37  {'classifier__criterion': 'entropy', 'classifi...         0.610159   \n",
       "38  {'classifier__criterion': 'entropy', 'classifi...         0.611043   \n",
       "39  {'classifier__criterion': 'entropy', 'classifi...         0.610159   \n",
       "40  {'classifier__criterion': 'entropy', 'classifi...         0.584136   \n",
       "41  {'classifier__criterion': 'entropy', 'classifi...         0.595471   \n",
       "42  {'classifier__criterion': 'entropy', 'classifi...         0.614193   \n",
       "43  {'classifier__criterion': 'entropy', 'classifi...         0.613637   \n",
       "44  {'classifier__criterion': 'entropy', 'classifi...         0.614193   \n",
       "45  {'classifier__criterion': 'entropy', 'classifi...         0.613637   \n",
       "46  {'classifier__criterion': 'entropy', 'classifi...         0.584013   \n",
       "47  {'classifier__criterion': 'entropy', 'classifi...         0.597740   \n",
       "48  {'classifier__criterion': 'entropy', 'classifi...         0.624007   \n",
       "49  {'classifier__criterion': 'entropy', 'classifi...         0.618223   \n",
       "50  {'classifier__criterion': 'entropy', 'classifi...         0.624007   \n",
       "51  {'classifier__criterion': 'entropy', 'classifi...         0.618223   \n",
       "52  {'classifier__criterion': 'entropy', 'classifi...         0.578760   \n",
       "53  {'classifier__criterion': 'entropy', 'classifi...         0.598353   \n",
       "54  {'classifier__criterion': 'entropy', 'classifi...         0.622409   \n",
       "55  {'classifier__criterion': 'entropy', 'classifi...         0.615793   \n",
       "56  {'classifier__criterion': 'entropy', 'classifi...         0.622409   \n",
       "57  {'classifier__criterion': 'entropy', 'classifi...         0.615793   \n",
       "58  {'classifier__criterion': 'entropy', 'classifi...         0.585329   \n",
       "59  {'classifier__criterion': 'entropy', 'classifi...         0.602186   \n",
       "\n",
       "    split1_test_AUC  split2_test_AUC  split3_test_AUC  split4_test_AUC  \\\n",
       "0          0.642552         0.605176         0.625003         0.637132   \n",
       "1          0.642968         0.609313         0.624326         0.636976   \n",
       "2          0.642552         0.605176         0.625003         0.637132   \n",
       "3          0.642968         0.609313         0.624326         0.636976   \n",
       "4          0.626086         0.591423         0.609137         0.611161   \n",
       "5          0.635799         0.599987         0.618952         0.620361   \n",
       "6          0.641802         0.610959         0.632507         0.644059   \n",
       "7          0.646523         0.612276         0.630623         0.642453   \n",
       "8          0.641802         0.610959         0.632507         0.644059   \n",
       "9          0.646523         0.612276         0.630623         0.642453   \n",
       "10         0.623357         0.587444         0.610341         0.613508   \n",
       "11         0.630983         0.596448         0.619739         0.621008   \n",
       "12         0.640520         0.610404         0.631481         0.642968   \n",
       "13         0.643601         0.609687         0.631537         0.642545   \n",
       "14         0.640520         0.610404         0.631481         0.642968   \n",
       "15         0.643601         0.609687         0.631537         0.642545   \n",
       "16         0.624566         0.601087         0.615580         0.619969   \n",
       "17         0.632043         0.600631         0.627548         0.625736   \n",
       "18         0.645710         0.614029         0.633723         0.644672   \n",
       "19         0.646471         0.615934         0.628656         0.644065   \n",
       "20         0.645710         0.614029         0.633723         0.644672   \n",
       "21         0.646471         0.615934         0.628656         0.644065   \n",
       "22         0.623935         0.594944         0.622007         0.621323   \n",
       "23         0.630757         0.597997         0.628879         0.627737   \n",
       "24         0.649660         0.615063         0.637617         0.639286   \n",
       "25         0.646971         0.617404         0.633285         0.643678   \n",
       "26         0.649660         0.615063         0.637617         0.639286   \n",
       "27         0.646971         0.617404         0.633285         0.643678   \n",
       "28         0.630738         0.590147         0.622369         0.626515   \n",
       "29         0.639674         0.597632         0.629960         0.637565   \n",
       "30         0.641967         0.608073         0.627693         0.639708   \n",
       "31         0.641238         0.610259         0.622706         0.637282   \n",
       "32         0.641967         0.608073         0.627693         0.639708   \n",
       "33         0.641238         0.610259         0.622706         0.637282   \n",
       "34         0.622221         0.589951         0.615048         0.611066   \n",
       "35         0.632319         0.599658         0.620578         0.619765   \n",
       "36         0.643271         0.609419         0.630101         0.644673   \n",
       "37         0.645059         0.613160         0.626581         0.640397   \n",
       "38         0.643271         0.609419         0.630101         0.644673   \n",
       "39         0.645059         0.613160         0.626581         0.640397   \n",
       "40         0.623679         0.588006         0.610994         0.614771   \n",
       "41         0.628957         0.595458         0.620941         0.621620   \n",
       "42         0.637045         0.610846         0.626457         0.642575   \n",
       "43         0.639593         0.612171         0.629049         0.642128   \n",
       "44         0.637045         0.610846         0.626457         0.642575   \n",
       "45         0.639593         0.612171         0.629049         0.642128   \n",
       "46         0.626254         0.598982         0.615970         0.621575   \n",
       "47         0.629822         0.598590         0.625240         0.627358   \n",
       "48         0.646267         0.614613         0.630735         0.643333   \n",
       "49         0.646356         0.615780         0.626248         0.642648   \n",
       "50         0.646267         0.614613         0.630735         0.643333   \n",
       "51         0.646356         0.615780         0.626248         0.642648   \n",
       "52         0.621685         0.593742         0.623523         0.622679   \n",
       "53         0.628098         0.597032         0.628517         0.628573   \n",
       "54         0.644774         0.613084         0.632890         0.639440   \n",
       "55         0.644985         0.616421         0.632370         0.641565   \n",
       "56         0.644774         0.613084         0.632890         0.639440   \n",
       "57         0.644985         0.616421         0.632370         0.641565   \n",
       "58         0.630302         0.590726         0.621010         0.627847   \n",
       "59         0.636879         0.597138         0.629051         0.633842   \n",
       "\n",
       "    mean_test_AUC  std_test_AUC  rank_test_AUC  split0_test_F1  \\\n",
       "0        0.624581      0.014074             37        0.620728   \n",
       "1        0.625107      0.013289             35        0.621576   \n",
       "2        0.624581      0.014074             37        0.620728   \n",
       "3        0.625107      0.013289             35        0.621576   \n",
       "4        0.605860      0.013136             57        0.621681   \n",
       "5        0.615401      0.013214             47        0.636853   \n",
       "6        0.628211      0.014308             23        0.620728   \n",
       "7        0.628943      0.014363             17        0.619608   \n",
       "8        0.628211      0.014308             23        0.620728   \n",
       "9        0.628943      0.014363             17        0.619608   \n",
       "10       0.604209      0.014760             60        0.614782   \n",
       "11       0.613199      0.013691             49        0.629006   \n",
       "12       0.628504      0.012799             21        0.617813   \n",
       "13       0.628631      0.013784             19        0.618079   \n",
       "14       0.628504      0.012799             21        0.617813   \n",
       "15       0.628631      0.013784             19        0.618079   \n",
       "16       0.609496      0.014025             53        0.609921   \n",
       "17       0.616712      0.014545             44        0.625954   \n",
       "18       0.631531      0.012878              5        0.621972   \n",
       "19       0.630298      0.013071             11        0.617730   \n",
       "20       0.631531      0.012878              5        0.621972   \n",
       "21       0.630298      0.013071             11        0.617730   \n",
       "22       0.609244      0.016527             55        0.600114   \n",
       "23       0.617003      0.014886             43        0.625828   \n",
       "24       0.631808      0.013386              1        0.616601   \n",
       "25       0.631453      0.012900              7        0.623245   \n",
       "26       0.631808      0.013386              1        0.616601   \n",
       "27       0.631453      0.012900              7        0.623245   \n",
       "28       0.611562      0.018547             51        0.593732   \n",
       "29       0.621661      0.017632             41        0.626039   \n",
       "30       0.625612      0.014163             33        0.622074   \n",
       "31       0.624316      0.013091             39        0.625139   \n",
       "32       0.625612      0.014163             33        0.622074   \n",
       "33       0.624316      0.013091             39        0.625139   \n",
       "34       0.605062      0.014028             58        0.621111   \n",
       "35       0.614391      0.012833             48        0.634989   \n",
       "36       0.627702      0.015152             25        0.620805   \n",
       "37       0.627071      0.014007             29        0.622197   \n",
       "38       0.627702      0.015152             25        0.620805   \n",
       "39       0.627071      0.014007             29        0.622197   \n",
       "40       0.604317      0.015505             59        0.609905   \n",
       "41       0.612490      0.014182             50        0.629650   \n",
       "42       0.626223      0.012375             31        0.623245   \n",
       "43       0.627316      0.012567             27        0.619449   \n",
       "44       0.626223      0.012375             31        0.623245   \n",
       "45       0.627316      0.012567             27        0.619449   \n",
       "46       0.609359      0.015675             54        0.600000   \n",
       "47       0.615750      0.014434             46        0.634332   \n",
       "48       0.631791      0.011828              3        0.617895   \n",
       "49       0.629851      0.012509             15        0.624018   \n",
       "50       0.631791      0.011828              3        0.617895   \n",
       "51       0.629851      0.012509             15        0.624018   \n",
       "52       0.608078      0.018450             56        0.595914   \n",
       "53       0.616114      0.015048             45        0.630939   \n",
       "54       0.630519      0.011473              9        0.624297   \n",
       "55       0.630227      0.012247             13        0.621896   \n",
       "56       0.630519      0.011473              9        0.624297   \n",
       "57       0.630227      0.012247             13        0.621896   \n",
       "58       0.611043      0.019113             52        0.593607   \n",
       "59       0.619819      0.016723             42        0.624303   \n",
       "\n",
       "    split1_test_F1  split2_test_F1  split3_test_F1  split4_test_F1  \\\n",
       "0         0.641360        0.619830        0.644701        0.624856   \n",
       "1         0.639047        0.621820        0.639600        0.626659   \n",
       "2         0.641360        0.619830        0.644701        0.624856   \n",
       "3         0.639047        0.621820        0.639600        0.626659   \n",
       "4         0.652861        0.632708        0.645092        0.626440   \n",
       "5         0.660657        0.625478        0.645301        0.652654   \n",
       "6         0.641084        0.622524        0.648498        0.628704   \n",
       "7         0.643301        0.621668        0.640625        0.632065   \n",
       "8         0.641084        0.622524        0.648498        0.628704   \n",
       "9         0.643301        0.621668        0.640625        0.632065   \n",
       "10        0.641180        0.621978        0.638600        0.619765   \n",
       "11        0.647764        0.625277        0.643288        0.645230   \n",
       "12        0.640821        0.617312        0.639955        0.629305   \n",
       "13        0.638952        0.620533        0.634064        0.632736   \n",
       "14        0.640821        0.617312        0.639955        0.629305   \n",
       "15        0.638952        0.620533        0.634064        0.632736   \n",
       "16        0.637266        0.621111        0.641676        0.631461   \n",
       "17        0.644948        0.621951        0.646086        0.650219   \n",
       "18        0.646259        0.620296        0.646205        0.616725   \n",
       "19        0.640000        0.617228        0.636569        0.637565   \n",
       "20        0.646259        0.620296        0.646205        0.616725   \n",
       "21        0.640000        0.617228        0.636569        0.637565   \n",
       "22        0.628090        0.624790        0.643372        0.619429   \n",
       "23        0.634444        0.616071        0.643729        0.639106   \n",
       "24        0.641187        0.616438        0.647746        0.629930   \n",
       "25        0.642327        0.614857        0.635334        0.639816   \n",
       "26        0.641187        0.616438        0.647746        0.629930   \n",
       "27        0.642327        0.614857        0.635334        0.639816   \n",
       "28        0.630926        0.618304        0.646700        0.617580   \n",
       "29        0.629797        0.616667        0.635903        0.643820   \n",
       "30        0.642534        0.617997        0.652103        0.626866   \n",
       "31        0.637188        0.625282        0.643094        0.629758   \n",
       "32        0.642534        0.617997        0.652103        0.626866   \n",
       "33        0.637188        0.625282        0.643094        0.629758   \n",
       "34        0.648796        0.636704        0.654526        0.624793   \n",
       "35        0.663462        0.627408        0.651491        0.656811   \n",
       "36        0.641934        0.623172        0.647059        0.630422   \n",
       "37        0.641040        0.622673        0.644199        0.629758   \n",
       "38        0.641934        0.623172        0.647059        0.630422   \n",
       "39        0.641040        0.622673        0.644199        0.629758   \n",
       "40        0.646355        0.620804        0.640787        0.619890   \n",
       "41        0.648796        0.619656        0.647027        0.646681   \n",
       "42        0.637648        0.623099        0.640798        0.622642   \n",
       "43        0.636364        0.621041        0.639554        0.630966   \n",
       "44        0.637648        0.623099        0.640798        0.622642   \n",
       "45        0.636364        0.621041        0.639554        0.630966   \n",
       "46        0.640089        0.618545        0.641364        0.632940   \n",
       "47        0.644162        0.620190        0.645695        0.651599   \n",
       "48        0.647856        0.621469        0.644124        0.630610   \n",
       "49        0.639727        0.621469        0.639776        0.634371   \n",
       "50        0.647856        0.621469        0.644124        0.630610   \n",
       "51        0.639727        0.621469        0.639776        0.634371   \n",
       "52        0.632997        0.622197        0.649007        0.619509   \n",
       "53        0.634228        0.616331        0.644875        0.640896   \n",
       "54        0.645491        0.619830        0.651422        0.629265   \n",
       "55        0.644356        0.622802        0.644295        0.631214   \n",
       "56        0.645491        0.619830        0.651422        0.629265   \n",
       "57        0.644356        0.622802        0.644295        0.631214   \n",
       "58        0.636821        0.620690        0.641196        0.626205   \n",
       "59        0.634670        0.619233        0.636721        0.648709   \n",
       "\n",
       "    mean_test_F1  std_test_F1  rank_test_F1  split0_test_Precision  \\\n",
       "0       0.630295     0.010588            37               0.563581   \n",
       "1       0.629740     0.008033            43               0.563323   \n",
       "2       0.630295     0.010588            37               0.563581   \n",
       "3       0.629740     0.008033            43               0.563323   \n",
       "4       0.635756     0.011610             8               0.558648   \n",
       "5       0.644189     0.012231             2               0.560721   \n",
       "6       0.632308     0.010788            21               0.563581   \n",
       "7       0.631453     0.009601            30               0.562564   \n",
       "8       0.632308     0.010788            21               0.563581   \n",
       "9       0.631453     0.009601            30               0.562564   \n",
       "10      0.627261     0.010603            55               0.557927   \n",
       "11      0.638113     0.009146             5               0.557267   \n",
       "12      0.629041     0.010214            49               0.563786   \n",
       "13      0.628873     0.008118            51               0.565083   \n",
       "14      0.629041     0.010214            49               0.563786   \n",
       "15      0.628873     0.008118            51               0.565083   \n",
       "16      0.628287     0.011478            53               0.556584   \n",
       "17      0.637832     0.011537             6               0.556202   \n",
       "18      0.630291     0.013125            39               0.567318   \n",
       "19      0.629818     0.010138            41               0.564499   \n",
       "20      0.630291     0.013125            39               0.567318   \n",
       "21      0.629818     0.010138            41               0.564499   \n",
       "22      0.623159     0.014004            59               0.553102   \n",
       "23      0.631836     0.009856            29               0.561386   \n",
       "24      0.630380     0.012671            35               0.563467   \n",
       "25      0.631116     0.010445            32               0.566905   \n",
       "26      0.630380     0.012671            35               0.563467   \n",
       "27      0.631116     0.010445            32               0.566905   \n",
       "28      0.621448     0.017438            60               0.546695   \n",
       "29      0.630445     0.009147            34               0.563310   \n",
       "30      0.632314     0.012927            19               0.562500   \n",
       "31      0.632092     0.007031            23               0.564257   \n",
       "32      0.632314     0.012927            19               0.562500   \n",
       "33      0.632092     0.007031            23               0.564257   \n",
       "34      0.637186     0.013021             7               0.560120   \n",
       "35      0.646832     0.013531             1               0.560000   \n",
       "36      0.632678     0.010285            15               0.562880   \n",
       "37      0.631974     0.009150            25               0.565173   \n",
       "38      0.632678     0.010285            15               0.562880   \n",
       "39      0.631974     0.009150            25               0.565173   \n",
       "40      0.627548     0.013743            54               0.550754   \n",
       "41      0.638362     0.011653             4               0.554606   \n",
       "42      0.629486     0.008015            45               0.566905   \n",
       "43      0.629475     0.008036            47               0.563971   \n",
       "44      0.629486     0.008015            45               0.566905   \n",
       "45      0.629475     0.008036            47               0.563971   \n",
       "46      0.626588     0.015573            56               0.548554   \n",
       "47      0.639196     0.011006             3               0.563408   \n",
       "48      0.632391     0.011911            17               0.563077   \n",
       "49      0.631872     0.007750            27               0.567347   \n",
       "50      0.632391     0.011911            17               0.563077   \n",
       "51      0.631872     0.007750            27               0.567347   \n",
       "52      0.623925     0.017427            57               0.546875   \n",
       "53      0.633454     0.009859            11               0.566468   \n",
       "54      0.634061     0.012271             9               0.568648   \n",
       "55      0.632913     0.009869            12               0.568041   \n",
       "56      0.634061     0.012271             9               0.568648   \n",
       "57      0.632913     0.009869            12               0.568041   \n",
       "58      0.623704     0.016728            58               0.547368   \n",
       "59      0.632727     0.010278            14               0.564516   \n",
       "\n",
       "    split1_test_Precision  split2_test_Precision  split3_test_Precision  \\\n",
       "0                0.587747               0.568017               0.576621   \n",
       "1                0.586458               0.568769               0.577889   \n",
       "2                0.587747               0.568017               0.576621   \n",
       "3                0.586458               0.568769               0.577889   \n",
       "4                0.579864               0.555033               0.568982   \n",
       "5                0.574723               0.556962               0.572254   \n",
       "6                0.585567               0.569948               0.585930   \n",
       "7                0.588418               0.570239               0.580384   \n",
       "8                0.585567               0.569948               0.585930   \n",
       "9                0.588418               0.570239               0.580384   \n",
       "10               0.570457               0.555992               0.569201   \n",
       "11               0.575581               0.562874               0.574364   \n",
       "12               0.590336               0.568134               0.578471   \n",
       "13               0.588050               0.569199               0.577869   \n",
       "14               0.590336               0.568134               0.578471   \n",
       "15               0.588050               0.569199               0.577869   \n",
       "16               0.571146               0.560120               0.575668   \n",
       "17               0.578791               0.559880               0.579624   \n",
       "18               0.592516               0.571429               0.585440   \n",
       "19               0.590717               0.568875               0.582043   \n",
       "20               0.592516               0.571429               0.585440   \n",
       "21               0.590717               0.568875               0.582043   \n",
       "22               0.571575               0.567788               0.580000   \n",
       "23               0.572144               0.557576               0.580581   \n",
       "24               0.590957               0.568421               0.585513   \n",
       "25               0.592008               0.567511               0.582555   \n",
       "26               0.590957               0.568421               0.585513   \n",
       "27               0.592008               0.567511               0.582555   \n",
       "28               0.576289               0.559596               0.583000   \n",
       "29               0.575258               0.556112               0.580082   \n",
       "30               0.587992               0.565803               0.580739   \n",
       "31               0.584200               0.571134               0.577954   \n",
       "32               0.587992               0.565803               0.580739   \n",
       "33               0.584200               0.571134               0.577954   \n",
       "34               0.577973               0.557638               0.574248   \n",
       "35               0.580374               0.561576               0.576775   \n",
       "36               0.584442               0.567623               0.583584   \n",
       "37               0.586350               0.568486               0.578947   \n",
       "38               0.584442               0.567623               0.583584   \n",
       "39               0.586350               0.568486               0.578947   \n",
       "40               0.573359               0.555665               0.571150   \n",
       "41               0.577973               0.558559               0.575728   \n",
       "42               0.583247               0.568345               0.577423   \n",
       "43               0.584551               0.568323               0.578629   \n",
       "44               0.583247               0.568345               0.577423   \n",
       "45               0.584551               0.568323               0.578629   \n",
       "46               0.575697               0.557558               0.574384   \n",
       "47               0.579104               0.561049               0.579782   \n",
       "48               0.591753               0.568182               0.580420   \n",
       "49               0.588482               0.568182               0.581466   \n",
       "50               0.591753               0.568182               0.580420   \n",
       "51               0.588482               0.568182               0.581466   \n",
       "52               0.575510               0.565173               0.582755   \n",
       "53               0.575051               0.558824               0.580838   \n",
       "54               0.592092               0.568017               0.589899   \n",
       "55               0.591051               0.571280               0.584772   \n",
       "56               0.592092               0.568017               0.589899   \n",
       "57               0.591051               0.571280               0.584772   \n",
       "58               0.577665               0.560241               0.577268   \n",
       "59               0.579979               0.558676               0.579755   \n",
       "\n",
       "    split4_test_Precision  mean_test_Precision  std_test_Precision  \\\n",
       "0                0.580749             0.575343            0.008680   \n",
       "1                0.583871             0.576062            0.008808   \n",
       "2                0.580749             0.575343            0.008680   \n",
       "3                0.583871             0.576062            0.008808   \n",
       "4                0.559804             0.564466            0.008967   \n",
       "5                0.564545             0.565841            0.006734   \n",
       "6                0.589325             0.578870            0.010172   \n",
       "7                0.588614             0.578044            0.010251   \n",
       "8                0.589325             0.578870            0.010172   \n",
       "9                0.588614             0.578044            0.010251   \n",
       "10               0.561741             0.563063            0.005839   \n",
       "11               0.566322             0.567282            0.006924   \n",
       "12               0.592308             0.578607            0.011442   \n",
       "13               0.590713             0.578183            0.010068   \n",
       "14               0.592308             0.578607            0.011442   \n",
       "15               0.590713             0.578183            0.010068   \n",
       "16               0.575230             0.567750            0.007913   \n",
       "17               0.580803             0.571060            0.010713   \n",
       "18               0.577802             0.578901            0.009158   \n",
       "19               0.591684             0.579564            0.011123   \n",
       "20               0.577802             0.578901            0.009158   \n",
       "21               0.591684             0.579564            0.011123   \n",
       "22               0.572334             0.568960            0.008865   \n",
       "23               0.579534             0.570244            0.009335   \n",
       "24               0.589577             0.579587            0.011390   \n",
       "25               0.594652             0.580726            0.011749   \n",
       "26               0.589577             0.579587            0.011390   \n",
       "27               0.594652             0.580726            0.011749   \n",
       "28               0.570074             0.567131            0.012797   \n",
       "29               0.586489             0.572250            0.011077   \n",
       "30               0.581470             0.575701            0.009818   \n",
       "31               0.586466             0.576802            0.008233   \n",
       "32               0.581470             0.575701            0.009818   \n",
       "33               0.586466             0.576802            0.008233   \n",
       "34               0.560277             0.566051            0.008350   \n",
       "35               0.570119             0.569769            0.008053   \n",
       "36               0.588553             0.577416            0.010185   \n",
       "37               0.586466             0.577084            0.008867   \n",
       "38               0.588553             0.577416            0.010185   \n",
       "39               0.586466             0.577084            0.008867   \n",
       "40               0.557100             0.561606            0.008973   \n",
       "41               0.567136             0.566800            0.009179   \n",
       "42               0.591265             0.577437            0.009149   \n",
       "43               0.592350             0.577565            0.010380   \n",
       "44               0.591265             0.577437            0.009149   \n",
       "45               0.592350             0.577565            0.010380   \n",
       "46               0.576844             0.566607            0.011452   \n",
       "47               0.584570             0.573583            0.009489   \n",
       "48               0.586096             0.577905            0.010766   \n",
       "49               0.590763             0.579248            0.009868   \n",
       "50               0.586096             0.577905            0.010766   \n",
       "51               0.590763             0.579248            0.009868   \n",
       "52               0.571579             0.568378            0.012169   \n",
       "53               0.582485             0.572733            0.008932   \n",
       "54               0.587473             0.581226            0.010630   \n",
       "55               0.588997             0.580828            0.009396   \n",
       "56               0.587473             0.581226            0.010630   \n",
       "57               0.588997             0.580828            0.009396   \n",
       "58               0.575000             0.567509            0.011941   \n",
       "59               0.590398             0.574665            0.011492   \n",
       "\n",
       "    rank_test_Precision  split0_test_Recall  split1_test_Recall  \\\n",
       "0                    39            0.690773            0.705736   \n",
       "1                    35            0.693267            0.701995   \n",
       "2                    39            0.690773            0.705736   \n",
       "3                    35            0.693267            0.701995   \n",
       "4                    58            0.700748            0.746883   \n",
       "5                    57            0.736908            0.776808   \n",
       "6                    15            0.690773            0.708229   \n",
       "7                    21            0.689526            0.709476   \n",
       "8                    15            0.690773            0.708229   \n",
       "9                    21            0.689526            0.709476   \n",
       "10                   59            0.684539            0.731920   \n",
       "11                   52            0.721945            0.740648   \n",
       "12                   17            0.683292            0.700748   \n",
       "13                   19            0.682045            0.699501   \n",
       "14                   17            0.683292            0.700748   \n",
       "15                   19            0.682045            0.699501   \n",
       "16                   50            0.674564            0.720698   \n",
       "17                   45            0.715711            0.728180   \n",
       "18                   13            0.688279            0.710723   \n",
       "19                    9            0.682045            0.698254   \n",
       "20                   13            0.688279            0.710723   \n",
       "21                    9            0.682045            0.698254   \n",
       "22                   48            0.655860            0.697007   \n",
       "23                   46            0.706983            0.711970   \n",
       "24                    7            0.680798            0.700748   \n",
       "25                    5            0.692020            0.701995   \n",
       "26                    7            0.680798            0.700748   \n",
       "27                    5            0.692020            0.701995   \n",
       "28                   53            0.649626            0.697007   \n",
       "29                   44            0.704489            0.695761   \n",
       "30                   37            0.695761            0.708229   \n",
       "31                   33            0.700748            0.700748   \n",
       "32                   37            0.695761            0.708229   \n",
       "33                   33            0.700748            0.700748   \n",
       "34                   56            0.697007            0.739401   \n",
       "35                   47            0.733167            0.774314   \n",
       "36                   29            0.692020            0.711970   \n",
       "37                   31            0.692020            0.706983   \n",
       "38                   29            0.692020            0.711970   \n",
       "39                   31            0.692020            0.706983   \n",
       "40                   60            0.683292            0.740648   \n",
       "41                   54            0.728180            0.739401   \n",
       "42                   27            0.692020            0.703242   \n",
       "43                   25            0.687032            0.698254   \n",
       "44                   27            0.692020            0.703242   \n",
       "45                   25            0.687032            0.698254   \n",
       "46                   55            0.662095            0.720698   \n",
       "47                   42            0.725686            0.725686   \n",
       "48                   23            0.684539            0.715711   \n",
       "49                   11            0.693267            0.700748   \n",
       "50                   23            0.684539            0.715711   \n",
       "51                   11            0.693267            0.700748   \n",
       "52                   49            0.654613            0.703242   \n",
       "53                   43            0.711970            0.706983   \n",
       "54                    1            0.692020            0.709476   \n",
       "55                    3            0.687032            0.708229   \n",
       "56                    1            0.692020            0.709476   \n",
       "57                    3            0.687032            0.708229   \n",
       "58                   51            0.648379            0.709476   \n",
       "59                   41            0.698254            0.700748   \n",
       "\n",
       "    split2_test_Recall  split3_test_Recall  split4_test_Recall  \\\n",
       "0             0.682045            0.731009            0.676214   \n",
       "1             0.685786            0.716065            0.676214   \n",
       "2             0.682045            0.731009            0.676214   \n",
       "3             0.685786            0.716065            0.676214   \n",
       "4             0.735661            0.744707            0.711083   \n",
       "5             0.713217            0.739726            0.773350   \n",
       "6             0.685786            0.726027            0.673724   \n",
       "7             0.683292            0.714819            0.682441   \n",
       "8             0.685786            0.726027            0.673724   \n",
       "9             0.683292            0.714819            0.682441   \n",
       "10            0.705736            0.727273            0.691158   \n",
       "11            0.703242            0.731009            0.749689   \n",
       "12            0.675810            0.716065            0.671233   \n",
       "13            0.682045            0.702366            0.681196   \n",
       "14            0.675810            0.716065            0.671233   \n",
       "15            0.682045            0.702366            0.681196   \n",
       "16            0.697007            0.724782            0.699875   \n",
       "17            0.699501            0.729763            0.738481   \n",
       "18            0.678304            0.721046            0.661270   \n",
       "19            0.674564            0.702366            0.691158   \n",
       "20            0.678304            0.721046            0.661270   \n",
       "21            0.674564            0.702366            0.691158   \n",
       "22            0.694514            0.722291            0.674969   \n",
       "23            0.688279            0.722291            0.712329   \n",
       "24            0.673317            0.724782            0.676214   \n",
       "25            0.670823            0.698630            0.692403   \n",
       "26            0.673317            0.724782            0.676214   \n",
       "27            0.670823            0.698630            0.692403   \n",
       "28            0.690773            0.726027            0.673724   \n",
       "29            0.692020            0.703611            0.713574   \n",
       "30            0.680798            0.743462            0.679950   \n",
       "31            0.690773            0.724782            0.679950   \n",
       "32            0.680798            0.743462            0.679950   \n",
       "33            0.690773            0.724782            0.679950   \n",
       "34            0.741895            0.760897            0.706102   \n",
       "35            0.710723            0.748443            0.774595   \n",
       "36            0.690773            0.726027            0.678705   \n",
       "37            0.688279            0.726027            0.679950   \n",
       "38            0.690773            0.726027            0.678705   \n",
       "39            0.688279            0.726027            0.679950   \n",
       "40            0.703242            0.729763            0.698630   \n",
       "41            0.695761            0.738481            0.752179   \n",
       "42            0.689526            0.719801            0.657534   \n",
       "43            0.684539            0.714819            0.674969   \n",
       "44            0.689526            0.719801            0.657534   \n",
       "45            0.684539            0.714819            0.674969   \n",
       "46            0.694514            0.726027            0.701121   \n",
       "47            0.693267            0.728518            0.735990   \n",
       "48            0.685786            0.723537            0.682441   \n",
       "49            0.685786            0.711083            0.684932   \n",
       "50            0.685786            0.723537            0.682441   \n",
       "51            0.685786            0.711083            0.684932   \n",
       "52            0.692020            0.732254            0.676214   \n",
       "53            0.687032            0.724782            0.712329   \n",
       "54            0.682045            0.727273            0.677460   \n",
       "55            0.684539            0.717310            0.679950   \n",
       "56            0.682045            0.727273            0.677460   \n",
       "57            0.684539            0.717310            0.679950   \n",
       "58            0.695761            0.721046            0.687422   \n",
       "59            0.694514            0.706102            0.719801   \n",
       "\n",
       "    mean_test_Recall  std_test_Recall  rank_test_Recall  split0_test_Accuracy  \\\n",
       "0           0.697155         0.019633                29              0.576875   \n",
       "1           0.694665         0.013660                39              0.576875   \n",
       "2           0.697155         0.019633                29              0.576875   \n",
       "3           0.694665         0.013660                39              0.576875   \n",
       "4           0.727817         0.018564                 6              0.572500   \n",
       "5           0.748002         0.023974                 2              0.578750   \n",
       "6           0.696908         0.018303                31              0.576875   \n",
       "7           0.695911         0.013587                33              0.575625   \n",
       "8           0.696908         0.018303                31              0.576875   \n",
       "9           0.695911         0.013587                33              0.575625   \n",
       "10          0.708125         0.018883                12              0.570000   \n",
       "11          0.729307         0.016003                 4              0.573125   \n",
       "12          0.689430         0.016684                57              0.576250   \n",
       "13          0.689431         0.009441                55              0.577500   \n",
       "14          0.689430         0.016684                57              0.576250   \n",
       "15          0.689431         0.009441                55              0.577500   \n",
       "16          0.703385         0.018118                14              0.567500   \n",
       "17          0.722327         0.013530                 7              0.571250   \n",
       "18          0.691925         0.021633                44              0.580625   \n",
       "19          0.689677         0.010232                53              0.576875   \n",
       "20          0.691925         0.021633                44              0.580625   \n",
       "21          0.689677         0.010232                53              0.576875   \n",
       "22          0.688928         0.022351                59              0.561875   \n",
       "23          0.708370         0.011207                11              0.576250   \n",
       "24          0.691172         0.019347                51              0.575625   \n",
       "25          0.691174         0.010854                49              0.580625   \n",
       "26          0.691172         0.019347                51              0.575625   \n",
       "27          0.691174         0.010854                49              0.580625   \n",
       "28          0.687431         0.025336                60              0.554375   \n",
       "29          0.701891         0.007500                15              0.578125   \n",
       "30          0.701640         0.023375                16              0.576250   \n",
       "31          0.699400         0.014836                21              0.578750   \n",
       "32          0.701640         0.023375                16              0.576250   \n",
       "33          0.699400         0.014836                21              0.578750   \n",
       "34          0.729061         0.023831                 5              0.573750   \n",
       "35          0.748249         0.024532                 1              0.577500   \n",
       "36          0.699899         0.016870                19              0.576250   \n",
       "37          0.698652         0.016249                23              0.578750   \n",
       "38          0.699899         0.016870                19              0.576250   \n",
       "39          0.698652         0.016249                23              0.578750   \n",
       "40          0.711115         0.021033                 9              0.561875   \n",
       "41          0.730800         0.019103                 3              0.570625   \n",
       "42          0.692425         0.020457                41              0.580625   \n",
       "43          0.691923         0.013640                46              0.576875   \n",
       "44          0.692425         0.020457                41              0.580625   \n",
       "45          0.691923         0.013640                46              0.576875   \n",
       "46          0.700891         0.022672                18              0.557500   \n",
       "47          0.721829         0.014770                 8              0.580625   \n",
       "48          0.698402         0.017536                25              0.575625   \n",
       "49          0.695163         0.009807                37              0.581250   \n",
       "50          0.698402         0.017536                25              0.575625   \n",
       "51          0.695163         0.009807                37              0.581250   \n",
       "52          0.691669         0.026051                48              0.555000   \n",
       "53          0.708619         0.012287                10              0.582500   \n",
       "54          0.697655         0.018440                27              0.582500   \n",
       "55          0.695412         0.014638                35              0.581250   \n",
       "56          0.697655         0.018440                27              0.582500   \n",
       "57          0.695412         0.014638                35              0.581250   \n",
       "58          0.692417         0.024844                43              0.555000   \n",
       "59          0.703884         0.008804                13              0.578750   \n",
       "\n",
       "    split1_test_Accuracy  split2_test_Accuracy  split3_test_Accuracy  \\\n",
       "0               0.604375              0.580625              0.595625   \n",
       "1               0.602500              0.581875              0.595000   \n",
       "2               0.604375              0.580625              0.595625   \n",
       "3               0.602500              0.581875              0.595000   \n",
       "4               0.601875              0.571875              0.588750   \n",
       "5               0.600000              0.571875              0.591875   \n",
       "6               0.602500              0.583125              0.605000   \n",
       "7               0.605625              0.583125              0.597500   \n",
       "8               0.602500              0.583125              0.605000   \n",
       "9               0.605625              0.583125              0.597500   \n",
       "10              0.589375              0.570000              0.586875   \n",
       "11              0.596250              0.577500              0.593125   \n",
       "12              0.606250              0.580000              0.595625   \n",
       "13              0.603750              0.581875              0.593125   \n",
       "14              0.606250              0.580000              0.595625   \n",
       "15              0.603750              0.581875              0.593125   \n",
       "16              0.588750              0.573750              0.593750   \n",
       "17              0.598125              0.573750              0.598750   \n",
       "18              0.610000              0.583750              0.603750   \n",
       "19              0.606250              0.580625              0.597500   \n",
       "20              0.610000              0.583750              0.603750   \n",
       "21              0.606250              0.580625              0.597500   \n",
       "22              0.586250              0.581875              0.598125   \n",
       "23              0.588750              0.570000              0.598750   \n",
       "24              0.606875              0.580000              0.604375   \n",
       "25              0.608125              0.578750              0.597500   \n",
       "26              0.606875              0.580000              0.604375   \n",
       "27              0.608125              0.578750              0.597500   \n",
       "28              0.591250              0.572500              0.601875   \n",
       "29              0.590000              0.568750              0.595625   \n",
       "30              0.605000              0.578125              0.601875   \n",
       "31              0.600000              0.585000              0.596250   \n",
       "32              0.605000              0.578125              0.601875   \n",
       "33              0.600000              0.585000              0.596250   \n",
       "34              0.598750              0.575625              0.596875   \n",
       "35              0.606250              0.576875              0.598125   \n",
       "36              0.601875              0.581250              0.602500   \n",
       "37              0.603125              0.581875              0.597500   \n",
       "38              0.601875              0.581250              0.602500   \n",
       "39              0.603125              0.581875              0.597500   \n",
       "40              0.593750              0.569375              0.589375   \n",
       "41              0.598750              0.571875              0.595625   \n",
       "42              0.599375              0.581875              0.595000   \n",
       "43              0.600000              0.581250              0.595625   \n",
       "44              0.599375              0.581875              0.595000   \n",
       "45              0.600000              0.581250              0.595625   \n",
       "46              0.593750              0.570625              0.592500   \n",
       "47              0.598125              0.574375              0.598750   \n",
       "48              0.610000              0.581250              0.598750   \n",
       "49              0.604375              0.581250              0.598125   \n",
       "50              0.610000              0.581250              0.598750   \n",
       "51              0.604375              0.581250              0.598125   \n",
       "52              0.591250              0.578750              0.602500   \n",
       "53              0.591250              0.571250              0.599375   \n",
       "54              0.609375              0.580625              0.609375   \n",
       "55              0.608125              0.584375              0.602500   \n",
       "56              0.609375              0.580625              0.609375   \n",
       "57              0.608125              0.584375              0.602500   \n",
       "58              0.594375              0.573750              0.595000   \n",
       "59              0.595625              0.571875              0.595625   \n",
       "\n",
       "    split4_test_Accuracy  mean_test_Accuracy  std_test_Accuracy  \\\n",
       "0               0.592500            0.590000           0.010047   \n",
       "1               0.595625            0.590375           0.009492   \n",
       "2               0.592500            0.590000           0.010047   \n",
       "3               0.595625            0.590375           0.009492   \n",
       "4               0.574375            0.581875           0.011759   \n",
       "5               0.586875            0.585875           0.009831   \n",
       "6               0.600625            0.593625           0.011384   \n",
       "7               0.601250            0.592625           0.011370   \n",
       "8               0.600625            0.593625           0.011384   \n",
       "9               0.601250            0.592625           0.011370   \n",
       "10              0.574375            0.578125           0.008357   \n",
       "11              0.586250            0.585250           0.008844   \n",
       "12              0.603125            0.592250           0.012097   \n",
       "13              0.603125            0.591875           0.010731   \n",
       "14              0.603125            0.592250           0.012097   \n",
       "15              0.603125            0.591875           0.010731   \n",
       "16              0.590000            0.582750           0.010229   \n",
       "17              0.601250            0.588625           0.013231   \n",
       "18              0.587500            0.593125           0.011605   \n",
       "19              0.605625            0.593375           0.012391   \n",
       "20              0.587500            0.593125           0.011605   \n",
       "21              0.605625            0.593375           0.012391   \n",
       "22              0.583750            0.582375           0.011709   \n",
       "23              0.596250            0.586000           0.011192   \n",
       "24              0.601250            0.593625           0.013107   \n",
       "25              0.608750            0.594750           0.012946   \n",
       "26              0.601250            0.593625           0.013107   \n",
       "27              0.608750            0.594750           0.012946   \n",
       "28              0.581250            0.580250           0.016243   \n",
       "29              0.603750            0.587250           0.012454   \n",
       "30              0.593750            0.591000           0.011876   \n",
       "31              0.598750            0.591750           0.008389   \n",
       "32              0.593750            0.591000           0.011876   \n",
       "33              0.598750            0.591750           0.008389   \n",
       "34              0.574375            0.583875           0.011411   \n",
       "35              0.593750            0.590500           0.011588   \n",
       "36              0.600625            0.592500           0.011354   \n",
       "37              0.598750            0.592000           0.009774   \n",
       "38              0.600625            0.592500           0.011354   \n",
       "39              0.598750            0.592000           0.009774   \n",
       "40              0.570000            0.576875           0.012406   \n",
       "41              0.587500            0.584875           0.011722   \n",
       "42              0.600000            0.591375           0.008454   \n",
       "43              0.603750            0.591500           0.010567   \n",
       "44              0.600000            0.591375           0.008454   \n",
       "45              0.603750            0.591500           0.010567   \n",
       "46              0.591875            0.581250           0.014647   \n",
       "47              0.605000            0.591375           0.011749   \n",
       "48              0.598750            0.592875           0.012610   \n",
       "49              0.603750            0.593750           0.010436   \n",
       "50              0.598750            0.592875           0.012610   \n",
       "51              0.603750            0.593750           0.010436   \n",
       "52              0.583125            0.582125           0.015790   \n",
       "53              0.599375            0.588750           0.010746   \n",
       "54              0.599375            0.596250           0.012550   \n",
       "55              0.601250            0.595500           0.010661   \n",
       "56              0.599375            0.596250           0.012550   \n",
       "57              0.601250            0.595500           0.010661   \n",
       "58              0.588125            0.581250           0.015191   \n",
       "59              0.608750            0.590125           0.013190   \n",
       "\n",
       "    rank_test_Accuracy  \n",
       "0                   42  \n",
       "1                   39  \n",
       "2                   42  \n",
       "3                   39  \n",
       "4                   55  \n",
       "5                   48  \n",
       "6                    9  \n",
       "7                   19  \n",
       "8                    9  \n",
       "9                   19  \n",
       "10                  59  \n",
       "11                  49  \n",
       "12                  23  \n",
       "13                  27  \n",
       "14                  23  \n",
       "15                  27  \n",
       "16                  52  \n",
       "17                  45  \n",
       "18                  15  \n",
       "19                  13  \n",
       "20                  15  \n",
       "21                  13  \n",
       "22                  53  \n",
       "23                  47  \n",
       "24                   9  \n",
       "25                   5  \n",
       "26                   9  \n",
       "27                   5  \n",
       "28                  58  \n",
       "29                  46  \n",
       "30                  36  \n",
       "31                  29  \n",
       "32                  36  \n",
       "33                  29  \n",
       "34                  51  \n",
       "35                  38  \n",
       "36                  21  \n",
       "37                  25  \n",
       "38                  21  \n",
       "39                  25  \n",
       "40                  60  \n",
       "41                  50  \n",
       "42                  33  \n",
       "43                  31  \n",
       "44                  33  \n",
       "45                  31  \n",
       "46                  56  \n",
       "47                  33  \n",
       "48                  17  \n",
       "49                   7  \n",
       "50                  17  \n",
       "51                   7  \n",
       "52                  54  \n",
       "53                  44  \n",
       "54                   1  \n",
       "55                   3  \n",
       "56                   1  \n",
       "57                   3  \n",
       "58                  56  \n",
       "59                  41  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CVscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = CV.predict(X_test)\n",
    "probabilities = CV.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6950800915331807 0.6394736842105262 0.7378542510121457 0.5642414860681114 0.589\n",
      "[[449 563]\n",
      " [259 729]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import fbeta_score, f1_score, recall_score, precision_score, roc_auc_score, roc_curve, accuracy_score\n",
    "\n",
    "tpr, fpr, thresolds = roc_curve(y_test, probabilities[:, 1])\n",
    "plt.plot(tpr, fpr)\n",
    "print(fbeta_score(y_test, predictions, beta=2), f1_score(y_test, predictions), recall_score(y_test, predictions), precision_score(y_test, predictions), accuracy_score(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY_OF_MONTH</th>\n",
       "      <th>DAY_OF_WEEK</th>\n",
       "      <th>OP_UNIQUE_CARRIER</th>\n",
       "      <th>TAIL_NUM</th>\n",
       "      <th>ORIGIN_CITY_MARKET_ID</th>\n",
       "      <th>ORIGIN</th>\n",
       "      <th>DEST_CITY_MARKET_ID</th>\n",
       "      <th>DEST</th>\n",
       "      <th>CRS_DEP_TIME</th>\n",
       "      <th>DEP_TIME</th>\n",
       "      <th>CRS_ARR_TIME</th>\n",
       "      <th>CRS_ELAPSED_TIME</th>\n",
       "      <th>DISTANCE</th>\n",
       "      <th>DISTANCE_GROUP</th>\n",
       "      <th>TAXI_OUT_median</th>\n",
       "      <th>TAXI_IN_median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6423502</th>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>NK</td>\n",
       "      <td>N677NK</td>\n",
       "      <td>32575</td>\n",
       "      <td>LAX</td>\n",
       "      <td>30194</td>\n",
       "      <td>DFW</td>\n",
       "      <td>1900</td>\n",
       "      <td>1854.0</td>\n",
       "      <td>2353</td>\n",
       "      <td>173.0</td>\n",
       "      <td>1235.0</td>\n",
       "      <td>5</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3391423</th>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>DL</td>\n",
       "      <td>N865DN</td>\n",
       "      <td>31295</td>\n",
       "      <td>DTW</td>\n",
       "      <td>30852</td>\n",
       "      <td>BWI</td>\n",
       "      <td>1740</td>\n",
       "      <td>1746.0</td>\n",
       "      <td>1912</td>\n",
       "      <td>92.0</td>\n",
       "      <td>409.0</td>\n",
       "      <td>2</td>\n",
       "      <td>16.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4702892</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>AA</td>\n",
       "      <td>N556UW</td>\n",
       "      <td>32467</td>\n",
       "      <td>MIA</td>\n",
       "      <td>30852</td>\n",
       "      <td>DCA</td>\n",
       "      <td>1825</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>2108</td>\n",
       "      <td>163.0</td>\n",
       "      <td>919.0</td>\n",
       "      <td>4</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603396</th>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>7</td>\n",
       "      <td>MQ</td>\n",
       "      <td>N533AE</td>\n",
       "      <td>31267</td>\n",
       "      <td>DAY</td>\n",
       "      <td>30977</td>\n",
       "      <td>ORD</td>\n",
       "      <td>1748</td>\n",
       "      <td>1820.0</td>\n",
       "      <td>1805</td>\n",
       "      <td>77.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>1</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33510</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>AA</td>\n",
       "      <td>N931AN</td>\n",
       "      <td>31703</td>\n",
       "      <td>JFK</td>\n",
       "      <td>30977</td>\n",
       "      <td>ORD</td>\n",
       "      <td>1300</td>\n",
       "      <td>1256.0</td>\n",
       "      <td>1448</td>\n",
       "      <td>168.0</td>\n",
       "      <td>740.0</td>\n",
       "      <td>3</td>\n",
       "      <td>21.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         MONTH  DAY_OF_MONTH  DAY_OF_WEEK OP_UNIQUE_CARRIER TAIL_NUM  \\\n",
       "6423502     11            16            6                NK   N677NK   \n",
       "3391423      6            17            1                DL   N865DN   \n",
       "4702892      8             8            4                AA   N556UW   \n",
       "603396       2            24            7                MQ   N533AE   \n",
       "33510        1             8            2                AA   N931AN   \n",
       "\n",
       "         ORIGIN_CITY_MARKET_ID ORIGIN  DEST_CITY_MARKET_ID DEST  CRS_DEP_TIME  \\\n",
       "6423502                  32575    LAX                30194  DFW          1900   \n",
       "3391423                  31295    DTW                30852  BWI          1740   \n",
       "4702892                  32467    MIA                30852  DCA          1825   \n",
       "603396                   31267    DAY                30977  ORD          1748   \n",
       "33510                    31703    JFK                30977  ORD          1300   \n",
       "\n",
       "         DEP_TIME  CRS_ARR_TIME  CRS_ELAPSED_TIME  DISTANCE  DISTANCE_GROUP  \\\n",
       "6423502    1854.0          2353             173.0    1235.0               5   \n",
       "3391423    1746.0          1912              92.0     409.0               2   \n",
       "4702892    2015.0          2108             163.0     919.0               4   \n",
       "603396     1820.0          1805              77.0     240.0               1   \n",
       "33510      1256.0          1448             168.0     740.0               3   \n",
       "\n",
       "         TAXI_OUT_median  TAXI_IN_median  \n",
       "6423502             14.0            10.0  \n",
       "3391423             16.0             6.0  \n",
       "4702892             16.0             5.0  \n",
       "603396              13.0            13.0  \n",
       "33510               21.0            14.0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_validation = df_validation.drop('ARR_DEL15', axis=1)\n",
    "X_validation.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3054304    0.0\n",
       "6240995    0.0\n",
       "6124397    0.0\n",
       "5169529    1.0\n",
       "6111204    0.0\n",
       "Name: ARR_DEL15, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_validation = df_validation['ARR_DEL15']\n",
    "y_validation.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = CV.predict(X_validation)\n",
    "probabilities = CV.predict_proba(X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5248391517530844 0.36242761412554575 0.7484311032737513 0.23910774663327833 0.49780235737849104\n",
      "[[2577147 3296832]\n",
      " [ 348235 1036018]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import fbeta_score, f1_score, recall_score, precision_score, roc_auc_score, roc_curve, accuracy_score\n",
    "\n",
    "tpr, fpr, thresolds = roc_curve(y_validation, probabilities[:, 1])\n",
    "plt.plot(tpr, fpr)\n",
    "print(fbeta_score(y_validation, predictions, beta=2), f1_score(y_validation, predictions), recall_score(y_validation, predictions), precision_score(y_validation, predictions), accuracy_score(y_validation, predictions))\n",
    "print(confusion_matrix(y_validation, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hacer aquí la confusion plot matrix !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x25abcb22730>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(rf, X_test, y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed:  2977.3682293\n"
     ]
    }
   ],
   "source": [
    "t1 = time.perf_counter() - t0\n",
    "print(\"Time elapsed: \", t1) # CPU seconds elapsed (floating point)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Time elapsed:  548.8254737"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Classification Metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics)\n",
    "\n",
    "- Binary classification (only):\n",
    "    - [precision_recall_curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html#sklearn.metrics.precision_recall_curve): Compute precision-recall pairs for different probability thresholds.\n",
    "        - The precision is the ratio `tp / (tp + fp)` where `tp` is the number of true positives and `fp` the number of false positives. The **precision** is intuitively the ability of the classifier **not to label as positive a sample that is negative**.\n",
    "        - The recall is the ratio `tp / (tp + fn)` where `tp` is the number of true positives and `fn` the number of false negatives. The **recall** is intuitively the ability of the classifier to **find all the positive samples**.\n",
    "\n",
    "    ### RECALL could be a potentially strong metric for this case; \"from all the flights classified as delayed, the actual (true) number delyed flights is as high as possible.\"\n",
    "\n",
    "    - [roc_curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve): A receiver operating characteristic (ROC), or simply ROC curve, is a graphical plot which illustrates the performance of a binary classifier system as its discrimination threshold is varied. It is created by plotting the fraction of true positives out of the positives (TPR = true positive rate) vs. the fraction of false positives out of the negatives (FPR = false positive rate), at various threshold settings. TPR is also known as sensitivity, and FPR is one minus the specificity or true negative rate.\n",
    "        \n",
    "- Multi-class classification (or binary):\n",
    "    - [confusion_matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix): Compute confusion matrix to evaluate the accuracy of a classification.\n",
    "    ![IMG_Confusion_Matrix](https://scikit-learn.org/stable/_images/sphx_glr_plot_confusion_matrix_0011.png)\n",
    "    - [roc_auc_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score): Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores.\n",
    "\n",
    "- Multi-label classification (or binary or multi-class):\n",
    "    - [accuracy_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score): Accuracy classification score.\n",
    "        - It is the ratio of number of correct predictions to the total number of input samples.\n",
    "        - **It works well only if there are equal number of samples belonging to each class.**\n",
    "    - [classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report): Build a text report showing the main classification metrics.\n",
    "    - [f1_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score): Compute the F1 score, also known as balanced F-score or F-measure.\n",
    "        - The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal: `F1 = 2 * (precision * recall) / (precision + recall)`\n",
    "    - [precision_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score)\n",
    "    - [recall_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which metric should be used then?\n",
    "##### Bear in mind that it's a clear case of imbalanced data\n",
    "\n",
    "Nomenclature:\n",
    "- Delayed = Positive\n",
    "- On-time = Negative \n",
    "\n",
    "Considering this:\n",
    "- False positive (Type I error) → Wrongly classifying an On-time flight as a Delayed flight → Not significantly relevant\n",
    "- **False negative (Type II error) → Wrongly classifying a Delayed flight as an On-time flight → Highly relevant**\n",
    "\n",
    "**F-beta** score (\\$ F_\\beta \\$):\n",
    "![F-beta score](https://wikimedia.org/api/rest_v1/media/math/render/svg/136f45612c08805f4254f63d2f2524bc25075fff)\n",
    "\n",
    "Two commonly used values for β are:\n",
    "- **2 : weighs recall higher than precision**\n",
    "- 0.5 : weighs recall lower than precision.\n",
    "\n",
    "\n",
    "<em>Probably most people in the industry would accept that an **OTP of 80%* or above is pretty good***. That’s 4 in 5 flights arriving within 15 minutes of their scheduled arrival time. The very best airlines and airports succeed in punctuality closer to 90% - but they remain the exception rather than the rule.</em>  \n",
    "(Source: [OAG](https://www.oag.com/on-time-performance-airlines-airports))\n",
    "\n",
    "The actual data from the 7268232 records comprising the OTP dataset accurately confirm this hypothesis:\n",
    "```\n",
    "Delays: 5878979 (80.89%)\n",
    "On-time: 1389253 (19.11%)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some rare cases, the calculation of Precision or Recall can cause a division by 0. Regarding the precision, this can happen if there are no results inside the answer of an annotator and, thus, the true as well as the false positives are 0. For these special cases, we have defined that **if the true positives, false positives and false negatives are all 0, the precision, recall and F1-measure are 1**. This might occur in cases in which the gold standard contains a document without any annotations and the annotator (correctly) returns no annotations. **If true positives are 0 and one of the two other counters is larger than 0, the precision, recall and F1-measure are 0.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[A Gentle Introduction to Imbalanced Classification](https://machinelearningmastery.com/what-is-imbalanced-classification/)  \n",
    "**Imbalanced classifications** pose a challenge for predictive modeling as **most of the machine learning algorithms used for classification were designed around the assumption of an equal number of examples for each class**. This results in models that have poor predictive performance, specifically for the minority class. This is a problem because typically, the minority class is more important and therefore **the problem is more sensitive to classification errors for the minority class than the majority class**.\n",
    "\n",
    "[Tactics to Combat Imbalanced Classes in Your Machine Learning Dataset](https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/)  \n",
    "1. Try Changing Your Performance Metric:\n",
    "    - Confusion Matrix\n",
    "    - Precision: \n",
    "    - Recall: \n",
    "    - F1 Score\n",
    "    - Kappa\n",
    "    - ROC Curves\n",
    "2. Try Resampling Your Dataset:\n",
    "    - You can add copies of instances from the under-represented class called **over-sampling** (or more formally **sampling with replacement**) → when you don’t have a lot of data (tens of thousands of records or less)\n",
    "    - You can delete instances from the over-represented class, called **under-sampling** → when you have an a lot data (tens- or hundreds of thousands of instances or more)\n",
    "3. Try Different Algorithms:\n",
    "    - That being said, **decision trees often perform well on imbalanced datasets**. The splitting rules that look at the class variable used in the creation of the trees, can force both classes to be addressed.  \n",
    "    If in doubt, try a few popular decision tree algorithms like C4.5, C5.0, CART, and Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Guide to Classification on Imbalanced Datasets](https://towardsdatascience.com/guide-to-classification-on-imbalanced-datasets-d6653aa5fa23)  \n",
    "There are two main types of techniques to handle imbalanced datasets:\n",
    "- ### Sampling methods:\n",
    "    - #### Oversampling\n",
    "        - How do we generate these samples? The most common way is to generate points that are close in dataspace proximity to existing samples or are ‘between’ two samples, as illustrated below\n",
    "        - There are some downsides to adding false data points:\n",
    "            - **Overfitting** risk\n",
    "            - In addition, adding these values randomly can also contribute **additional noise to our model**\n",
    "        - Techniques:\n",
    "            - **SMOTE** (Synthetic minority oversampling technique) → SMOTE generates new samples in between existing data points based on their local density and their borders with the other class. Algorithm:\n",
    "                - Find its k-nearest minority neighbours\n",
    "                – Randomly select j of these neighbours\n",
    "                – Randomly generate synthetic samples along the lines joining the minority sample and its j selected neighbours (j depends on the amount of oversampling desired)\n",
    "    - #### Undersampling\n",
    "        - Is undersampling a good idea? Undersampling is recommended by many statistical researchers but is **only good if enough data points are available on the undersampled class**\n",
    "        - Is undersampling a good idea? Undersampling is recommended by many statistical researchers but is only good if enough data points are available on the undersampled class\n",
    "- ### Cost-sensitive methods\n",
    "    - #### Upweighting\n",
    "    Upweighting is analogous to over-sampling and works by increasing the weight of one of the classes keeping the weight of the other class at one\n",
    "    - #### Down-weighting\n",
    "    Down-weighting is analogous to under-sampling and works by decreasing the weight of one of the classes keeping the weight of the other class at one\n",
    "\n",
    "```python\n",
    "# Example of how to implement cost-sensitive learning:\n",
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "model.fit(X_train, y_train, class_weight=class_weights)\n",
    "```\n",
    "\n",
    "- Benefits of cost-sensitive learning:\n",
    "    - It is much simpler to implement\n",
    "    - Easier to communicate to individuals\n",
    "\n",
    "### Assessment Metrics\n",
    "- #### F1-score\n",
    "    - What does a high F1 score mean? It suggests that both the precision and recall have high values — this is good and is what you would hope to see upon generating a well-functioning classification model on an imbalanced dataset. A low value indicates that either precision or recall is low, and maybe a call for concern\n",
    "    - Good F1 scores are generally lower than good accuracies (in many situations, an **F1 score of 0.5 would be considered pretty good**)\n",
    "\n",
    "- #### Receiver Operating Characteristic (ROC) Curve\n",
    "   - Depending on your application, you may be very averse to false positives as they may be very costly (e.g. launches of nuclear missiles) and thus **would like a classifier that has a very low false-positive rate**.\n",
    "\n",
    "- #### Area Under Curve (AUC)\n",
    "   - If a particular classifier has an ROC of 0.6 and another has an ROC of 0.8, the latter is clearly a better classifier. The AUC has the benefit that it is independent of the decision criteria — the classification threshold — and thus makes it easier to compare these classifiers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
