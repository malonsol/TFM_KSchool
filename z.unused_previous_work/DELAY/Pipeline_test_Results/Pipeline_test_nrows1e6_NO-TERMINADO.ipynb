{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries to be used\n",
    "\n",
    "# Warning messages display\n",
    "## import warnings\n",
    "## warnings.filterwarnings(action='ignore') # https://docs.python.org/3/library/warnings.html#the-warnings-filter\n",
    "\n",
    "# Directories/Files management\n",
    "import os.path\n",
    "## from zipfile import ZipFile # De momento no ha hecho falta \n",
    "\n",
    "# Timing\n",
    "import time\n",
    "\n",
    "# Data analysis and wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', None) # Show all columns in DataFrames\n",
    "pd.set_option('display.max_rows', 100) # If too high, it greatly slows down the output display and freezes the kernel\n",
    "\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot') # choose a style: 'plt.style.available'\n",
    "sns.set_theme(context='notebook',\n",
    "              style=\"darkgrid\") # {darkgrid, whitegrid, dark, white, ticks}\n",
    "palette = sns.color_palette(\"flare\", as_cmap=True);\n",
    "import altair as alt\n",
    "\n",
    "# Machine Learning\n",
    "## from sklearn.[...] import ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.perf_counter() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on Windows.\n",
      "root path\t C:\\Users\\turge\\CompartidoVM\\0.TFM\n"
     ]
    }
   ],
   "source": [
    "# Detect Operating System running and manage paths accordingly\n",
    "\n",
    "if os.name == 'nt': # Windows\n",
    "    root = r\"C:\\Users\\turge\\CompartidoVM\\0.TFM\"\n",
    "    print(\"Running on Windows.\")\n",
    "elif os.name == 'posix': # Ubuntu\n",
    "    root = \"/home/dsc/shared/0.TFM\"\n",
    "    print(\"Running on Ubuntu.\")\n",
    "print(\"root path\\t\", root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    \n",
    "### -----  < X > (PRE-FLIGHT DATA) -----\n",
    "\n",
    "# Time Period\n",
    " 'YEAR',\n",
    "#  'QUARTER', # Disregarded: redundant\n",
    " 'MONTH',\n",
    " 'DAY_OF_MONTH',\n",
    " 'DAY_OF_WEEK',\n",
    "#  'FL_DATE', # Disregarded: redundant\n",
    "# Airline / Aircraft\n",
    " 'OP_UNIQUE_CARRIER',\n",
    "#  'OP_CARRIER_AIRLINE_ID', # Disregarded: redundant\n",
    "#  'OP_CARRIER', # Disregarded: redundant\n",
    " 'TAIL_NUM',\n",
    "#  'OP_CARRIER_FL_NUM', # Unknown in advance?\n",
    "# Origin\n",
    "#  'ORIGIN_AIRPORT_ID', # Disregarded: redundant\n",
    "#  'ORIGIN_AIRPORT_SEQ_ID', # Disregarded: redundant\n",
    " 'ORIGIN_CITY_MARKET_ID',\n",
    " 'ORIGIN',\n",
    "#  'ORIGIN_CITY_NAME', # Disregarded: redundant\n",
    "#  'ORIGIN_STATE_ABR', # Disregarded: redundant\n",
    "#  'ORIGIN_STATE_FIPS', # Federal Information Processing Standards # Not used for the moment\n",
    "#  'ORIGIN_STATE_NM', # Disregarded: redundant\n",
    "#  'ORIGIN_WAC', # World Area Code # Not used for the moment\n",
    "# Destination\n",
    "#  'DEST_AIRPORT_ID', # Disregarded: redundant\n",
    "#  'DEST_AIRPORT_SEQ_ID', # Disregarded: redundant\n",
    " 'DEST_CITY_MARKET_ID',\n",
    " 'DEST',\n",
    "#  'DEST_CITY_NAME', # Disregarded: redundant\n",
    "#  'DEST_STATE_ABR', # Disregarded: redundant\n",
    "#  'DEST_STATE_FIPS', # Federal Information Processing Standards # Not used for the moment\n",
    "#  'DEST_STATE_NM', # Disregarded: redundant\n",
    "#  'DEST_WAC', # World Area Code # Not used for the moment\n",
    "# Departure Performance\n",
    " 'CRS_DEP_TIME',\n",
    "#  'TAXI_OUT_median', #  Output / However, the median for each airport could be used as input !! (explanation below)   \n",
    "# Arrival Performance\n",
    " 'CRS_ARR_TIME',\n",
    "#  'TAXI_IN_median', #  Output / However, the median for each airport could be used as input !! (explanation below) \n",
    "# Flight Summaries\n",
    " 'CRS_ELAPSED_TIME',\n",
    " 'FLIGHTS',\n",
    " 'DISTANCE',\n",
    " 'DISTANCE_GROUP',\n",
    "\n",
    "### ----- < y > (PRE-FLIGHT DATA) -----\n",
    "\n",
    "# Departure Performance\n",
    "#  'DEP_TIME', # Disregarded: redundant\n",
    "#  'DEP_DELAY', # Disregarded: other potentially useful target\n",
    "#  'DEP_DELAY_NEW', # Disregarded: redundant\n",
    "#  'DEP_DEL15', # Disregarded: other potentially useful target\n",
    "#  'DEP_DELAY_GROUP', # Disregarded: not relevant for this particular analysis\n",
    "#  'DEP_TIME_BLK', # Disregarded: redundant\n",
    "#  'TAXI_OUT', #  Output / However, the median for each airport could be used as input !! (explanation below)\n",
    "#  'WHEELS_OFF', # Disregarded: redundant\n",
    "# Arrival Performance\n",
    "#  'WHEELS_ON', # Disregarded: redundant\n",
    "#  'TAXI_IN', #  Output / However, the median for each airport could be used as input !! (explanation below)\n",
    "#  'ARR_TIME', # Disregarded: redundant\n",
    "#  'ARR_DELAY', # -------------------------------------------> MAIN TARGET !! (i.e. < y >)\n",
    "#  'ARR_DELAY_NEW', # Disregarded: redundant\n",
    " 'ARR_DEL15', # Disregarded: other potentially useful target\n",
    "#  'ARR_DELAY_GROUP', # Disregarded: not relevant for this particular analysis\n",
    "#  'ARR_TIME_BLK', # Disregarded: redundant\n",
    "# Cancellations and Diversions\n",
    "#  'CANCELLED', # Disregarded: not relevant for this particular analysis\n",
    "#  'CANCELLATION_CODE', # Disregarded: not relevant for this particular analysis\n",
    "#  'DIVERTED', # Disregarded: not relevant for this particular analysis\n",
    "# Flight Summaries\n",
    "#  'ACTUAL_ELAPSED_TIME', # Disregarded: redundant\n",
    "#  'AIR_TIME', # Disregarded: redundant\n",
    "# Cause of Delay\n",
    "#  'CARRIER_DELAY', # Disregarded: other potentially useful target\n",
    "#  'WEATHER_DELAY', # Disregarded: other potentially useful target\n",
    "#  'NAS_DELAY', # Disregarded: other potentially useful target\n",
    "#  'SECURITY_DELAY', # Disregarded: other potentially useful target\n",
    "#  'LATE_AIRCRAFT_DELAY', # Disregarded: other potentially useful target\n",
    "# Gate Return Information at Origin Airport (Data starts 10/2008)\n",
    "#  'FIRST_DEP_TIME', # Disregarded: not relevant for this particular analysis\n",
    "#  'TOTAL_ADD_GTIME', # Disregarded: not relevant for this particular analysis\n",
    "#  'LONGEST_ADD_GTIME', # Disregarded: not relevant for this particular analysis\n",
    "# Diverted Airport Information (Data starts 10/2008)\n",
    "#  'DIV_AIRPORT_LANDINGS', # Disregarded: not relevant for this particular analysis\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['YEAR',\n",
       " 'MONTH',\n",
       " 'DAY_OF_MONTH',\n",
       " 'DAY_OF_WEEK',\n",
       " 'OP_UNIQUE_CARRIER',\n",
       " 'TAIL_NUM',\n",
       " 'ORIGIN_CITY_MARKET_ID',\n",
       " 'ORIGIN',\n",
       " 'DEST_CITY_MARKET_ID',\n",
       " 'DEST',\n",
       " 'CRS_DEP_TIME',\n",
       " 'CRS_ARR_TIME',\n",
       " 'CRS_ELAPSED_TIME',\n",
       " 'FLIGHTS',\n",
       " 'DISTANCE',\n",
       " 'DISTANCE_GROUP',\n",
       " 'ARR_DEL15']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\turge\\\\CompartidoVM\\\\0.TFM\\\\Output_Data\\\\US_DoT\\\\AL_OTP_MVP_Preprocessed_19_v2_clean.csv'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_input_csv_path = os.path.join(root,\n",
    "                                           \"Output_Data\",\n",
    "                                           \"US_DoT\",\n",
    "                                           \"AL_OTP_MVP_Preprocessed_19_v2_clean.csv\")\n",
    "preprocessed_input_csv_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df = pd.read_csv(preprocessed_input_csv_path,\n",
    "                 encoding='latin1',\n",
    "                 nrows=1e6,\n",
    "                 usecols=cols, # This way, the extra column is disregarded for the loading process\n",
    "                 low_memory = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Dealing with categorical features with high cardinality: Target Encoding](https://medium.com/@kr.vishwesh54/dealing-with-categorical-features-with-high-cardinality-target-encoding-baa9298bf257)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cols = [\n",
    "    \n",
    "### -----  < X > (PRE-FLIGHT DATA) -----\n",
    "\n",
    "# Time Period\n",
    " 'YEAR',\n",
    "#  'QUARTER', # Disregarded: redundant\n",
    " 'MONTH',\n",
    " 'DAY_OF_MONTH',\n",
    " 'DAY_OF_WEEK',\n",
    "#  'FL_DATE', # Disregarded: redundant\n",
    "# Airline / Aircraft\n",
    " 'OP_UNIQUE_CARRIER',\n",
    "#  'OP_CARRIER_AIRLINE_ID', # Disregarded: redundant\n",
    "#  'OP_CARRIER', # Disregarded: redundant\n",
    " 'TAIL_NUM',\n",
    "#  'OP_CARRIER_FL_NUM', # Unknown in advance?\n",
    "# Origin\n",
    "#  'ORIGIN_AIRPORT_ID', # Disregarded: redundant\n",
    "#  'ORIGIN_AIRPORT_SEQ_ID', # Disregarded: redundant\n",
    " 'ORIGIN_CITY_MARKET_ID',\n",
    " 'ORIGIN',\n",
    "#  'ORIGIN_CITY_NAME', # Disregarded: redundant\n",
    "#  'ORIGIN_STATE_ABR', # Disregarded: redundant\n",
    "#  'ORIGIN_STATE_FIPS', # Federal Information Processing Standards # Not used for the moment\n",
    "#  'ORIGIN_STATE_NM', # Disregarded: redundant\n",
    "#  'ORIGIN_WAC', # World Area Code # Not used for the moment\n",
    "# Destination\n",
    "#  'DEST_AIRPORT_ID', # Disregarded: redundant\n",
    "#  'DEST_AIRPORT_SEQ_ID', # Disregarded: redundant\n",
    " 'DEST_CITY_MARKET_ID',\n",
    " 'DEST',\n",
    "#  'DEST_CITY_NAME', # Disregarded: redundant\n",
    "#  'DEST_STATE_ABR', # Disregarded: redundant\n",
    "#  'DEST_STATE_FIPS', # Federal Information Processing Standards # Not used for the moment\n",
    "#  'DEST_STATE_NM', # Disregarded: redundant\n",
    "#  'DEST_WAC', # World Area Code # Not used for the moment\n",
    "# Departure Performance\n",
    " 'CRS_DEP_TIME',\n",
    "#  'TAXI_OUT_median', #  Output / However, the median for each airport could be used as input !! (explanation below)   \n",
    "# Arrival Performance\n",
    " 'CRS_ARR_TIME',\n",
    "#  'TAXI_IN_median', #  Output / However, the median for each airport could be used as input !! (explanation below) \n",
    "# Flight Summaries\n",
    " 'CRS_ELAPSED_TIME',\n",
    " 'FLIGHTS',\n",
    " 'DISTANCE',\n",
    " 'DISTANCE_GROUP',\n",
    "\n",
    "### ----- < y > (PRE-FLIGHT DATA) -----\n",
    "\n",
    "# Departure Performance\n",
    "#  'DEP_TIME', # Disregarded: redundant\n",
    "#  'DEP_DELAY', # Disregarded: other potentially useful target\n",
    "#  'DEP_DELAY_NEW', # Disregarded: redundant\n",
    "#  'DEP_DEL15', # Disregarded: other potentially useful target\n",
    "#  'DEP_DELAY_GROUP', # Disregarded: not relevant for this particular analysis\n",
    "#  'DEP_TIME_BLK', # Disregarded: redundant\n",
    "#  'TAXI_OUT', #  Output / However, the median for each airport could be used as input !! (explanation below)\n",
    "#  'WHEELS_OFF', # Disregarded: redundant\n",
    "# Arrival Performance\n",
    "#  'WHEELS_ON', # Disregarded: redundant\n",
    "#  'TAXI_IN', #  Output / However, the median for each airport could be used as input !! (explanation below)\n",
    "#  'ARR_TIME', # Disregarded: redundant\n",
    " 'ARR_DELAY', # -------------------------------------------> MAIN TARGET !! (i.e. < y >)\n",
    "#  'ARR_DELAY_NEW', # Disregarded: redundant\n",
    "#  'ARR_DEL15', # Disregarded: other potentially useful target\n",
    "#  'ARR_DELAY_GROUP', # Disregarded: not relevant for this particular analysis\n",
    "#  'ARR_TIME_BLK', # Disregarded: redundant\n",
    "# Cancellations and Diversions\n",
    "#  'CANCELLED', # Disregarded: not relevant for this particular analysis\n",
    "#  'CANCELLATION_CODE', # Disregarded: not relevant for this particular analysis\n",
    "#  'DIVERTED', # Disregarded: not relevant for this particular analysis\n",
    "# Flight Summaries\n",
    "#  'ACTUAL_ELAPSED_TIME', # Disregarded: redundant\n",
    "#  'AIR_TIME', # Disregarded: redundant\n",
    "# Cause of Delay\n",
    "#  'CARRIER_DELAY', # Disregarded: other potentially useful target\n",
    "#  'WEATHER_DELAY', # Disregarded: other potentially useful target\n",
    "#  'NAS_DELAY', # Disregarded: other potentially useful target\n",
    "#  'SECURITY_DELAY', # Disregarded: other potentially useful target\n",
    "#  'LATE_AIRCRAFT_DELAY', # Disregarded: other potentially useful target\n",
    "# Gate Return Information at Origin Airport (Data starts 10/2008)\n",
    "#  'FIRST_DEP_TIME', # Disregarded: not relevant for this particular analysis\n",
    "#  'TOTAL_ADD_GTIME', # Disregarded: not relevant for this particular analysis\n",
    "#  'LONGEST_ADD_GTIME', # Disregarded: not relevant for this particular analysis\n",
    "# Diverted Airport Information (Data starts 10/2008)\n",
    "#  'DIV_AIRPORT_LANDINGS', # Disregarded: not relevant for this particular analysis\n",
    "]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 'TAXI_OUT' time is not an input, since there is no way to predict its actual value in advance\n",
    "# However, there is indeed some prior knowledge about it, based on past data concerning to operations on each airport.\n",
    "# Therefore, let's see whether some value could be used as a baseline for each airport and hence use it as an input feature.\n",
    "\n",
    "DepDel_TaxOutTim = df.groupby(['ORIGIN'])['TAXI_OUT', 'DEP_DELAY'].agg({'TAXI_OUT' : ['count', 'mean', 'median', 'min', 'max'],\n",
    "                                                                        'DEP_DELAY' : ['mean', 'median', 'min', 'max']})\n",
    "DepDel_TaxOutTim\n",
    "\n",
    "# Based on the results, it is observed that the median is commonly close to the mean.\n",
    "# In cases where this assumption is not satisfied, it is normally due to outliers.\n",
    "# These extreme values significantly move the means to the right.\n",
    "# Nevertheless, these outliers are almost always accompanied by high 'DEP_DELAY' values.\n",
    "\n",
    "# In a nutshell, it is fair to assume the each airport median as the baseline value for an input feature."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Similarly to what happens with 'TAXI_OUT', 'TAXI_IN' time is not an input, (unpredictable actual value in advance)\n",
    "# However, there is indeed some prior knowledge about it, based on past data concerning to operations on each airport.\n",
    "# Therefore, let's see whether some value could be used as a baseline for each airport and hence use it as an input feature.\n",
    "\n",
    "TaxInTim_ArrDel = df.groupby(['DEST'])['TAXI_IN', 'ARR_DELAY'].agg({'TAXI_IN' : ['count', 'mean', 'median', 'min', 'max'],\n",
    "                                                                    'ARR_DELAY' : ['mean', 'median', 'min', 'max']})\n",
    "TaxInTim_ArrDel\n",
    "\n",
    "# Based on the results, the same results as for 'TAXI_OUT': it is observed that the median is commonly close to the mean.\n",
    "# In cases where this assumption is not satisfied, it is normally due to outliers.\n",
    "# These extreme values significantly move the means to the right.\n",
    "# Nevertheless, these outliers are almost always accompanied by high 'ARR_DELAY' values.\n",
    "\n",
    "# In a nutshell, it is fair to assume the each airport median as the baseline value for an input feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY_OF_MONTH</th>\n",
       "      <th>DAY_OF_WEEK</th>\n",
       "      <th>OP_UNIQUE_CARRIER</th>\n",
       "      <th>TAIL_NUM</th>\n",
       "      <th>ORIGIN_CITY_MARKET_ID</th>\n",
       "      <th>ORIGIN</th>\n",
       "      <th>DEST_CITY_MARKET_ID</th>\n",
       "      <th>DEST</th>\n",
       "      <th>CRS_DEP_TIME</th>\n",
       "      <th>CRS_ARR_TIME</th>\n",
       "      <th>CRS_ELAPSED_TIME</th>\n",
       "      <th>FLIGHTS</th>\n",
       "      <th>DISTANCE</th>\n",
       "      <th>DISTANCE_GROUP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>64316</th>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>WN</td>\n",
       "      <td>N8682B</td>\n",
       "      <td>30423</td>\n",
       "      <td>AUS</td>\n",
       "      <td>32457</td>\n",
       "      <td>SJC</td>\n",
       "      <td>1655</td>\n",
       "      <td>1850</td>\n",
       "      <td>235.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1476.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293415</th>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>WN</td>\n",
       "      <td>N8509U</td>\n",
       "      <td>30325</td>\n",
       "      <td>DEN</td>\n",
       "      <td>31453</td>\n",
       "      <td>HOU</td>\n",
       "      <td>1235</td>\n",
       "      <td>1600</td>\n",
       "      <td>145.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>883.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653484</th>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>OO</td>\n",
       "      <td>N973SW</td>\n",
       "      <td>34006</td>\n",
       "      <td>PAH</td>\n",
       "      <td>30977</td>\n",
       "      <td>ORD</td>\n",
       "      <td>1719</td>\n",
       "      <td>1905</td>\n",
       "      <td>106.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>342.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461905</th>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>WN</td>\n",
       "      <td>N250WN</td>\n",
       "      <td>33192</td>\n",
       "      <td>SMF</td>\n",
       "      <td>32575</td>\n",
       "      <td>BUR</td>\n",
       "      <td>1945</td>\n",
       "      <td>2100</td>\n",
       "      <td>75.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>358.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806849</th>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>DL</td>\n",
       "      <td>N357NW</td>\n",
       "      <td>33195</td>\n",
       "      <td>TPA</td>\n",
       "      <td>30721</td>\n",
       "      <td>BOS</td>\n",
       "      <td>1500</td>\n",
       "      <td>1803</td>\n",
       "      <td>183.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        YEAR  MONTH  DAY_OF_MONTH  DAY_OF_WEEK OP_UNIQUE_CARRIER TAIL_NUM  \\\n",
       "64316   2019      1             3            4                WN   N8682B   \n",
       "293415  2019      1            13            7                WN   N8509U   \n",
       "653484  2019      2            12            2                OO   N973SW   \n",
       "461905  2019      1            10            4                WN   N250WN   \n",
       "806849  2019      2            20            3                DL   N357NW   \n",
       "\n",
       "        ORIGIN_CITY_MARKET_ID ORIGIN  DEST_CITY_MARKET_ID DEST  CRS_DEP_TIME  \\\n",
       "64316                   30423    AUS                32457  SJC          1655   \n",
       "293415                  30325    DEN                31453  HOU          1235   \n",
       "653484                  34006    PAH                30977  ORD          1719   \n",
       "461905                  33192    SMF                32575  BUR          1945   \n",
       "806849                  33195    TPA                30721  BOS          1500   \n",
       "\n",
       "        CRS_ARR_TIME  CRS_ELAPSED_TIME  FLIGHTS  DISTANCE  DISTANCE_GROUP  \n",
       "64316           1850             235.0      1.0    1476.0               6  \n",
       "293415          1600             145.0      1.0     883.0               4  \n",
       "653484          1905             106.0      1.0     342.0               2  \n",
       "461905          2100              75.0      1.0     358.0               2  \n",
       "806849          1803             183.0      1.0    1185.0               5  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.drop('ARR_DEL15', axis=1)\n",
    "X.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "292688    0.0\n",
       "37309     0.0\n",
       "476296    0.0\n",
       "372396    0.0\n",
       "848274    0.0\n",
       "Name: ARR_DEL15, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df['ARR_DEL15']\n",
    "y.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 396.48 MiB, increment: 0.24 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Simple Guide to Scikit-learn Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before building the pipeline, let's split the data into a train and test set so that the performance of the model can be validated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The first step in building the pipeline is to define each transformer type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 295 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Next, let's use the ColumnTransformer to apply the transformations to the correct columns in the dataframe. Before building this, the numeric and categorical columns shall be listed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 161 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "numeric_features = df.select_dtypes(include=['int64', 'float64']).drop(['ARR_DEL15'], axis=1).columns\n",
    "categorical_features = df.select_dtypes(include=['object']).columns\n",
    "from sklearn.compose import ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create a pipeline that combines the preprocessor created above with a classifier. In this case a simple RandomForestClassifier has been used to start with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5h 33min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', RandomForestClassifier(random_state=0))])\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A pipeline can also be used during the model selection process**. The following example code loops through a number of scikit-learn classifiers applying the transformations and training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"rbf\", C=0.025, probability=True, random_state=0),\n",
    "#     NuSVC(probability=True), # ValueError: b'specified nu is infeasible'\n",
    "    DecisionTreeClassifier(random_state=0),\n",
    "    RandomForestClassifier(random_state=0),\n",
    "    AdaBoostClassifier(random_state=0),\n",
    "    GradientBoostingClassifier(random_state=0)\n",
    "    ]\n",
    "for classifier in classifiers:\n",
    "    pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', classifier)])\n",
    "    pipe.fit(X_train, y_train)   \n",
    "    print(classifier)\n",
    "    print(\"model score: %.3f \\n\" % pipe.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The pipeline can also be used in grid search** to find the best performing parameters. To do this, let's first create a parameter grid for the chosen model.\n",
    "\n",
    "*One important thing to note is that there is a need to append the name given to the classifier part of the pipeline to each parameter name. In the code above its name is ‘classifier’ so 'classifier__' has been added to each parameter.*\n",
    "\n",
    "Next a grid search object has been created, which includes the original pipeline. When fit is called, the transformations are applied to the data, before a cross-validated grid-search is performed over the parameter grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.metrics.SCORERS.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "param_grid = { \n",
    "    'classifier__n_estimators': [200, 500],\n",
    "    'classifier__max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'classifier__max_depth' : [4,5,6,7,8],\n",
    "    'classifier__criterion' :['gini', 'entropy']}\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Setting refit='AUC', refits an estimator on the whole dataset with the\n",
    "# parameter setting that has the best cross-validated AUC score.\n",
    "# That estimator is made available at ``gs.best_estimator_`` along with\n",
    "# parameters like ``gs.best_score_``, ``gs.best_params_`` and\n",
    "# ``gs.best_index_``\n",
    "scoring = {'AUC': 'roc_auc', 'F1': 'f1', 'Precision': 'precision',\n",
    "           'Recall': 'recall', 'Accuracy': 'accuracy'}\n",
    "CV = GridSearchCV(rf, param_grid, n_jobs= 1, scoring=scoring, refit='AUC') # change the scorer metric\n",
    "                  \n",
    "CV.fit(X_train, y_train)  \n",
    "print(CV.best_params_)    \n",
    "print(CV.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = CV.predict(X_test)\n",
    "probabilities = CV.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. [Main Classification Model Metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#the-scoring-parameter-defining-model-evaluation-rules):\n",
    "\n",
    "- [Accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score)\n",
    "- \n",
    "\n",
    "#### 2. [Classification Metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics)\n",
    "\n",
    "- Binary classification (only):\n",
    "    - [precision_recall_curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html#sklearn.metrics.precision_recall_curve): Compute precision-recall pairs for different probability thresholds.\n",
    "        - The precision is the ratio `tp / (tp + fp)` where `tp` is the number of true positives and `fp` the number of false positives. The **precision** is intuitively the ability of the classifier **not to label as positive a sample that is negative**.\n",
    "        - The recall is the ratio `tp / (tp + fn)` where `tp` is the number of true positives and `fn` the number of false negatives. The **recall** is intuitively the ability of the classifier to **find all the positive samples**.\n",
    "    - [roc_curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve): A receiver operating characteristic (ROC), or simply ROC curve, is a graphical plot which illustrates the performance of a binary classifier system as its discrimination threshold is varied. It is created by plotting the fraction of true positives out of the positives (TPR = true positive rate) vs. the fraction of false positives out of the negatives (FPR = false positive rate), at various threshold settings. TPR is also known as sensitivity, and FPR is one minus the specificity or true negative rate.\n",
    "        \n",
    "- Multi-class classification (or binary):\n",
    "    - [confusion_matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix): Compute confusion matrix to evaluate the accuracy of a classification.\n",
    "    ![IMG_Confusion_Matrix](https://scikit-learn.org/stable/_images/sphx_glr_plot_confusion_matrix_0011.png)\n",
    "    - [roc_auc_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score): Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores.\n",
    "\n",
    "- Multi-label classification (or binary or multi-class):\n",
    "    - [accuracy_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score): Accuracy classification score.\n",
    "    - [classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report): Build a text report showing the main classification metrics.\n",
    "    - [f1_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score): Compute the F1 score, also known as balanced F-score or F-measure.\n",
    "        - The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal: `F1 = 2 * (precision * recall) / (precision + recall)`\n",
    "    - [precision_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score)\n",
    "    - [recall_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, recall_score, precision_score, roc_auc_score, roc_curve, accuracy_score\n",
    "\n",
    "tpr, fpr, thresolds = roc_curve(y_test, probabilities[:, 1])\n",
    "plt.plot(tpr, fpr)\n",
    "f1_score(y_test, predictions), recall_score(y_test, predictions), precision_score(y_test, predictions), accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
